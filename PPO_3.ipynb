{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PPO_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lz4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmHSKZHWAGD5",
        "outputId": "a2138419-b8d9-4ba5-ce50-6d9f7b8052ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lz4\n",
            "  Downloading lz4-4.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 17.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "Successfully installed lz4-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ray"
      ],
      "metadata": {
        "id": "DmeqIoXewIuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "767a3957-d34e-4678-c486-8c3bf21bef2d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray\n",
            "  Downloading ray-1.13.0-cp37-cp37m-manylinux2014_x86_64.whl (54.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.5 MB 263 kB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Collecting aiosignal\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting frozenlist\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 75.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.7.1)\n",
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.15.1-py2.py3-none-any.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.4)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Collecting grpcio<=1.43.0,>=1.28.1\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.1.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2022.6.15)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 69.2 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: platformdirs, frozenlist, distlib, virtualenv, grpcio, aiosignal, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.3\n",
            "    Uninstalling grpcio-1.46.3:\n",
            "      Successfully uninstalled grpcio-1.46.3\n",
            "Successfully installed aiosignal-1.2.0 distlib-0.3.4 frozenlist-1.3.0 grpcio-1.43.0 platformdirs-2.5.2 ray-1.13.0 virtualenv-20.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "ray.init(num_gpus=2, num_cpus=80,ignore_reinit_error=True)"
      ],
      "metadata": {
        "id": "gpU0Ekk4uhnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70376d1b-2fd9-474f-e00f-427584c1a820"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RayContext(dashboard_url='', python_version='3.7.13', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '172.28.0.2', 'raylet_ip_address': '172.28.0.2', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-07-04_15-51-54_548889_70/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-07-04_15-51-54_548889_70/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-07-04_15-51-54_548889_70', 'metrics_export_port': 61564, 'gcs_address': '172.28.0.2:64531', 'address': '172.28.0.2:64531', 'node_id': '60f5971eb379323b3476b8b31af9ac59deb357e1da726a8258bc04f3'})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray.rllib.agents.ppo as ppo"
      ],
      "metadata": {
        "id": "qSARSd4Xuil9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install \"ray[tune]\""
      ],
      "metadata": {
        "id": "bpXL8Y6F4ATY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray import tune"
      ],
      "metadata": {
        "id": "H51SznLXxUEA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute this, only when you want to save Trained Checkpoints**"
      ],
      "metadata": {
        "id": "3lcYMGAW5o6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "wyRrkTs_678O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopper(trial_id, result):\n",
        "    return result[\"training_iteration\"] >= 100"
      ],
      "metadata": {
        "id": "fWMGq-TV7GnQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis = tune.run(\n",
        "    \"PPO\",\n",
        "    mode=\"max\",\n",
        "    checkpoint_at_end=True,\n",
        "    stop=stopper,\n",
        "    config={\"env\":\"CartPole-v0\",\"evaluation_interval\":2,\"evaluation_num_episodes\": 20},\n",
        ")\n",
        "\n",
        "dfs = analysis.trial_dataframes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1mTzonUk7Pub",
        "outputId": "a30e1ed3-be10-4525-bc3a-c1cd8633e5c6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-04 15:55:53,229\tWARNING callback.py:106 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "\u001b[2m\u001b[36m(PPOTrainer pid=622)\u001b[0m 2022-07-04 15:55:57,918\tINFO trainer.py:2333 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "\u001b[2m\u001b[36m(PPOTrainer pid=622)\u001b[0m 2022-07-04 15:55:57,918\tWARNING deprecation.py:47 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(PPOTrainer pid=622)\u001b[0m 2022-07-04 15:55:57,918\tINFO ppo.py:415 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            "\u001b[2m\u001b[36m(PPOTrainer pid=622)\u001b[0m 2022-07-04 15:55:57,918\tINFO trainer.py:906 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(PPOTrainer pid=622)\u001b[0m 2022-07-04 15:56:07,012\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:07 (running for 00:00:14.55)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(PPOTrainer pid=622)\u001b[0m 2022-07-04 15:56:07,781\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
            "\u001b[2m\u001b[36m(PPOTrainer pid=622)\u001b[0m 2022-07-04 15:56:10,595\tWARNING deprecation.py:47 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:12 (running for 00:00:19.56)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 4000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 4000\n",
            "    num_agent_steps_trained: 4000\n",
            "    num_env_steps_sampled: 4000\n",
            "    num_env_steps_trained: 4000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-56-15\n",
            "  done: false\n",
            "  episode_len_mean: 21.818681318681318\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 74.0\n",
            "  episode_reward_mean: 21.818681318681318\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 182\n",
            "  episodes_total: 182\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.20000000298023224\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.666664719581604\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.027661588042974472\n",
            "          model: {}\n",
            "          policy_loss: -0.04528815299272537\n",
            "          total_loss: 8.772823333740234\n",
            "          vf_explained_var: 0.0055037811398506165\n",
            "          vf_loss: 8.812579154968262\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 4000\n",
            "    num_agent_steps_trained: 4000\n",
            "    num_env_steps_sampled: 4000\n",
            "    num_env_steps_trained: 4000\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 4000\n",
            "  num_agent_steps_trained: 4000\n",
            "  num_env_steps_sampled: 4000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 4000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.87272727272726\n",
            "    ram_util_percent: 21.990909090909092\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09443934505278352\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07958922314496296\n",
            "    mean_inference_ms: 1.0384800083327934\n",
            "    mean_raw_obs_processing_ms: 0.15877111465952112\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 21.818681318681318\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 74.0\n",
            "    episode_reward_mean: 21.818681318681318\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 182\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 14\n",
            "      - 23\n",
            "      - 9\n",
            "      - 30\n",
            "      - 21\n",
            "      - 9\n",
            "      - 35\n",
            "      - 19\n",
            "      - 24\n",
            "      - 24\n",
            "      - 10\n",
            "      - 27\n",
            "      - 29\n",
            "      - 39\n",
            "      - 74\n",
            "      - 55\n",
            "      - 28\n",
            "      - 17\n",
            "      - 18\n",
            "      - 14\n",
            "      - 19\n",
            "      - 22\n",
            "      - 28\n",
            "      - 58\n",
            "      - 17\n",
            "      - 47\n",
            "      - 37\n",
            "      - 13\n",
            "      - 12\n",
            "      - 11\n",
            "      - 32\n",
            "      - 20\n",
            "      - 24\n",
            "      - 39\n",
            "      - 13\n",
            "      - 38\n",
            "      - 45\n",
            "      - 48\n",
            "      - 17\n",
            "      - 57\n",
            "      - 26\n",
            "      - 13\n",
            "      - 11\n",
            "      - 26\n",
            "      - 46\n",
            "      - 18\n",
            "      - 20\n",
            "      - 21\n",
            "      - 29\n",
            "      - 13\n",
            "      - 9\n",
            "      - 23\n",
            "      - 16\n",
            "      - 15\n",
            "      - 21\n",
            "      - 13\n",
            "      - 28\n",
            "      - 14\n",
            "      - 16\n",
            "      - 18\n",
            "      - 19\n",
            "      - 14\n",
            "      - 58\n",
            "      - 33\n",
            "      - 18\n",
            "      - 19\n",
            "      - 26\n",
            "      - 18\n",
            "      - 18\n",
            "      - 19\n",
            "      - 15\n",
            "      - 11\n",
            "      - 19\n",
            "      - 66\n",
            "      - 9\n",
            "      - 9\n",
            "      - 23\n",
            "      - 10\n",
            "      - 63\n",
            "      - 12\n",
            "      - 14\n",
            "      - 28\n",
            "      - 14\n",
            "      - 40\n",
            "      - 17\n",
            "      - 11\n",
            "      - 37\n",
            "      - 18\n",
            "      - 20\n",
            "      - 9\n",
            "      - 11\n",
            "      - 22\n",
            "      - 18\n",
            "      - 20\n",
            "      - 13\n",
            "      - 11\n",
            "      - 12\n",
            "      - 19\n",
            "      - 21\n",
            "      - 13\n",
            "      - 13\n",
            "      - 21\n",
            "      - 40\n",
            "      - 17\n",
            "      - 14\n",
            "      - 22\n",
            "      - 11\n",
            "      - 10\n",
            "      - 10\n",
            "      - 32\n",
            "      - 10\n",
            "      - 21\n",
            "      - 36\n",
            "      - 16\n",
            "      - 16\n",
            "      - 18\n",
            "      - 41\n",
            "      - 37\n",
            "      - 15\n",
            "      - 13\n",
            "      - 30\n",
            "      - 19\n",
            "      - 21\n",
            "      - 15\n",
            "      - 18\n",
            "      - 10\n",
            "      - 9\n",
            "      - 20\n",
            "      - 24\n",
            "      - 48\n",
            "      - 13\n",
            "      - 16\n",
            "      - 28\n",
            "      - 19\n",
            "      - 15\n",
            "      - 17\n",
            "      - 26\n",
            "      - 22\n",
            "      - 24\n",
            "      - 28\n",
            "      - 20\n",
            "      - 17\n",
            "      - 14\n",
            "      - 19\n",
            "      - 34\n",
            "      - 33\n",
            "      - 24\n",
            "      - 10\n",
            "      - 9\n",
            "      - 14\n",
            "      - 35\n",
            "      - 30\n",
            "      - 18\n",
            "      - 13\n",
            "      - 19\n",
            "      - 13\n",
            "      - 12\n",
            "      - 14\n",
            "      - 10\n",
            "      - 13\n",
            "      - 10\n",
            "      - 15\n",
            "      - 9\n",
            "      - 11\n",
            "      - 14\n",
            "      - 16\n",
            "      - 20\n",
            "      - 15\n",
            "      - 10\n",
            "      - 13\n",
            "      - 42\n",
            "      - 19\n",
            "      - 18\n",
            "      - 10\n",
            "      - 17\n",
            "      - 30\n",
            "      - 13\n",
            "      - 19\n",
            "      - 52\n",
            "      - 13\n",
            "      - 24\n",
            "      - 16\n",
            "      episode_reward:\n",
            "      - 14.0\n",
            "      - 23.0\n",
            "      - 9.0\n",
            "      - 30.0\n",
            "      - 21.0\n",
            "      - 9.0\n",
            "      - 35.0\n",
            "      - 19.0\n",
            "      - 24.0\n",
            "      - 24.0\n",
            "      - 10.0\n",
            "      - 27.0\n",
            "      - 29.0\n",
            "      - 39.0\n",
            "      - 74.0\n",
            "      - 55.0\n",
            "      - 28.0\n",
            "      - 17.0\n",
            "      - 18.0\n",
            "      - 14.0\n",
            "      - 19.0\n",
            "      - 22.0\n",
            "      - 28.0\n",
            "      - 58.0\n",
            "      - 17.0\n",
            "      - 47.0\n",
            "      - 37.0\n",
            "      - 13.0\n",
            "      - 12.0\n",
            "      - 11.0\n",
            "      - 32.0\n",
            "      - 20.0\n",
            "      - 24.0\n",
            "      - 39.0\n",
            "      - 13.0\n",
            "      - 38.0\n",
            "      - 45.0\n",
            "      - 48.0\n",
            "      - 17.0\n",
            "      - 57.0\n",
            "      - 26.0\n",
            "      - 13.0\n",
            "      - 11.0\n",
            "      - 26.0\n",
            "      - 46.0\n",
            "      - 18.0\n",
            "      - 20.0\n",
            "      - 21.0\n",
            "      - 29.0\n",
            "      - 13.0\n",
            "      - 9.0\n",
            "      - 23.0\n",
            "      - 16.0\n",
            "      - 15.0\n",
            "      - 21.0\n",
            "      - 13.0\n",
            "      - 28.0\n",
            "      - 14.0\n",
            "      - 16.0\n",
            "      - 18.0\n",
            "      - 19.0\n",
            "      - 14.0\n",
            "      - 58.0\n",
            "      - 33.0\n",
            "      - 18.0\n",
            "      - 19.0\n",
            "      - 26.0\n",
            "      - 18.0\n",
            "      - 18.0\n",
            "      - 19.0\n",
            "      - 15.0\n",
            "      - 11.0\n",
            "      - 19.0\n",
            "      - 66.0\n",
            "      - 9.0\n",
            "      - 9.0\n",
            "      - 23.0\n",
            "      - 10.0\n",
            "      - 63.0\n",
            "      - 12.0\n",
            "      - 14.0\n",
            "      - 28.0\n",
            "      - 14.0\n",
            "      - 40.0\n",
            "      - 17.0\n",
            "      - 11.0\n",
            "      - 37.0\n",
            "      - 18.0\n",
            "      - 20.0\n",
            "      - 9.0\n",
            "      - 11.0\n",
            "      - 22.0\n",
            "      - 18.0\n",
            "      - 20.0\n",
            "      - 13.0\n",
            "      - 11.0\n",
            "      - 12.0\n",
            "      - 19.0\n",
            "      - 21.0\n",
            "      - 13.0\n",
            "      - 13.0\n",
            "      - 21.0\n",
            "      - 40.0\n",
            "      - 17.0\n",
            "      - 14.0\n",
            "      - 22.0\n",
            "      - 11.0\n",
            "      - 10.0\n",
            "      - 10.0\n",
            "      - 32.0\n",
            "      - 10.0\n",
            "      - 21.0\n",
            "      - 36.0\n",
            "      - 16.0\n",
            "      - 16.0\n",
            "      - 18.0\n",
            "      - 41.0\n",
            "      - 37.0\n",
            "      - 15.0\n",
            "      - 13.0\n",
            "      - 30.0\n",
            "      - 19.0\n",
            "      - 21.0\n",
            "      - 15.0\n",
            "      - 18.0\n",
            "      - 10.0\n",
            "      - 9.0\n",
            "      - 20.0\n",
            "      - 24.0\n",
            "      - 48.0\n",
            "      - 13.0\n",
            "      - 16.0\n",
            "      - 28.0\n",
            "      - 19.0\n",
            "      - 15.0\n",
            "      - 17.0\n",
            "      - 26.0\n",
            "      - 22.0\n",
            "      - 24.0\n",
            "      - 28.0\n",
            "      - 20.0\n",
            "      - 17.0\n",
            "      - 14.0\n",
            "      - 19.0\n",
            "      - 34.0\n",
            "      - 33.0\n",
            "      - 24.0\n",
            "      - 10.0\n",
            "      - 9.0\n",
            "      - 14.0\n",
            "      - 35.0\n",
            "      - 30.0\n",
            "      - 18.0\n",
            "      - 13.0\n",
            "      - 19.0\n",
            "      - 13.0\n",
            "      - 12.0\n",
            "      - 14.0\n",
            "      - 10.0\n",
            "      - 13.0\n",
            "      - 10.0\n",
            "      - 15.0\n",
            "      - 9.0\n",
            "      - 11.0\n",
            "      - 14.0\n",
            "      - 16.0\n",
            "      - 20.0\n",
            "      - 15.0\n",
            "      - 10.0\n",
            "      - 13.0\n",
            "      - 42.0\n",
            "      - 19.0\n",
            "      - 18.0\n",
            "      - 10.0\n",
            "      - 17.0\n",
            "      - 30.0\n",
            "      - 13.0\n",
            "      - 19.0\n",
            "      - 52.0\n",
            "      - 13.0\n",
            "      - 24.0\n",
            "      - 16.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09443934505278352\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07958922314496296\n",
            "      mean_inference_ms: 1.0384800083327934\n",
            "      mean_raw_obs_processing_ms: 0.15877111465952112\n",
            "  time_since_restore: 7.616447925567627\n",
            "  time_this_iter_s: 7.616447925567627\n",
            "  time_total_s: 7.616447925567627\n",
            "  timers:\n",
            "    learn_throughput: 834.625\n",
            "    learn_time_ms: 4792.57\n",
            "    load_throughput: 16844594.378\n",
            "    load_time_ms: 0.237\n",
            "    training_iteration_time_ms: 7610.71\n",
            "    update_time_ms: 2.582\n",
            "  timestamp: 1656950175\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 4000\n",
            "  training_iteration: 1\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:20 (running for 00:00:27.19)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.61645</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 21.8187</td><td style=\"text-align: right;\">                  74</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           21.8187</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:25 (running for 00:00:32.27)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.61645</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 21.8187</td><td style=\"text-align: right;\">                  74</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           21.8187</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 8000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_agent_steps_trained: 8000\n",
            "    num_env_steps_sampled: 8000\n",
            "    num_env_steps_trained: 8000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-56-25\n",
            "  done: false\n",
            "  episode_len_mean: 42.83\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 42.83\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 88\n",
            "  episodes_total: 270\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 114.6\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 114.6\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 152\n",
            "      - 157\n",
            "      - 92\n",
            "      - 103\n",
            "      - 95\n",
            "      - 39\n",
            "      - 124\n",
            "      - 200\n",
            "      - 15\n",
            "      - 49\n",
            "      - 195\n",
            "      - 113\n",
            "      - 72\n",
            "      - 82\n",
            "      - 63\n",
            "      - 200\n",
            "      - 65\n",
            "      - 76\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 152.0\n",
            "      - 157.0\n",
            "      - 92.0\n",
            "      - 103.0\n",
            "      - 95.0\n",
            "      - 39.0\n",
            "      - 124.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 49.0\n",
            "      - 195.0\n",
            "      - 113.0\n",
            "      - 72.0\n",
            "      - 82.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 65.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.08062592667364626\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.061409576356125053\n",
            "      mean_inference_ms: 0.8300495522021208\n",
            "      mean_raw_obs_processing_ms: 0.08795129332644318\n",
            "    timesteps_this_iter: 2292\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.30000001192092896\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.6147445440292358\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.017079275101423264\n",
            "          model: {}\n",
            "          policy_loss: -0.03028806857764721\n",
            "          total_loss: 8.754085540771484\n",
            "          vf_explained_var: 0.04221675172448158\n",
            "          vf_loss: 8.77924919128418\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_agent_steps_trained: 8000\n",
            "    num_env_steps_sampled: 8000\n",
            "    num_env_steps_trained: 8000\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 8000\n",
            "  num_agent_steps_trained: 8000\n",
            "  num_env_steps_sampled: 8000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 8000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 87.00666666666666\n",
            "    ram_util_percent: 21.933333333333334\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09567919988386661\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07960954219269264\n",
            "    mean_inference_ms: 1.0340084728185135\n",
            "    mean_raw_obs_processing_ms: 0.1526064902254259\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 42.83\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 42.83\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 88\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 42\n",
            "      - 19\n",
            "      - 18\n",
            "      - 10\n",
            "      - 17\n",
            "      - 30\n",
            "      - 13\n",
            "      - 19\n",
            "      - 52\n",
            "      - 13\n",
            "      - 24\n",
            "      - 16\n",
            "      - 38\n",
            "      - 36\n",
            "      - 69\n",
            "      - 15\n",
            "      - 33\n",
            "      - 42\n",
            "      - 33\n",
            "      - 13\n",
            "      - 161\n",
            "      - 37\n",
            "      - 27\n",
            "      - 24\n",
            "      - 14\n",
            "      - 17\n",
            "      - 64\n",
            "      - 86\n",
            "      - 73\n",
            "      - 82\n",
            "      - 35\n",
            "      - 26\n",
            "      - 39\n",
            "      - 26\n",
            "      - 16\n",
            "      - 31\n",
            "      - 87\n",
            "      - 61\n",
            "      - 10\n",
            "      - 107\n",
            "      - 54\n",
            "      - 19\n",
            "      - 15\n",
            "      - 26\n",
            "      - 148\n",
            "      - 34\n",
            "      - 31\n",
            "      - 19\n",
            "      - 27\n",
            "      - 151\n",
            "      - 40\n",
            "      - 105\n",
            "      - 36\n",
            "      - 17\n",
            "      - 19\n",
            "      - 17\n",
            "      - 34\n",
            "      - 25\n",
            "      - 27\n",
            "      - 59\n",
            "      - 14\n",
            "      - 46\n",
            "      - 10\n",
            "      - 40\n",
            "      - 103\n",
            "      - 28\n",
            "      - 38\n",
            "      - 19\n",
            "      - 38\n",
            "      - 117\n",
            "      - 23\n",
            "      - 43\n",
            "      - 20\n",
            "      - 16\n",
            "      - 45\n",
            "      - 67\n",
            "      - 138\n",
            "      - 19\n",
            "      - 200\n",
            "      - 27\n",
            "      - 15\n",
            "      - 27\n",
            "      - 41\n",
            "      - 31\n",
            "      - 70\n",
            "      - 22\n",
            "      - 29\n",
            "      - 31\n",
            "      - 132\n",
            "      - 9\n",
            "      - 27\n",
            "      - 99\n",
            "      - 23\n",
            "      - 50\n",
            "      - 33\n",
            "      - 13\n",
            "      - 30\n",
            "      - 14\n",
            "      - 15\n",
            "      - 43\n",
            "      episode_reward:\n",
            "      - 42.0\n",
            "      - 19.0\n",
            "      - 18.0\n",
            "      - 10.0\n",
            "      - 17.0\n",
            "      - 30.0\n",
            "      - 13.0\n",
            "      - 19.0\n",
            "      - 52.0\n",
            "      - 13.0\n",
            "      - 24.0\n",
            "      - 16.0\n",
            "      - 38.0\n",
            "      - 36.0\n",
            "      - 69.0\n",
            "      - 15.0\n",
            "      - 33.0\n",
            "      - 42.0\n",
            "      - 33.0\n",
            "      - 13.0\n",
            "      - 161.0\n",
            "      - 37.0\n",
            "      - 27.0\n",
            "      - 24.0\n",
            "      - 14.0\n",
            "      - 17.0\n",
            "      - 64.0\n",
            "      - 86.0\n",
            "      - 73.0\n",
            "      - 82.0\n",
            "      - 35.0\n",
            "      - 26.0\n",
            "      - 39.0\n",
            "      - 26.0\n",
            "      - 16.0\n",
            "      - 31.0\n",
            "      - 87.0\n",
            "      - 61.0\n",
            "      - 10.0\n",
            "      - 107.0\n",
            "      - 54.0\n",
            "      - 19.0\n",
            "      - 15.0\n",
            "      - 26.0\n",
            "      - 148.0\n",
            "      - 34.0\n",
            "      - 31.0\n",
            "      - 19.0\n",
            "      - 27.0\n",
            "      - 151.0\n",
            "      - 40.0\n",
            "      - 105.0\n",
            "      - 36.0\n",
            "      - 17.0\n",
            "      - 19.0\n",
            "      - 17.0\n",
            "      - 34.0\n",
            "      - 25.0\n",
            "      - 27.0\n",
            "      - 59.0\n",
            "      - 14.0\n",
            "      - 46.0\n",
            "      - 10.0\n",
            "      - 40.0\n",
            "      - 103.0\n",
            "      - 28.0\n",
            "      - 38.0\n",
            "      - 19.0\n",
            "      - 38.0\n",
            "      - 117.0\n",
            "      - 23.0\n",
            "      - 43.0\n",
            "      - 20.0\n",
            "      - 16.0\n",
            "      - 45.0\n",
            "      - 67.0\n",
            "      - 138.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 15.0\n",
            "      - 27.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 70.0\n",
            "      - 22.0\n",
            "      - 29.0\n",
            "      - 31.0\n",
            "      - 132.0\n",
            "      - 9.0\n",
            "      - 27.0\n",
            "      - 99.0\n",
            "      - 23.0\n",
            "      - 50.0\n",
            "      - 33.0\n",
            "      - 13.0\n",
            "      - 30.0\n",
            "      - 14.0\n",
            "      - 15.0\n",
            "      - 43.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09567919988386661\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07960954219269264\n",
            "      mean_inference_ms: 1.0340084728185135\n",
            "      mean_raw_obs_processing_ms: 0.1526064902254259\n",
            "  time_since_restore: 17.886704206466675\n",
            "  time_this_iter_s: 10.270256280899048\n",
            "  time_total_s: 17.886704206466675\n",
            "  timers:\n",
            "    learn_throughput: 813.131\n",
            "    learn_time_ms: 4919.254\n",
            "    load_throughput: 6894274.091\n",
            "    load_time_ms: 0.58\n",
            "    training_iteration_time_ms: 7704.017\n",
            "    update_time_ms: 3.092\n",
            "  timestamp: 1656950185\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 8000\n",
            "  training_iteration: 2\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:30 (running for 00:00:37.49)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         17.8867</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">   42.83</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             42.83</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 12000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 12000\n",
            "    num_agent_steps_trained: 12000\n",
            "    num_env_steps_sampled: 12000\n",
            "    num_env_steps_trained: 12000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-56-34\n",
            "  done: false\n",
            "  episode_len_mean: 69.52\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 69.52\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 36\n",
            "  episodes_total: 306\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.30000001192092896\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5736927390098572\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.007355896290391684\n",
            "          model: {}\n",
            "          policy_loss: -0.021817049011588097\n",
            "          total_loss: 9.313850402832031\n",
            "          vf_explained_var: 0.0824841558933258\n",
            "          vf_loss: 9.33346176147461\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 12000\n",
            "    num_agent_steps_trained: 12000\n",
            "    num_env_steps_sampled: 12000\n",
            "    num_env_steps_trained: 12000\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 12000\n",
            "  num_agent_steps_trained: 12000\n",
            "  num_env_steps_sampled: 12000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 12000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.54615384615386\n",
            "    ram_util_percent: 21.915384615384614\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09755760939548956\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08026876876750805\n",
            "    mean_inference_ms: 1.0989818725968832\n",
            "    mean_raw_obs_processing_ms: 0.15237488003080393\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 69.52\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 69.52\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 36\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 87\n",
            "      - 61\n",
            "      - 10\n",
            "      - 107\n",
            "      - 54\n",
            "      - 19\n",
            "      - 15\n",
            "      - 26\n",
            "      - 148\n",
            "      - 34\n",
            "      - 31\n",
            "      - 19\n",
            "      - 27\n",
            "      - 151\n",
            "      - 40\n",
            "      - 105\n",
            "      - 36\n",
            "      - 17\n",
            "      - 19\n",
            "      - 17\n",
            "      - 34\n",
            "      - 25\n",
            "      - 27\n",
            "      - 59\n",
            "      - 14\n",
            "      - 46\n",
            "      - 10\n",
            "      - 40\n",
            "      - 103\n",
            "      - 28\n",
            "      - 38\n",
            "      - 19\n",
            "      - 38\n",
            "      - 117\n",
            "      - 23\n",
            "      - 43\n",
            "      - 20\n",
            "      - 16\n",
            "      - 45\n",
            "      - 67\n",
            "      - 138\n",
            "      - 19\n",
            "      - 200\n",
            "      - 27\n",
            "      - 15\n",
            "      - 27\n",
            "      - 41\n",
            "      - 31\n",
            "      - 70\n",
            "      - 22\n",
            "      - 29\n",
            "      - 31\n",
            "      - 132\n",
            "      - 9\n",
            "      - 27\n",
            "      - 99\n",
            "      - 23\n",
            "      - 50\n",
            "      - 33\n",
            "      - 13\n",
            "      - 30\n",
            "      - 14\n",
            "      - 15\n",
            "      - 43\n",
            "      - 197\n",
            "      - 41\n",
            "      - 64\n",
            "      - 107\n",
            "      - 146\n",
            "      - 159\n",
            "      - 200\n",
            "      - 137\n",
            "      - 200\n",
            "      - 22\n",
            "      - 167\n",
            "      - 30\n",
            "      - 144\n",
            "      - 200\n",
            "      - 116\n",
            "      - 56\n",
            "      - 72\n",
            "      - 146\n",
            "      - 52\n",
            "      - 118\n",
            "      - 33\n",
            "      - 105\n",
            "      - 175\n",
            "      - 31\n",
            "      - 86\n",
            "      - 125\n",
            "      - 200\n",
            "      - 69\n",
            "      - 41\n",
            "      - 134\n",
            "      - 82\n",
            "      - 73\n",
            "      - 198\n",
            "      - 119\n",
            "      - 31\n",
            "      - 103\n",
            "      episode_reward:\n",
            "      - 87.0\n",
            "      - 61.0\n",
            "      - 10.0\n",
            "      - 107.0\n",
            "      - 54.0\n",
            "      - 19.0\n",
            "      - 15.0\n",
            "      - 26.0\n",
            "      - 148.0\n",
            "      - 34.0\n",
            "      - 31.0\n",
            "      - 19.0\n",
            "      - 27.0\n",
            "      - 151.0\n",
            "      - 40.0\n",
            "      - 105.0\n",
            "      - 36.0\n",
            "      - 17.0\n",
            "      - 19.0\n",
            "      - 17.0\n",
            "      - 34.0\n",
            "      - 25.0\n",
            "      - 27.0\n",
            "      - 59.0\n",
            "      - 14.0\n",
            "      - 46.0\n",
            "      - 10.0\n",
            "      - 40.0\n",
            "      - 103.0\n",
            "      - 28.0\n",
            "      - 38.0\n",
            "      - 19.0\n",
            "      - 38.0\n",
            "      - 117.0\n",
            "      - 23.0\n",
            "      - 43.0\n",
            "      - 20.0\n",
            "      - 16.0\n",
            "      - 45.0\n",
            "      - 67.0\n",
            "      - 138.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 15.0\n",
            "      - 27.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 70.0\n",
            "      - 22.0\n",
            "      - 29.0\n",
            "      - 31.0\n",
            "      - 132.0\n",
            "      - 9.0\n",
            "      - 27.0\n",
            "      - 99.0\n",
            "      - 23.0\n",
            "      - 50.0\n",
            "      - 33.0\n",
            "      - 13.0\n",
            "      - 30.0\n",
            "      - 14.0\n",
            "      - 15.0\n",
            "      - 43.0\n",
            "      - 197.0\n",
            "      - 41.0\n",
            "      - 64.0\n",
            "      - 107.0\n",
            "      - 146.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 22.0\n",
            "      - 167.0\n",
            "      - 30.0\n",
            "      - 144.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 56.0\n",
            "      - 72.0\n",
            "      - 146.0\n",
            "      - 52.0\n",
            "      - 118.0\n",
            "      - 33.0\n",
            "      - 105.0\n",
            "      - 175.0\n",
            "      - 31.0\n",
            "      - 86.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 41.0\n",
            "      - 134.0\n",
            "      - 82.0\n",
            "      - 73.0\n",
            "      - 198.0\n",
            "      - 119.0\n",
            "      - 31.0\n",
            "      - 103.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09755760939548956\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08026876876750805\n",
            "      mean_inference_ms: 1.0989818725968832\n",
            "      mean_raw_obs_processing_ms: 0.15237488003080393\n",
            "  time_since_restore: 26.80667519569397\n",
            "  time_this_iter_s: 8.919970989227295\n",
            "  time_total_s: 26.80667519569397\n",
            "  timers:\n",
            "    learn_throughput: 821.527\n",
            "    learn_time_ms: 4868.981\n",
            "    load_throughput: 8850298.576\n",
            "    load_time_ms: 0.452\n",
            "    training_iteration_time_ms: 8106.459\n",
            "    update_time_ms: 3.292\n",
            "  timestamp: 1656950194\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 12000\n",
            "  training_iteration: 3\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:39 (running for 00:00:46.60)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         26.8067</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   69.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             69.52</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:44 (running for 00:00:51.63)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         26.8067</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">   69.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             69.52</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 16000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 16000\n",
            "    num_agent_steps_trained: 16000\n",
            "    num_env_steps_sampled: 16000\n",
            "    num_env_steps_trained: 16000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-56-45\n",
            "  done: false\n",
            "  episode_len_mean: 97.46\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 97.46\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 26\n",
            "  episodes_total: 332\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 165.15\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 165.15\n",
            "    episode_reward_min: 40.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 74\n",
            "      - 123\n",
            "      - 200\n",
            "      - 115\n",
            "      - 192\n",
            "      - 200\n",
            "      - 145\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 40\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 168\n",
            "      - 46\n",
            "      episode_reward:\n",
            "      - 74.0\n",
            "      - 123.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 40.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 46.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.08007102220547549\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060896561604214174\n",
            "      mean_inference_ms: 0.8168749676336979\n",
            "      mean_raw_obs_processing_ms: 0.08600830946588961\n",
            "    timesteps_this_iter: 3303\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.30000001192092896\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5687455534934998\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003935570828616619\n",
            "          model: {}\n",
            "          policy_loss: -0.013853424228727818\n",
            "          total_loss: 9.413006782531738\n",
            "          vf_explained_var: 0.07146818190813065\n",
            "          vf_loss: 9.425679206848145\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 16000\n",
            "    num_agent_steps_trained: 16000\n",
            "    num_env_steps_sampled: 16000\n",
            "    num_env_steps_trained: 16000\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 16000\n",
            "  num_agent_steps_trained: 16000\n",
            "  num_env_steps_sampled: 16000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 16000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 83.69333333333333\n",
            "    ram_util_percent: 21.899999999999995\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09895618137959557\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08094296466651435\n",
            "    mean_inference_ms: 1.1356148069262026\n",
            "    mean_raw_obs_processing_ms: 0.15219637408877928\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 97.46\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 97.46\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 26\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 10\n",
            "      - 40\n",
            "      - 103\n",
            "      - 28\n",
            "      - 38\n",
            "      - 19\n",
            "      - 38\n",
            "      - 117\n",
            "      - 23\n",
            "      - 43\n",
            "      - 20\n",
            "      - 16\n",
            "      - 45\n",
            "      - 67\n",
            "      - 138\n",
            "      - 19\n",
            "      - 200\n",
            "      - 27\n",
            "      - 15\n",
            "      - 27\n",
            "      - 41\n",
            "      - 31\n",
            "      - 70\n",
            "      - 22\n",
            "      - 29\n",
            "      - 31\n",
            "      - 132\n",
            "      - 9\n",
            "      - 27\n",
            "      - 99\n",
            "      - 23\n",
            "      - 50\n",
            "      - 33\n",
            "      - 13\n",
            "      - 30\n",
            "      - 14\n",
            "      - 15\n",
            "      - 43\n",
            "      - 197\n",
            "      - 41\n",
            "      - 64\n",
            "      - 107\n",
            "      - 146\n",
            "      - 159\n",
            "      - 200\n",
            "      - 137\n",
            "      - 200\n",
            "      - 22\n",
            "      - 167\n",
            "      - 30\n",
            "      - 144\n",
            "      - 200\n",
            "      - 116\n",
            "      - 56\n",
            "      - 72\n",
            "      - 146\n",
            "      - 52\n",
            "      - 118\n",
            "      - 33\n",
            "      - 105\n",
            "      - 175\n",
            "      - 31\n",
            "      - 86\n",
            "      - 125\n",
            "      - 200\n",
            "      - 69\n",
            "      - 41\n",
            "      - 134\n",
            "      - 82\n",
            "      - 73\n",
            "      - 198\n",
            "      - 119\n",
            "      - 31\n",
            "      - 103\n",
            "      - 187\n",
            "      - 100\n",
            "      - 182\n",
            "      - 200\n",
            "      - 188\n",
            "      - 98\n",
            "      - 51\n",
            "      - 200\n",
            "      - 99\n",
            "      - 128\n",
            "      - 200\n",
            "      - 35\n",
            "      - 200\n",
            "      - 142\n",
            "      - 52\n",
            "      - 152\n",
            "      - 200\n",
            "      - 114\n",
            "      - 134\n",
            "      - 185\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      episode_reward:\n",
            "      - 10.0\n",
            "      - 40.0\n",
            "      - 103.0\n",
            "      - 28.0\n",
            "      - 38.0\n",
            "      - 19.0\n",
            "      - 38.0\n",
            "      - 117.0\n",
            "      - 23.0\n",
            "      - 43.0\n",
            "      - 20.0\n",
            "      - 16.0\n",
            "      - 45.0\n",
            "      - 67.0\n",
            "      - 138.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 15.0\n",
            "      - 27.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 70.0\n",
            "      - 22.0\n",
            "      - 29.0\n",
            "      - 31.0\n",
            "      - 132.0\n",
            "      - 9.0\n",
            "      - 27.0\n",
            "      - 99.0\n",
            "      - 23.0\n",
            "      - 50.0\n",
            "      - 33.0\n",
            "      - 13.0\n",
            "      - 30.0\n",
            "      - 14.0\n",
            "      - 15.0\n",
            "      - 43.0\n",
            "      - 197.0\n",
            "      - 41.0\n",
            "      - 64.0\n",
            "      - 107.0\n",
            "      - 146.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 22.0\n",
            "      - 167.0\n",
            "      - 30.0\n",
            "      - 144.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 56.0\n",
            "      - 72.0\n",
            "      - 146.0\n",
            "      - 52.0\n",
            "      - 118.0\n",
            "      - 33.0\n",
            "      - 105.0\n",
            "      - 175.0\n",
            "      - 31.0\n",
            "      - 86.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 41.0\n",
            "      - 134.0\n",
            "      - 82.0\n",
            "      - 73.0\n",
            "      - 198.0\n",
            "      - 119.0\n",
            "      - 31.0\n",
            "      - 103.0\n",
            "      - 187.0\n",
            "      - 100.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 98.0\n",
            "      - 51.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 52.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 134.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09895618137959557\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08094296466651435\n",
            "      mean_inference_ms: 1.1356148069262026\n",
            "      mean_raw_obs_processing_ms: 0.15219637408877928\n",
            "  time_since_restore: 37.67256259918213\n",
            "  time_this_iter_s: 10.86588740348816\n",
            "  time_total_s: 37.67256259918213\n",
            "  timers:\n",
            "    learn_throughput: 832.378\n",
            "    learn_time_ms: 4805.509\n",
            "    load_throughput: 9267900.014\n",
            "    load_time_ms: 0.432\n",
            "    training_iteration_time_ms: 7938.1\n",
            "    update_time_ms: 3.53\n",
            "  timestamp: 1656950205\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 16000\n",
            "  training_iteration: 4\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:50 (running for 00:00:57.50)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         37.6726</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   97.46</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             97.46</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 20000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 20000\n",
            "    num_agent_steps_trained: 20000\n",
            "    num_env_steps_sampled: 20000\n",
            "    num_env_steps_trained: 20000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-56-52\n",
            "  done: false\n",
            "  episode_len_mean: 125.45\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 125.45\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 352\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.15000000596046448\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5676717758178711\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005763818975538015\n",
            "          model: {}\n",
            "          policy_loss: -0.011805802583694458\n",
            "          total_loss: 9.507740020751953\n",
            "          vf_explained_var: 0.021475395187735558\n",
            "          vf_loss: 9.518680572509766\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 20000\n",
            "    num_agent_steps_trained: 20000\n",
            "    num_env_steps_sampled: 20000\n",
            "    num_env_steps_trained: 20000\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 20000\n",
            "  num_agent_steps_trained: 20000\n",
            "  num_env_steps_sampled: 20000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 20000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.43636363636364\n",
            "    ram_util_percent: 21.900000000000002\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10001418489534643\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0812353477255724\n",
            "    mean_inference_ms: 1.156487596278852\n",
            "    mean_raw_obs_processing_ms: 0.1516083632120969\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 125.45\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 125.45\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 41\n",
            "      - 31\n",
            "      - 70\n",
            "      - 22\n",
            "      - 29\n",
            "      - 31\n",
            "      - 132\n",
            "      - 9\n",
            "      - 27\n",
            "      - 99\n",
            "      - 23\n",
            "      - 50\n",
            "      - 33\n",
            "      - 13\n",
            "      - 30\n",
            "      - 14\n",
            "      - 15\n",
            "      - 43\n",
            "      - 197\n",
            "      - 41\n",
            "      - 64\n",
            "      - 107\n",
            "      - 146\n",
            "      - 159\n",
            "      - 200\n",
            "      - 137\n",
            "      - 200\n",
            "      - 22\n",
            "      - 167\n",
            "      - 30\n",
            "      - 144\n",
            "      - 200\n",
            "      - 116\n",
            "      - 56\n",
            "      - 72\n",
            "      - 146\n",
            "      - 52\n",
            "      - 118\n",
            "      - 33\n",
            "      - 105\n",
            "      - 175\n",
            "      - 31\n",
            "      - 86\n",
            "      - 125\n",
            "      - 200\n",
            "      - 69\n",
            "      - 41\n",
            "      - 134\n",
            "      - 82\n",
            "      - 73\n",
            "      - 198\n",
            "      - 119\n",
            "      - 31\n",
            "      - 103\n",
            "      - 187\n",
            "      - 100\n",
            "      - 182\n",
            "      - 200\n",
            "      - 188\n",
            "      - 98\n",
            "      - 51\n",
            "      - 200\n",
            "      - 99\n",
            "      - 128\n",
            "      - 200\n",
            "      - 35\n",
            "      - 200\n",
            "      - 142\n",
            "      - 52\n",
            "      - 152\n",
            "      - 200\n",
            "      - 114\n",
            "      - 134\n",
            "      - 185\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 88\n",
            "      episode_reward:\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 70.0\n",
            "      - 22.0\n",
            "      - 29.0\n",
            "      - 31.0\n",
            "      - 132.0\n",
            "      - 9.0\n",
            "      - 27.0\n",
            "      - 99.0\n",
            "      - 23.0\n",
            "      - 50.0\n",
            "      - 33.0\n",
            "      - 13.0\n",
            "      - 30.0\n",
            "      - 14.0\n",
            "      - 15.0\n",
            "      - 43.0\n",
            "      - 197.0\n",
            "      - 41.0\n",
            "      - 64.0\n",
            "      - 107.0\n",
            "      - 146.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 22.0\n",
            "      - 167.0\n",
            "      - 30.0\n",
            "      - 144.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 56.0\n",
            "      - 72.0\n",
            "      - 146.0\n",
            "      - 52.0\n",
            "      - 118.0\n",
            "      - 33.0\n",
            "      - 105.0\n",
            "      - 175.0\n",
            "      - 31.0\n",
            "      - 86.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 41.0\n",
            "      - 134.0\n",
            "      - 82.0\n",
            "      - 73.0\n",
            "      - 198.0\n",
            "      - 119.0\n",
            "      - 31.0\n",
            "      - 103.0\n",
            "      - 187.0\n",
            "      - 100.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 98.0\n",
            "      - 51.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 52.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 134.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10001418489534643\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0812353477255724\n",
            "      mean_inference_ms: 1.156487596278852\n",
            "      mean_raw_obs_processing_ms: 0.1516083632120969\n",
            "  time_since_restore: 45.083932399749756\n",
            "  time_this_iter_s: 7.411369800567627\n",
            "  time_total_s: 45.083932399749756\n",
            "  timers:\n",
            "    learn_throughput: 836.745\n",
            "    learn_time_ms: 4780.429\n",
            "    load_throughput: 9516288.145\n",
            "    load_time_ms: 0.42\n",
            "    training_iteration_time_ms: 7831.11\n",
            "    update_time_ms: 3.596\n",
            "  timestamp: 1656950212\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 20000\n",
            "  training_iteration: 5\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:56:58 (running for 00:01:04.77)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         45.0839</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  125.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            125.45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:03 (running for 00:01:09.84)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         45.0839</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  125.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">            125.45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 24000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 24000\n",
            "    num_agent_steps_trained: 24000\n",
            "    num_env_steps_sampled: 24000\n",
            "    num_env_steps_trained: 24000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-57-04\n",
            "  done: false\n",
            "  episode_len_mean: 155.05\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 155.05\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 374\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 188.8\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 188.8\n",
            "    episode_reward_min: 48.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 128\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 48\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 48.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07956937699556046\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06051120521978491\n",
            "      mean_inference_ms: 0.811943985486814\n",
            "      mean_raw_obs_processing_ms: 0.08473750418493295\n",
            "    timesteps_this_iter: 3776\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.15000000596046448\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5630147457122803\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005242129322141409\n",
            "          model: {}\n",
            "          policy_loss: -0.012592672370374203\n",
            "          total_loss: 9.411340713500977\n",
            "          vf_explained_var: 0.05192869156599045\n",
            "          vf_loss: 9.42314624786377\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 24000\n",
            "    num_agent_steps_trained: 24000\n",
            "    num_env_steps_sampled: 24000\n",
            "    num_env_steps_trained: 24000\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 24000\n",
            "  num_agent_steps_trained: 24000\n",
            "  num_env_steps_sampled: 24000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 24000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.4625\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10086836002223601\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08135353619947545\n",
            "    mean_inference_ms: 1.1679880021288778\n",
            "    mean_raw_obs_processing_ms: 0.15035904229056107\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 155.05\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 155.05\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 146\n",
            "      - 159\n",
            "      - 200\n",
            "      - 137\n",
            "      - 200\n",
            "      - 22\n",
            "      - 167\n",
            "      - 30\n",
            "      - 144\n",
            "      - 200\n",
            "      - 116\n",
            "      - 56\n",
            "      - 72\n",
            "      - 146\n",
            "      - 52\n",
            "      - 118\n",
            "      - 33\n",
            "      - 105\n",
            "      - 175\n",
            "      - 31\n",
            "      - 86\n",
            "      - 125\n",
            "      - 200\n",
            "      - 69\n",
            "      - 41\n",
            "      - 134\n",
            "      - 82\n",
            "      - 73\n",
            "      - 198\n",
            "      - 119\n",
            "      - 31\n",
            "      - 103\n",
            "      - 187\n",
            "      - 100\n",
            "      - 182\n",
            "      - 200\n",
            "      - 188\n",
            "      - 98\n",
            "      - 51\n",
            "      - 200\n",
            "      - 99\n",
            "      - 128\n",
            "      - 200\n",
            "      - 35\n",
            "      - 200\n",
            "      - 142\n",
            "      - 52\n",
            "      - 152\n",
            "      - 200\n",
            "      - 114\n",
            "      - 134\n",
            "      - 185\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 14\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 169\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 98\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 146.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 22.0\n",
            "      - 167.0\n",
            "      - 30.0\n",
            "      - 144.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 56.0\n",
            "      - 72.0\n",
            "      - 146.0\n",
            "      - 52.0\n",
            "      - 118.0\n",
            "      - 33.0\n",
            "      - 105.0\n",
            "      - 175.0\n",
            "      - 31.0\n",
            "      - 86.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 41.0\n",
            "      - 134.0\n",
            "      - 82.0\n",
            "      - 73.0\n",
            "      - 198.0\n",
            "      - 119.0\n",
            "      - 31.0\n",
            "      - 103.0\n",
            "      - 187.0\n",
            "      - 100.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 98.0\n",
            "      - 51.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 52.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 134.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 14.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10086836002223601\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08135353619947545\n",
            "      mean_inference_ms: 1.1679880021288778\n",
            "      mean_raw_obs_processing_ms: 0.15035904229056107\n",
            "  time_since_restore: 56.37036967277527\n",
            "  time_this_iter_s: 11.286437273025513\n",
            "  time_total_s: 56.37036967277527\n",
            "  timers:\n",
            "    learn_throughput: 840.546\n",
            "    learn_time_ms: 4758.812\n",
            "    load_throughput: 9897089.372\n",
            "    load_time_ms: 0.404\n",
            "    training_iteration_time_ms: 7756.142\n",
            "    update_time_ms: 3.58\n",
            "  timestamp: 1656950224\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 24000\n",
            "  training_iteration: 6\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:09 (running for 00:01:16.08)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         56.3704</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  155.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            155.05</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 28000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 28000\n",
            "    num_agent_steps_trained: 28000\n",
            "    num_env_steps_sampled: 28000\n",
            "    num_env_steps_trained: 28000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-57-11\n",
            "  done: false\n",
            "  episode_len_mean: 171.96\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 171.96\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 394\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.15000000596046448\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5769367218017578\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003913264721632004\n",
            "          model: {}\n",
            "          policy_loss: -0.010219834744930267\n",
            "          total_loss: 9.43729305267334\n",
            "          vf_explained_var: 0.03722012788057327\n",
            "          vf_loss: 9.44692611694336\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 28000\n",
            "    num_agent_steps_trained: 28000\n",
            "    num_env_steps_sampled: 28000\n",
            "    num_env_steps_trained: 28000\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 28000\n",
            "  num_agent_steps_trained: 28000\n",
            "  num_env_steps_sampled: 28000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 28000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.41818181818181\n",
            "    ram_util_percent: 21.900000000000002\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10079270872196563\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08116531760618863\n",
            "    mean_inference_ms: 1.1461849977018705\n",
            "    mean_raw_obs_processing_ms: 0.14847491742812724\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 171.96\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 171.96\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 86\n",
            "      - 125\n",
            "      - 200\n",
            "      - 69\n",
            "      - 41\n",
            "      - 134\n",
            "      - 82\n",
            "      - 73\n",
            "      - 198\n",
            "      - 119\n",
            "      - 31\n",
            "      - 103\n",
            "      - 187\n",
            "      - 100\n",
            "      - 182\n",
            "      - 200\n",
            "      - 188\n",
            "      - 98\n",
            "      - 51\n",
            "      - 200\n",
            "      - 99\n",
            "      - 128\n",
            "      - 200\n",
            "      - 35\n",
            "      - 200\n",
            "      - 142\n",
            "      - 52\n",
            "      - 152\n",
            "      - 200\n",
            "      - 114\n",
            "      - 134\n",
            "      - 185\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 14\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 169\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 98\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 86.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 41.0\n",
            "      - 134.0\n",
            "      - 82.0\n",
            "      - 73.0\n",
            "      - 198.0\n",
            "      - 119.0\n",
            "      - 31.0\n",
            "      - 103.0\n",
            "      - 187.0\n",
            "      - 100.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 98.0\n",
            "      - 51.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 52.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 134.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 14.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10079270872196563\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08116531760618863\n",
            "      mean_inference_ms: 1.1461849977018705\n",
            "      mean_raw_obs_processing_ms: 0.14847491742812724\n",
            "  time_since_restore: 63.81508135795593\n",
            "  time_this_iter_s: 7.444711685180664\n",
            "  time_total_s: 63.81508135795593\n",
            "  timers:\n",
            "    learn_throughput: 841.487\n",
            "    learn_time_ms: 4753.49\n",
            "    load_throughput: 10081596.017\n",
            "    load_time_ms: 0.397\n",
            "    training_iteration_time_ms: 7711.194\n",
            "    update_time_ms: 4.12\n",
            "  timestamp: 1656950231\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 28000\n",
            "  training_iteration: 7\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:16 (running for 00:01:23.70)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         63.8151</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  171.96</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            171.96</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:21 (running for 00:01:28.72)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         63.8151</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  171.96</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            171.96</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 32000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 32000\n",
            "    num_agent_steps_trained: 32000\n",
            "    num_env_steps_sampled: 32000\n",
            "    num_env_steps_trained: 32000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-57-23\n",
            "  done: false\n",
            "  episode_len_mean: 185.51\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 185.51\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 415\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 197.5\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 197.5\n",
            "    episode_reward_min: 151.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 151\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 199\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07996237362098665\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060707236746556224\n",
            "      mean_inference_ms: 0.8125269561536209\n",
            "      mean_raw_obs_processing_ms: 0.0845293725973993\n",
            "    timesteps_this_iter: 3950\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.07500000298023224\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5252103209495544\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0044493223540484905\n",
            "          model: {}\n",
            "          policy_loss: -0.006326364818960428\n",
            "          total_loss: 9.406146049499512\n",
            "          vf_explained_var: 0.02096382901072502\n",
            "          vf_loss: 9.412138938903809\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 32000\n",
            "    num_agent_steps_trained: 32000\n",
            "    num_env_steps_sampled: 32000\n",
            "    num_env_steps_trained: 32000\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 32000\n",
            "  num_agent_steps_trained: 32000\n",
            "  num_env_steps_sampled: 32000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 32000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.05625\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10054998748539022\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08092764749883145\n",
            "    mean_inference_ms: 1.1246964561513104\n",
            "    mean_raw_obs_processing_ms: 0.14662821022289135\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 185.51\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 185.51\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 128\n",
            "      - 200\n",
            "      - 35\n",
            "      - 200\n",
            "      - 142\n",
            "      - 52\n",
            "      - 152\n",
            "      - 200\n",
            "      - 114\n",
            "      - 134\n",
            "      - 185\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 14\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 169\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 98\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 200\n",
            "      - 173\n",
            "      - 200\n",
            "      - 86\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 52.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 134.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 14.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10054998748539022\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08092764749883145\n",
            "      mean_inference_ms: 1.1246964561513104\n",
            "      mean_raw_obs_processing_ms: 0.14662821022289135\n",
            "  time_since_restore: 75.35256838798523\n",
            "  time_this_iter_s: 11.537487030029297\n",
            "  time_total_s: 75.35256838798523\n",
            "  timers:\n",
            "    learn_throughput: 843.08\n",
            "    learn_time_ms: 4744.509\n",
            "    load_throughput: 10042478.713\n",
            "    load_time_ms: 0.398\n",
            "    training_iteration_time_ms: 7672.94\n",
            "    update_time_ms: 4.127\n",
            "  timestamp: 1656950243\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 32000\n",
            "  training_iteration: 8\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:28 (running for 00:01:35.28)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         75.3526</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  185.51</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            185.51</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 36000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 36000\n",
            "    num_agent_steps_trained: 36000\n",
            "    num_env_steps_sampled: 36000\n",
            "    num_env_steps_trained: 36000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-57-30\n",
            "  done: false\n",
            "  episode_len_mean: 192.21\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 192.21\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 435\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.03750000149011612\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5285487771034241\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0018967402866110206\n",
            "          model: {}\n",
            "          policy_loss: -0.0043160393834114075\n",
            "          total_loss: 9.340112686157227\n",
            "          vf_explained_var: 0.01880650222301483\n",
            "          vf_loss: 9.344358444213867\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 36000\n",
            "    num_agent_steps_trained: 36000\n",
            "    num_env_steps_sampled: 36000\n",
            "    num_env_steps_trained: 36000\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 36000\n",
            "  num_agent_steps_trained: 36000\n",
            "  num_env_steps_sampled: 36000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 36000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.49999999999999\n",
            "    ram_util_percent: 21.900000000000002\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.10017214355126121\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08069829518872183\n",
            "    mean_inference_ms: 1.108243776972812\n",
            "    mean_raw_obs_processing_ms: 0.14496952569602992\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 192.21\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 192.21\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 192\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 14\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 169\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 98\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 200\n",
            "      - 173\n",
            "      - 200\n",
            "      - 86\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 187\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 14.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10017214355126121\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08069829518872183\n",
            "      mean_inference_ms: 1.108243776972812\n",
            "      mean_raw_obs_processing_ms: 0.14496952569602992\n",
            "  time_since_restore: 82.64619636535645\n",
            "  time_this_iter_s: 7.293627977371216\n",
            "  time_total_s: 82.64619636535645\n",
            "  timers:\n",
            "    learn_throughput: 846.145\n",
            "    learn_time_ms: 4727.324\n",
            "    load_throughput: 10374807.201\n",
            "    load_time_ms: 0.386\n",
            "    training_iteration_time_ms: 7630.29\n",
            "    update_time_ms: 4.076\n",
            "  timestamp: 1656950250\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 36000\n",
            "  training_iteration: 9\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:35 (running for 00:01:42.44)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         82.6462</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  192.21</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            192.21</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:40 (running for 00:01:47.52)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         82.6462</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  192.21</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            192.21</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 40000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 40000\n",
            "    num_agent_steps_trained: 40000\n",
            "    num_env_steps_sampled: 40000\n",
            "    num_env_steps_trained: 40000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-57-41\n",
            "  done: false\n",
            "  episode_len_mean: 194.76\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 194.76\n",
            "  episode_reward_min: 86.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 456\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 186.65\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 186.65\n",
            "    episode_reward_min: 106.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 176\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 113\n",
            "      - 200\n",
            "      - 106\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 138\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 176.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 200.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07986808515161198\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060482802400837844\n",
            "      mean_inference_ms: 0.811016632992653\n",
            "      mean_raw_obs_processing_ms: 0.08417941439854229\n",
            "    timesteps_this_iter: 3733\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.01875000074505806\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5549140572547913\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002449640305712819\n",
            "          model: {}\n",
            "          policy_loss: -0.0038078727666288614\n",
            "          total_loss: 9.397706985473633\n",
            "          vf_explained_var: 0.03557699918746948\n",
            "          vf_loss: 9.401470184326172\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 40000\n",
            "    num_agent_steps_trained: 40000\n",
            "    num_env_steps_sampled: 40000\n",
            "    num_env_steps_trained: 40000\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 40000\n",
            "  num_agent_steps_trained: 40000\n",
            "  num_env_steps_sampled: 40000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 40000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.8375\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09992014710899022\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08051305270634915\n",
            "    mean_inference_ms: 1.0953849269895648\n",
            "    mean_raw_obs_processing_ms: 0.1436616791306037\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 194.76\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 194.76\n",
            "    episode_reward_min: 86.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 169\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 98\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 200\n",
            "      - 173\n",
            "      - 200\n",
            "      - 86\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 187\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09992014710899022\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08051305270634915\n",
            "      mean_inference_ms: 1.0953849269895648\n",
            "      mean_raw_obs_processing_ms: 0.1436616791306037\n",
            "  time_since_restore: 93.80483102798462\n",
            "  time_this_iter_s: 11.158634662628174\n",
            "  time_total_s: 93.80483102798462\n",
            "  timers:\n",
            "    learn_throughput: 848.165\n",
            "    learn_time_ms: 4716.061\n",
            "    load_throughput: 10662355.259\n",
            "    load_time_ms: 0.375\n",
            "    training_iteration_time_ms: 7596.884\n",
            "    update_time_ms: 4.065\n",
            "  timestamp: 1656950261\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 40000\n",
            "  training_iteration: 10\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:46 (running for 00:01:53.63)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         93.8048</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  194.76</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  86</td><td style=\"text-align: right;\">            194.76</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 44000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 44000\n",
            "    num_agent_steps_trained: 44000\n",
            "    num_env_steps_sampled: 44000\n",
            "    num_env_steps_trained: 44000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-57-49\n",
            "  done: false\n",
            "  episode_len_mean: 195.12\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 195.12\n",
            "  episode_reward_min: 86.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 476\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.00937500037252903\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5466403961181641\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003053300315514207\n",
            "          model: {}\n",
            "          policy_loss: -0.0022270584013313055\n",
            "          total_loss: 9.259286880493164\n",
            "          vf_explained_var: 0.05034446343779564\n",
            "          vf_loss: 9.261486053466797\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 44000\n",
            "    num_agent_steps_trained: 44000\n",
            "    num_env_steps_sampled: 44000\n",
            "    num_env_steps_trained: 44000\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 44000\n",
            "  num_agent_steps_trained: 44000\n",
            "  num_env_steps_sampled: 44000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 44000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.73\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09972068200827895\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08040180701621144\n",
            "    mean_inference_ms: 1.085158788218476\n",
            "    mean_raw_obs_processing_ms: 0.1426438927484131\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 195.12\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 195.12\n",
            "    episode_reward_min: 86.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 200\n",
            "      - 173\n",
            "      - 200\n",
            "      - 86\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 187\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 103\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09972068200827895\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08040180701621144\n",
            "      mean_inference_ms: 1.085158788218476\n",
            "      mean_raw_obs_processing_ms: 0.1426438927484131\n",
            "  time_since_restore: 101.09237718582153\n",
            "  time_this_iter_s: 7.287546157836914\n",
            "  time_total_s: 101.09237718582153\n",
            "  timers:\n",
            "    learn_throughput: 851.544\n",
            "    learn_time_ms: 4697.348\n",
            "    load_throughput: 10824708.691\n",
            "    load_time_ms: 0.37\n",
            "    training_iteration_time_ms: 7564.115\n",
            "    update_time_ms: 4.13\n",
            "  timestamp: 1656950269\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 44000\n",
            "  training_iteration: 11\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:54 (running for 00:02:01.10)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         101.092</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  195.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  86</td><td style=\"text-align: right;\">            195.12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:57:59 (running for 00:02:06.11)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         101.092</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  195.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  86</td><td style=\"text-align: right;\">            195.12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 48000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 48000\n",
            "    num_agent_steps_trained: 48000\n",
            "    num_env_steps_sampled: 48000\n",
            "    num_env_steps_trained: 48000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-58-00\n",
            "  done: false\n",
            "  episode_len_mean: 194.72\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 194.72\n",
            "  episode_reward_min: 86.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 496\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0798523213738247\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060427191704193174\n",
            "      mean_inference_ms: 0.8097884151664377\n",
            "      mean_raw_obs_processing_ms: 0.08406392244453585\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.004687500186264515\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5122678875923157\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005388234276324511\n",
            "          model: {}\n",
            "          policy_loss: -0.004982763901352882\n",
            "          total_loss: 9.229813575744629\n",
            "          vf_explained_var: 0.0925838053226471\n",
            "          vf_loss: 9.234771728515625\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 48000\n",
            "    num_agent_steps_trained: 48000\n",
            "    num_env_steps_sampled: 48000\n",
            "    num_env_steps_trained: 48000\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 48000\n",
            "  num_agent_steps_trained: 48000\n",
            "  num_env_steps_sampled: 48000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 48000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.30588235294117\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09950866591575613\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.080249679141739\n",
            "    mean_inference_ms: 1.0770769609032984\n",
            "    mean_raw_obs_processing_ms: 0.14184471314666605\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 194.72\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 194.72\n",
            "    episode_reward_min: 86.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 200\n",
            "      - 173\n",
            "      - 200\n",
            "      - 86\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 187\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 103\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 193\n",
            "      - 200\n",
            "      - 167\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09950866591575613\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.080249679141739\n",
            "      mean_inference_ms: 1.0770769609032984\n",
            "      mean_raw_obs_processing_ms: 0.14184471314666605\n",
            "  time_since_restore: 112.58097696304321\n",
            "  time_this_iter_s: 11.48859977722168\n",
            "  time_total_s: 112.58097696304321\n",
            "  timers:\n",
            "    learn_throughput: 858.9\n",
            "    learn_time_ms: 4657.121\n",
            "    load_throughput: 12810946.854\n",
            "    load_time_ms: 0.312\n",
            "    training_iteration_time_ms: 7518.897\n",
            "    update_time_ms: 4.182\n",
            "  timestamp: 1656950280\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 48000\n",
            "  training_iteration: 12\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:05 (running for 00:02:12.60)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         112.581</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  194.72</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  86</td><td style=\"text-align: right;\">            194.72</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 52000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 52000\n",
            "    num_agent_steps_trained: 52000\n",
            "    num_env_steps_sampled: 52000\n",
            "    num_env_steps_trained: 52000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-58-08\n",
            "  done: false\n",
            "  episode_len_mean: 197.4\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 197.4\n",
            "  episode_reward_min: 103.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 516\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.004687500186264515\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5175327658653259\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.012457579374313354\n",
            "          model: {}\n",
            "          policy_loss: -0.006326541304588318\n",
            "          total_loss: 9.387017250061035\n",
            "          vf_explained_var: 0.052794188261032104\n",
            "          vf_loss: 9.393285751342773\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 52000\n",
            "    num_agent_steps_trained: 52000\n",
            "    num_env_steps_sampled: 52000\n",
            "    num_env_steps_trained: 52000\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 52000\n",
            "  num_agent_steps_trained: 52000\n",
            "  num_env_steps_sampled: 52000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 52000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.53\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09929555658229663\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.08011923842623903\n",
            "    mean_inference_ms: 1.0704954362075976\n",
            "    mean_raw_obs_processing_ms: 0.14116220261496137\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 197.4\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 197.4\n",
            "    episode_reward_min: 103.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 187\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 103\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 193\n",
            "      - 200\n",
            "      - 167\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09929555658229663\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.08011923842623903\n",
            "      mean_inference_ms: 1.0704954362075976\n",
            "      mean_raw_obs_processing_ms: 0.14116220261496137\n",
            "  time_since_restore: 119.98742532730103\n",
            "  time_this_iter_s: 7.4064483642578125\n",
            "  time_total_s: 119.98742532730103\n",
            "  timers:\n",
            "    learn_throughput: 860.64\n",
            "    learn_time_ms: 4647.706\n",
            "    load_throughput: 12152119.368\n",
            "    load_time_ms: 0.329\n",
            "    training_iteration_time_ms: 7367.372\n",
            "    update_time_ms: 4.188\n",
            "  timestamp: 1656950288\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 52000\n",
            "  training_iteration: 13\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:13 (running for 00:02:19.90)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         119.987</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   197.4</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">             197.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:18 (running for 00:02:24.99)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         119.987</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   197.4</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">             197.4</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 56000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 56000\n",
            "    num_agent_steps_trained: 56000\n",
            "    num_env_steps_sampled: 56000\n",
            "    num_env_steps_trained: 56000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-58-19\n",
            "  done: false\n",
            "  episode_len_mean: 196.68\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 196.68\n",
            "  episode_reward_min: 103.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 536\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0798748867451324\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06028893607220852\n",
            "      mean_inference_ms: 0.8079946362265711\n",
            "      mean_raw_obs_processing_ms: 0.0838714977196652\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.004687500186264515\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5291156768798828\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.006872090511023998\n",
            "          model: {}\n",
            "          policy_loss: -0.007310028187930584\n",
            "          total_loss: 9.554701805114746\n",
            "          vf_explained_var: 0.18919256329536438\n",
            "          vf_loss: 9.561980247497559\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 56000\n",
            "    num_agent_steps_trained: 56000\n",
            "    num_env_steps_sampled: 56000\n",
            "    num_env_steps_trained: 56000\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 56000\n",
            "  num_agent_steps_trained: 56000\n",
            "  num_env_steps_sampled: 56000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 56000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.64117647058823\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09913096959554496\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07997135076515703\n",
            "    mean_inference_ms: 1.0651124637101652\n",
            "    mean_raw_obs_processing_ms: 0.14072652375290814\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 196.68\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 196.68\n",
            "    episode_reward_min: 103.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 103\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 193\n",
            "      - 200\n",
            "      - 167\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 175\n",
            "      - 199\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 175.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09913096959554496\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07997135076515703\n",
            "      mean_inference_ms: 1.0651124637101652\n",
            "      mean_raw_obs_processing_ms: 0.14072652375290814\n",
            "  time_since_restore: 131.49468612670898\n",
            "  time_this_iter_s: 11.507260799407959\n",
            "  time_total_s: 131.49468612670898\n",
            "  timers:\n",
            "    learn_throughput: 859.428\n",
            "    learn_time_ms: 4654.259\n",
            "    load_throughput: 11731498.497\n",
            "    load_time_ms: 0.341\n",
            "    training_iteration_time_ms: 7363.525\n",
            "    update_time_ms: 4.191\n",
            "  timestamp: 1656950299\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 56000\n",
            "  training_iteration: 14\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:24 (running for 00:02:31.43)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         131.495</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\">  196.68</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            196.68</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 60000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 60000\n",
            "    num_agent_steps_trained: 60000\n",
            "    num_env_steps_sampled: 60000\n",
            "    num_env_steps_trained: 60000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-58-27\n",
            "  done: false\n",
            "  episode_len_mean: 197.67\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 197.67\n",
            "  episode_reward_min: 103.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 556\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.004687500186264515\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5407613515853882\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00628024572506547\n",
            "          model: {}\n",
            "          policy_loss: -0.005156826227903366\n",
            "          total_loss: 9.583671569824219\n",
            "          vf_explained_var: 0.06470470130443573\n",
            "          vf_loss: 9.588798522949219\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 60000\n",
            "    num_agent_steps_trained: 60000\n",
            "    num_env_steps_sampled: 60000\n",
            "    num_env_steps_trained: 60000\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 60000\n",
            "  num_agent_steps_trained: 60000\n",
            "  num_env_steps_sampled: 60000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 60000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.81\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09893388608945093\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07984122185833305\n",
            "    mean_inference_ms: 1.0606409382763093\n",
            "    mean_raw_obs_processing_ms: 0.14029624866778875\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 197.67\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 197.67\n",
            "    episode_reward_min: 103.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 103\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 193\n",
            "      - 200\n",
            "      - 167\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 175\n",
            "      - 199\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 175.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09893388608945093\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07984122185833305\n",
            "      mean_inference_ms: 1.0606409382763093\n",
            "      mean_raw_obs_processing_ms: 0.14029624866778875\n",
            "  time_since_restore: 138.82547163963318\n",
            "  time_this_iter_s: 7.330785512924194\n",
            "  time_total_s: 138.82547163963318\n",
            "  timers:\n",
            "    learn_throughput: 860.054\n",
            "    learn_time_ms: 4650.87\n",
            "    load_throughput: 11936831.021\n",
            "    load_time_ms: 0.335\n",
            "    training_iteration_time_ms: 7355.457\n",
            "    update_time_ms: 4.251\n",
            "  timestamp: 1656950307\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 60000\n",
            "  training_iteration: 15\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:32 (running for 00:02:38.95)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         138.825</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  197.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            197.67</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:37 (running for 00:02:43.97)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         138.825</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  197.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 103</td><td style=\"text-align: right;\">            197.67</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 64000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 64000\n",
            "    num_agent_steps_trained: 64000\n",
            "    num_env_steps_sampled: 64000\n",
            "    num_env_steps_trained: 64000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-58-38\n",
            "  done: false\n",
            "  episode_len_mean: 198.6\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 198.6\n",
            "  episode_reward_min: 157.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 576\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 199.55\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 199.55\n",
            "    episode_reward_min: 191.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 191\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 191.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07985244479905323\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060341480160304564\n",
            "      mean_inference_ms: 0.8070822106177327\n",
            "      mean_raw_obs_processing_ms: 0.08387757851418684\n",
            "    timesteps_this_iter: 3991\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.004687500186264515\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5423096418380737\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.010048574768006802\n",
            "          model: {}\n",
            "          policy_loss: -0.006576772779226303\n",
            "          total_loss: 9.510848999023438\n",
            "          vf_explained_var: 0.0451180636882782\n",
            "          vf_loss: 9.517379760742188\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 64000\n",
            "    num_agent_steps_trained: 64000\n",
            "    num_env_steps_sampled: 64000\n",
            "    num_env_steps_trained: 64000\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 64000\n",
            "  num_agent_steps_trained: 64000\n",
            "  num_env_steps_sampled: 64000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 64000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.4875\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0987843329800409\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07972830250695928\n",
            "    mean_inference_ms: 1.0571106460248496\n",
            "    mean_raw_obs_processing_ms: 0.13996453917807983\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 198.6\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 198.6\n",
            "    episode_reward_min: 157.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 193\n",
            "      - 200\n",
            "      - 167\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 175\n",
            "      - 199\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 196\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 175.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0987843329800409\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07972830250695928\n",
            "      mean_inference_ms: 1.0571106460248496\n",
            "      mean_raw_obs_processing_ms: 0.13996453917807983\n",
            "  time_since_restore: 150.27904653549194\n",
            "  time_this_iter_s: 11.453574895858765\n",
            "  time_total_s: 150.27904653549194\n",
            "  timers:\n",
            "    learn_throughput: 860.698\n",
            "    learn_time_ms: 4647.389\n",
            "    load_throughput: 12113513.357\n",
            "    load_time_ms: 0.33\n",
            "    training_iteration_time_ms: 7350.454\n",
            "    update_time_ms: 4.26\n",
            "  timestamp: 1656950318\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 64000\n",
            "  training_iteration: 16\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:43 (running for 00:02:50.41)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         150.279</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">   198.6</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 157</td><td style=\"text-align: right;\">             198.6</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 68000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 68000\n",
            "    num_agent_steps_trained: 68000\n",
            "    num_env_steps_sampled: 68000\n",
            "    num_env_steps_trained: 68000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-58-45\n",
            "  done: false\n",
            "  episode_len_mean: 198.49\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 198.49\n",
            "  episode_reward_min: 157.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 596\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.004687500186264515\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5312400460243225\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0077537125907838345\n",
            "          model: {}\n",
            "          policy_loss: -0.008706937544047832\n",
            "          total_loss: 9.409144401550293\n",
            "          vf_explained_var: 0.03703566640615463\n",
            "          vf_loss: 9.417815208435059\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 68000\n",
            "    num_agent_steps_trained: 68000\n",
            "    num_env_steps_sampled: 68000\n",
            "    num_env_steps_trained: 68000\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 68000\n",
            "  num_agent_steps_trained: 68000\n",
            "  num_env_steps_sampled: 68000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 68000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.68181818181817\n",
            "    ram_util_percent: 21.900000000000002\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09869101646459807\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07964199482494999\n",
            "    mean_inference_ms: 1.054045721088566\n",
            "    mean_raw_obs_processing_ms: 0.13966635802046845\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 198.49\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 198.49\n",
            "    episode_reward_min: 157.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 175\n",
            "      - 199\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 196\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 194\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 184\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 175.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 184.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09869101646459807\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07964199482494999\n",
            "      mean_inference_ms: 1.054045721088566\n",
            "      mean_raw_obs_processing_ms: 0.13966635802046845\n",
            "  time_since_restore: 157.6505172252655\n",
            "  time_this_iter_s: 7.37147068977356\n",
            "  time_total_s: 157.6505172252655\n",
            "  timers:\n",
            "    learn_throughput: 862.239\n",
            "    learn_time_ms: 4639.086\n",
            "    load_throughput: 12117888.046\n",
            "    load_time_ms: 0.33\n",
            "    training_iteration_time_ms: 7342.582\n",
            "    update_time_ms: 3.9\n",
            "  timestamp: 1656950325\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 68000\n",
            "  training_iteration: 17\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:50 (running for 00:02:57.67)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         157.651</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  198.49</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 157</td><td style=\"text-align: right;\">            198.49</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:58:56 (running for 00:03:02.75)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         157.651</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  198.49</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 157</td><td style=\"text-align: right;\">            198.49</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 72000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 72000\n",
            "    num_agent_steps_trained: 72000\n",
            "    num_env_steps_sampled: 72000\n",
            "    num_env_steps_trained: 72000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-58-57\n",
            "  done: false\n",
            "  episode_len_mean: 198.39\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 198.39\n",
            "  episode_reward_min: 157.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 617\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07980352364909324\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06032643707484101\n",
            "      mean_inference_ms: 0.8070815499106079\n",
            "      mean_raw_obs_processing_ms: 0.08392145679439017\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.004687500186264515\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5161400437355042\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003469402901828289\n",
            "          model: {}\n",
            "          policy_loss: -0.005355875473469496\n",
            "          total_loss: 9.27637767791748\n",
            "          vf_explained_var: 0.01682479865849018\n",
            "          vf_loss: 9.281718254089355\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 72000\n",
            "    num_agent_steps_trained: 72000\n",
            "    num_env_steps_sampled: 72000\n",
            "    num_env_steps_trained: 72000\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 72000\n",
            "  num_agent_steps_trained: 72000\n",
            "  num_env_steps_sampled: 72000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 72000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.325\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09860024891931606\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07951952266042006\n",
            "    mean_inference_ms: 1.0509537176285064\n",
            "    mean_raw_obs_processing_ms: 0.13937169401129082\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 198.39\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 198.39\n",
            "    episode_reward_min: 157.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 198\n",
            "      - 175\n",
            "      - 199\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 196\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 194\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 184\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 175.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 184.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09860024891931606\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07951952266042006\n",
            "      mean_inference_ms: 1.0509537176285064\n",
            "      mean_raw_obs_processing_ms: 0.13937169401129082\n",
            "  time_since_restore: 169.12882161140442\n",
            "  time_this_iter_s: 11.478304386138916\n",
            "  time_total_s: 169.12882161140442\n",
            "  timers:\n",
            "    learn_throughput: 862.409\n",
            "    learn_time_ms: 4638.168\n",
            "    load_throughput: 12538088.334\n",
            "    load_time_ms: 0.319\n",
            "    training_iteration_time_ms: 7334.701\n",
            "    update_time_ms: 4.075\n",
            "  timestamp: 1656950337\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 72000\n",
            "  training_iteration: 18\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:02 (running for 00:03:09.18)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         169.129</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  198.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 157</td><td style=\"text-align: right;\">            198.39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 76000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 76000\n",
            "    num_agent_steps_trained: 76000\n",
            "    num_env_steps_sampled: 76000\n",
            "    num_env_steps_trained: 76000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-59-04\n",
            "  done: false\n",
            "  episode_len_mean: 199.24\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 199.24\n",
            "  episode_reward_min: 179.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 637\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0023437500931322575\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5190119743347168\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0064101493917405605\n",
            "          model: {}\n",
            "          policy_loss: -0.005057918839156628\n",
            "          total_loss: 9.097750663757324\n",
            "          vf_explained_var: -0.010796913877129555\n",
            "          vf_loss: 9.102794647216797\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 76000\n",
            "    num_agent_steps_trained: 76000\n",
            "    num_env_steps_sampled: 76000\n",
            "    num_env_steps_trained: 76000\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 76000\n",
            "  num_agent_steps_trained: 76000\n",
            "  num_env_steps_sampled: 76000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 76000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.92727272727272\n",
            "    ram_util_percent: 21.900000000000002\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09855046231477284\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07940823780111528\n",
            "    mean_inference_ms: 1.04829306210549\n",
            "    mean_raw_obs_processing_ms: 0.13902442760091802\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 199.24\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 199.24\n",
            "    episode_reward_min: 179.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 196\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 194\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 184\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 184.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09855046231477284\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07940823780111528\n",
            "      mean_inference_ms: 1.04829306210549\n",
            "      mean_raw_obs_processing_ms: 0.13902442760091802\n",
            "  time_since_restore: 176.44551730155945\n",
            "  time_this_iter_s: 7.316695690155029\n",
            "  time_total_s: 176.44551730155945\n",
            "  timers:\n",
            "    learn_throughput: 862.109\n",
            "    learn_time_ms: 4639.784\n",
            "    load_throughput: 12432171.916\n",
            "    load_time_ms: 0.322\n",
            "    training_iteration_time_ms: 7336.915\n",
            "    update_time_ms: 4.388\n",
            "  timestamp: 1656950344\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 76000\n",
            "  training_iteration: 19\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:09 (running for 00:03:16.71)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         176.446</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  199.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">            199.24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:14 (running for 00:03:21.72)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         176.446</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  199.24</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">            199.24</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 80000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 80000\n",
            "    num_agent_steps_trained: 80000\n",
            "    num_env_steps_sampled: 80000\n",
            "    num_env_steps_trained: 80000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-59-16\n",
            "  done: false\n",
            "  episode_len_mean: 199.28\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 199.28\n",
            "  episode_reward_min: 179.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 657\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07980565417760883\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06028929211334837\n",
            "      mean_inference_ms: 0.8066269386434429\n",
            "      mean_raw_obs_processing_ms: 0.08386416961170712\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0023437500931322575\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.5123928189277649\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0032984071876853704\n",
            "          model: {}\n",
            "          policy_loss: -0.000486412231111899\n",
            "          total_loss: 9.018834114074707\n",
            "          vf_explained_var: -0.020947085693478584\n",
            "          vf_loss: 9.019312858581543\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 80000\n",
            "    num_agent_steps_trained: 80000\n",
            "    num_env_steps_sampled: 80000\n",
            "    num_env_steps_trained: 80000\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 80000\n",
            "  num_agent_steps_trained: 80000\n",
            "  num_env_steps_sampled: 80000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 80000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.4\n",
            "    ram_util_percent: 21.9\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09850687471968562\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07932794972891853\n",
            "    mean_inference_ms: 1.0460179573264088\n",
            "    mean_raw_obs_processing_ms: 0.138786788830765\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 199.28\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 199.28\n",
            "    episode_reward_min: 179.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 194\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 184\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 184.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09850687471968562\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07932794972891853\n",
            "      mean_inference_ms: 1.0460179573264088\n",
            "      mean_raw_obs_processing_ms: 0.138786788830765\n",
            "  time_since_restore: 187.94005942344666\n",
            "  time_this_iter_s: 11.494542121887207\n",
            "  time_total_s: 187.94005942344666\n",
            "  timers:\n",
            "    learn_throughput: 861.435\n",
            "    learn_time_ms: 4643.417\n",
            "    load_throughput: 12174164.429\n",
            "    load_time_ms: 0.329\n",
            "    training_iteration_time_ms: 7343.852\n",
            "    update_time_ms: 4.369\n",
            "  timestamp: 1656950356\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 80000\n",
            "  training_iteration: 20\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:21 (running for 00:03:28.23)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">          187.94</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  199.28</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">            199.28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 84000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 84000\n",
            "    num_agent_steps_trained: 84000\n",
            "    num_env_steps_sampled: 84000\n",
            "    num_env_steps_trained: 84000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-59-23\n",
            "  done: false\n",
            "  episode_len_mean: 199.28\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 199.28\n",
            "  episode_reward_min: 179.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 677\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0011718750465661287\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.49018627405166626\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0025196883361786604\n",
            "          model: {}\n",
            "          policy_loss: 0.0017685670172795653\n",
            "          total_loss: 9.003631591796875\n",
            "          vf_explained_var: -0.07525413483381271\n",
            "          vf_loss: 9.001858711242676\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 84000\n",
            "    num_agent_steps_trained: 84000\n",
            "    num_env_steps_sampled: 84000\n",
            "    num_env_steps_trained: 84000\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 84000\n",
            "  num_agent_steps_trained: 84000\n",
            "  num_env_steps_sampled: 84000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 84000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.41818181818182\n",
            "    ram_util_percent: 21.900000000000002\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09844504574392463\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07926545040921312\n",
            "    mean_inference_ms: 1.04394328311274\n",
            "    mean_raw_obs_processing_ms: 0.138577688694979\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 199.28\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 199.28\n",
            "    episode_reward_min: 179.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 194\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 184\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 184.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09844504574392463\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07926545040921312\n",
            "      mean_inference_ms: 1.04394328311274\n",
            "      mean_raw_obs_processing_ms: 0.138577688694979\n",
            "  time_since_restore: 195.25618290901184\n",
            "  time_this_iter_s: 7.3161234855651855\n",
            "  time_total_s: 195.25618290901184\n",
            "  timers:\n",
            "    learn_throughput: 861.324\n",
            "    learn_time_ms: 4644.013\n",
            "    load_throughput: 11702857.143\n",
            "    load_time_ms: 0.342\n",
            "    training_iteration_time_ms: 7346.603\n",
            "    update_time_ms: 4.437\n",
            "  timestamp: 1656950363\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 84000\n",
            "  training_iteration: 21\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:28 (running for 00:03:35.39)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         195.256</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  199.28</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">            199.28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:33 (running for 00:03:40.47)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         195.256</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  199.28</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">            199.28</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 88000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 88000\n",
            "    num_agent_steps_trained: 88000\n",
            "    num_env_steps_sampled: 88000\n",
            "    num_env_steps_trained: 88000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-59-35\n",
            "  done: false\n",
            "  episode_len_mean: 199.79\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 199.79\n",
            "  episode_reward_min: 179.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 697\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0798877252215258\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060324454431859555\n",
            "      mean_inference_ms: 0.8072123357684259\n",
            "      mean_raw_obs_processing_ms: 0.08401946587467998\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0005859375232830644\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4955625534057617\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0073826913721859455\n",
            "          model: {}\n",
            "          policy_loss: -0.00017741136252880096\n",
            "          total_loss: 8.874384880065918\n",
            "          vf_explained_var: -0.017627613618969917\n",
            "          vf_loss: 8.874557495117188\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 88000\n",
            "    num_agent_steps_trained: 88000\n",
            "    num_env_steps_sampled: 88000\n",
            "    num_env_steps_trained: 88000\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 88000\n",
            "  num_agent_steps_trained: 88000\n",
            "  num_env_steps_sampled: 88000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 88000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.4125\n",
            "    ram_util_percent: 21.94375\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09837410463631892\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07921052095416972\n",
            "    mean_inference_ms: 1.0421385855355312\n",
            "    mean_raw_obs_processing_ms: 0.138431975891069\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 199.79\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 199.79\n",
            "    episode_reward_min: 179.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09837410463631892\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07921052095416972\n",
            "      mean_inference_ms: 1.0421385855355312\n",
            "      mean_raw_obs_processing_ms: 0.138431975891069\n",
            "  time_since_restore: 206.8645203113556\n",
            "  time_this_iter_s: 11.60833740234375\n",
            "  time_total_s: 206.8645203113556\n",
            "  timers:\n",
            "    learn_throughput: 860.75\n",
            "    learn_time_ms: 4647.112\n",
            "    load_throughput: 11699592.748\n",
            "    load_time_ms: 0.342\n",
            "    training_iteration_time_ms: 7354.654\n",
            "    update_time_ms: 4.399\n",
            "  timestamp: 1656950375\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 88000\n",
            "  training_iteration: 22\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:40 (running for 00:03:47.03)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         206.865</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  199.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 179</td><td style=\"text-align: right;\">            199.79</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 92000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 92000\n",
            "    num_agent_steps_trained: 92000\n",
            "    num_env_steps_sampled: 92000\n",
            "    num_env_steps_trained: 92000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-59-42\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 717\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0005859375232830644\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.48853909969329834\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0016956586623564363\n",
            "          model: {}\n",
            "          policy_loss: 0.0019845394417643547\n",
            "          total_loss: 8.729048728942871\n",
            "          vf_explained_var: -0.0028900853358209133\n",
            "          vf_loss: 8.727063179016113\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 92000\n",
            "    num_agent_steps_trained: 92000\n",
            "    num_env_steps_sampled: 92000\n",
            "    num_env_steps_trained: 92000\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 92000\n",
            "  num_agent_steps_trained: 92000\n",
            "  num_env_steps_sampled: 92000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 92000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.90909090909089\n",
            "    ram_util_percent: 21.981818181818184\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09832988976199097\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07920539867874826\n",
            "    mean_inference_ms: 1.0406844858672428\n",
            "    mean_raw_obs_processing_ms: 0.13835841186100464\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09832988976199097\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07920539867874826\n",
            "      mean_inference_ms: 1.0406844858672428\n",
            "      mean_raw_obs_processing_ms: 0.13835841186100464\n",
            "  time_since_restore: 214.2476463317871\n",
            "  time_this_iter_s: 7.3831260204315186\n",
            "  time_total_s: 214.2476463317871\n",
            "  timers:\n",
            "    learn_throughput: 861.114\n",
            "    learn_time_ms: 4645.143\n",
            "    load_throughput: 12136296.296\n",
            "    load_time_ms: 0.33\n",
            "    training_iteration_time_ms: 7352.855\n",
            "    update_time_ms: 4.379\n",
            "  timestamp: 1656950382\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 92000\n",
            "  training_iteration: 23\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:47 (running for 00:03:54.62)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         214.248</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:52 (running for 00:03:59.63)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         214.248</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 96000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 96000\n",
            "    num_agent_steps_trained: 96000\n",
            "    num_env_steps_sampled: 96000\n",
            "    num_env_steps_trained: 96000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_15-59-54\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 737\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07981837063092286\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06024557399965701\n",
            "      mean_inference_ms: 0.806458894129794\n",
            "      mean_raw_obs_processing_ms: 0.0839776331835454\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0002929687616415322\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4808996617794037\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0028743306174874306\n",
            "          model: {}\n",
            "          policy_loss: 0.0020438209176063538\n",
            "          total_loss: 8.841259002685547\n",
            "          vf_explained_var: -0.001318540656939149\n",
            "          vf_loss: 8.839216232299805\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 96000\n",
            "    num_agent_steps_trained: 96000\n",
            "    num_env_steps_sampled: 96000\n",
            "    num_env_steps_trained: 96000\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 96000\n",
            "  num_agent_steps_trained: 96000\n",
            "  num_env_steps_sampled: 96000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 96000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.3625\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09824559063653439\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07921273712327319\n",
            "    mean_inference_ms: 1.039434945911025\n",
            "    mean_raw_obs_processing_ms: 0.13829431497778621\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09824559063653439\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07921273712327319\n",
            "      mean_inference_ms: 1.039434945911025\n",
            "      mean_raw_obs_processing_ms: 0.13829431497778621\n",
            "  time_since_restore: 225.7088861465454\n",
            "  time_this_iter_s: 11.4612398147583\n",
            "  time_total_s: 225.7088861465454\n",
            "  timers:\n",
            "    learn_throughput: 861.815\n",
            "    learn_time_ms: 4641.365\n",
            "    load_throughput: 12943385.28\n",
            "    load_time_ms: 0.309\n",
            "    training_iteration_time_ms: 7348.629\n",
            "    update_time_ms: 4.735\n",
            "  timestamp: 1656950394\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 96000\n",
            "  training_iteration: 24\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 15:59:59 (running for 00:04:06.12)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         225.709</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 100000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 100000\n",
            "    num_agent_steps_trained: 100000\n",
            "    num_env_steps_sampled: 100000\n",
            "    num_env_steps_trained: 100000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-00-01\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 757\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 0.0001464843808207661\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4582532048225403\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004508816171437502\n",
            "          model: {}\n",
            "          policy_loss: 0.00035473325988277793\n",
            "          total_loss: 8.95980167388916\n",
            "          vf_explained_var: 0.001378778018988669\n",
            "          vf_loss: 8.959444999694824\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 100000\n",
            "    num_agent_steps_trained: 100000\n",
            "    num_env_steps_sampled: 100000\n",
            "    num_env_steps_trained: 100000\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 100000\n",
            "  num_agent_steps_trained: 100000\n",
            "  num_env_steps_sampled: 100000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 100000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.30909090909091\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09816143777936105\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07923739365889457\n",
            "    mean_inference_ms: 1.0383319137862923\n",
            "    mean_raw_obs_processing_ms: 0.13823464692755486\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09816143777936105\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07923739365889457\n",
            "      mean_inference_ms: 1.0383319137862923\n",
            "      mean_raw_obs_processing_ms: 0.13823464692755486\n",
            "  time_since_restore: 233.03765058517456\n",
            "  time_this_iter_s: 7.32876443862915\n",
            "  time_total_s: 233.03765058517456\n",
            "  timers:\n",
            "    learn_throughput: 862.534\n",
            "    learn_time_ms: 4637.498\n",
            "    load_throughput: 12961384.425\n",
            "    load_time_ms: 0.309\n",
            "    training_iteration_time_ms: 7348.77\n",
            "    update_time_ms: 4.625\n",
            "  timestamp: 1656950401\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 100000\n",
            "  training_iteration: 25\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:06 (running for 00:04:13.28)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         233.038</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:11 (running for 00:04:18.37)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         233.038</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 104000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 104000\n",
            "    num_agent_steps_trained: 104000\n",
            "    num_env_steps_sampled: 104000\n",
            "    num_env_steps_trained: 104000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-00-13\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 777\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07983874823546103\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06025082883324225\n",
            "      mean_inference_ms: 0.8061839536416346\n",
            "      mean_raw_obs_processing_ms: 0.08400960044983732\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 7.324219041038305e-05\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4820234179496765\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005158422514796257\n",
            "          model: {}\n",
            "          policy_loss: -0.0004880179767496884\n",
            "          total_loss: 9.071398735046387\n",
            "          vf_explained_var: 0.00031785995815880597\n",
            "          vf_loss: 9.071887016296387\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 104000\n",
            "    num_agent_steps_trained: 104000\n",
            "    num_env_steps_sampled: 104000\n",
            "    num_env_steps_trained: 104000\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 104000\n",
            "  num_agent_steps_trained: 104000\n",
            "  num_env_steps_sampled: 104000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 104000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.2625\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09810208221446474\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07926558703710633\n",
            "    mean_inference_ms: 1.037322232414677\n",
            "    mean_raw_obs_processing_ms: 0.13816721274223656\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09810208221446474\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07926558703710633\n",
            "      mean_inference_ms: 1.037322232414677\n",
            "      mean_raw_obs_processing_ms: 0.13816721274223656\n",
            "  time_since_restore: 244.5553638935089\n",
            "  time_this_iter_s: 11.51771330833435\n",
            "  time_total_s: 244.5553638935089\n",
            "  timers:\n",
            "    learn_throughput: 861.367\n",
            "    learn_time_ms: 4643.781\n",
            "    load_throughput: 12667786.167\n",
            "    load_time_ms: 0.316\n",
            "    training_iteration_time_ms: 7353.564\n",
            "    update_time_ms: 5.017\n",
            "  timestamp: 1656950413\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 104000\n",
            "  training_iteration: 26\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:18 (running for 00:04:24.83)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         244.555</td><td style=\"text-align: right;\">104000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 108000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 108000\n",
            "    num_agent_steps_trained: 108000\n",
            "    num_env_steps_sampled: 108000\n",
            "    num_env_steps_trained: 108000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-00-20\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 797\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 7.324219041038305e-05\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.48865872621536255\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0028396309353411198\n",
            "          model: {}\n",
            "          policy_loss: 0.003071364015340805\n",
            "          total_loss: 8.957419395446777\n",
            "          vf_explained_var: 0.0001584600395290181\n",
            "          vf_loss: 8.954349517822266\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 108000\n",
            "    num_agent_steps_trained: 108000\n",
            "    num_env_steps_sampled: 108000\n",
            "    num_env_steps_trained: 108000\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 108000\n",
            "  num_agent_steps_trained: 108000\n",
            "  num_env_steps_sampled: 108000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 108000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.14545454545454\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09803713257832722\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0792958728954436\n",
            "    mean_inference_ms: 1.036140095768556\n",
            "    mean_raw_obs_processing_ms: 0.13808999110456774\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09803713257832722\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0792958728954436\n",
            "      mean_inference_ms: 1.036140095768556\n",
            "      mean_raw_obs_processing_ms: 0.13808999110456774\n",
            "  time_since_restore: 251.7887303829193\n",
            "  time_this_iter_s: 7.2333664894104\n",
            "  time_total_s: 251.7887303829193\n",
            "  timers:\n",
            "    learn_throughput: 862.801\n",
            "    learn_time_ms: 4636.063\n",
            "    load_throughput: 12803125.763\n",
            "    load_time_ms: 0.312\n",
            "    training_iteration_time_ms: 7339.97\n",
            "    update_time_ms: 5.001\n",
            "  timestamp: 1656950420\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 108000\n",
            "  training_iteration: 27\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:25 (running for 00:04:32.24)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         251.789</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:30 (running for 00:04:37.26)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         251.789</td><td style=\"text-align: right;\">108000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 112000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 112000\n",
            "    num_agent_steps_trained: 112000\n",
            "    num_env_steps_sampled: 112000\n",
            "    num_env_steps_trained: 112000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-00-31\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 817\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07981358801298757\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06021240405384125\n",
            "      mean_inference_ms: 0.80601769829796\n",
            "      mean_raw_obs_processing_ms: 0.08388647094497595\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.662109520519152e-05\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4774397909641266\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004033371806144714\n",
            "          model: {}\n",
            "          policy_loss: 0.002714092144742608\n",
            "          total_loss: 7.6116766929626465\n",
            "          vf_explained_var: 0.00018718524370342493\n",
            "          vf_loss: 7.608962535858154\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 112000\n",
            "    num_agent_steps_trained: 112000\n",
            "    num_env_steps_sampled: 112000\n",
            "    num_env_steps_trained: 112000\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 112000\n",
            "  num_agent_steps_trained: 112000\n",
            "  num_env_steps_sampled: 112000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 112000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.5375\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09797002827931028\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0792845930916572\n",
            "    mean_inference_ms: 1.034927695946311\n",
            "    mean_raw_obs_processing_ms: 0.13795707084089462\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09797002827931028\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0792845930916572\n",
            "      mean_inference_ms: 1.034927695946311\n",
            "      mean_raw_obs_processing_ms: 0.13795707084089462\n",
            "  time_since_restore: 263.13091492652893\n",
            "  time_this_iter_s: 11.34218454360962\n",
            "  time_total_s: 263.13091492652893\n",
            "  timers:\n",
            "    learn_throughput: 864.945\n",
            "    learn_time_ms: 4624.574\n",
            "    load_throughput: 12685026.463\n",
            "    load_time_ms: 0.315\n",
            "    training_iteration_time_ms: 7328.747\n",
            "    update_time_ms: 4.764\n",
            "  timestamp: 1656950431\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 112000\n",
            "  training_iteration: 28\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:36 (running for 00:04:43.59)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         263.131</td><td style=\"text-align: right;\">112000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 116000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 116000\n",
            "    num_agent_steps_trained: 116000\n",
            "    num_env_steps_sampled: 116000\n",
            "    num_env_steps_trained: 116000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-00-39\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 837\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.831054760259576e-05\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.46439215540885925\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005948249250650406\n",
            "          model: {}\n",
            "          policy_loss: 0.0025708344765007496\n",
            "          total_loss: 5.304265022277832\n",
            "          vf_explained_var: 3.0425222576013766e-05\n",
            "          vf_loss: 5.301693916320801\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 116000\n",
            "    num_agent_steps_trained: 116000\n",
            "    num_env_steps_sampled: 116000\n",
            "    num_env_steps_trained: 116000\n",
            "  iterations_since_restore: 29\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 116000\n",
            "  num_agent_steps_trained: 116000\n",
            "  num_env_steps_sampled: 116000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 116000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.31\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09792257949372864\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07926223334195276\n",
            "    mean_inference_ms: 1.0337630693942366\n",
            "    mean_raw_obs_processing_ms: 0.13782938444637688\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09792257949372864\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07926223334195276\n",
            "      mean_inference_ms: 1.0337630693942366\n",
            "      mean_raw_obs_processing_ms: 0.13782938444637688\n",
            "  time_since_restore: 270.4912474155426\n",
            "  time_this_iter_s: 7.360332489013672\n",
            "  time_total_s: 270.4912474155426\n",
            "  timers:\n",
            "    learn_throughput: 864.06\n",
            "    learn_time_ms: 4629.306\n",
            "    load_throughput: 13200012.589\n",
            "    load_time_ms: 0.303\n",
            "    training_iteration_time_ms: 7333.061\n",
            "    update_time_ms: 4.772\n",
            "  timestamp: 1656950439\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 116000\n",
            "  training_iteration: 29\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:44 (running for 00:04:50.85)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         270.491</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:49 (running for 00:04:55.93)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         270.491</td><td style=\"text-align: right;\">116000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 120000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 120000\n",
            "    num_agent_steps_trained: 120000\n",
            "    num_env_steps_sampled: 120000\n",
            "    num_env_steps_trained: 120000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-00-50\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 857\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0797901130828467\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060173529242539756\n",
            "      mean_inference_ms: 0.8056283193563581\n",
            "      mean_raw_obs_processing_ms: 0.08383672576680684\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.831054760259576e-05\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4784185290336609\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003083910560235381\n",
            "          model: {}\n",
            "          policy_loss: 0.0038155587390065193\n",
            "          total_loss: 4.995626449584961\n",
            "          vf_explained_var: 0.00012879935093224049\n",
            "          vf_loss: 4.991811275482178\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 120000\n",
            "    num_agent_steps_trained: 120000\n",
            "    num_env_steps_sampled: 120000\n",
            "    num_env_steps_trained: 120000\n",
            "  iterations_since_restore: 30\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 120000\n",
            "  num_agent_steps_trained: 120000\n",
            "  num_env_steps_sampled: 120000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 120000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.73529411764706\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09789082918660429\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07922867443449391\n",
            "    mean_inference_ms: 1.0326918935975256\n",
            "    mean_raw_obs_processing_ms: 0.13769600701947138\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09789082918660429\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07922867443449391\n",
            "      mean_inference_ms: 1.0326918935975256\n",
            "      mean_raw_obs_processing_ms: 0.13769600701947138\n",
            "  time_since_restore: 281.9081127643585\n",
            "  time_this_iter_s: 11.416865348815918\n",
            "  time_total_s: 281.9081127643585\n",
            "  timers:\n",
            "    learn_throughput: 865.714\n",
            "    learn_time_ms: 4620.462\n",
            "    load_throughput: 13084710.654\n",
            "    load_time_ms: 0.306\n",
            "    training_iteration_time_ms: 7326.555\n",
            "    update_time_ms: 4.775\n",
            "  timestamp: 1656950450\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 120000\n",
            "  training_iteration: 30\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:00:55 (running for 00:05:02.30)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         281.908</td><td style=\"text-align: right;\">120000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 124000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 124000\n",
            "    num_agent_steps_trained: 124000\n",
            "    num_env_steps_sampled: 124000\n",
            "    num_env_steps_trained: 124000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-00-57\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 877\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 9.15527380129788e-06\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4917449355125427\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002611268777400255\n",
            "          model: {}\n",
            "          policy_loss: 0.005137159023433924\n",
            "          total_loss: 4.934508323669434\n",
            "          vf_explained_var: -0.00082964583998546\n",
            "          vf_loss: 4.929371356964111\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 124000\n",
            "    num_agent_steps_trained: 124000\n",
            "    num_env_steps_sampled: 124000\n",
            "    num_env_steps_trained: 124000\n",
            "  iterations_since_restore: 31\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 124000\n",
            "  num_agent_steps_trained: 124000\n",
            "  num_env_steps_sampled: 124000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 124000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.74999999999999\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09787239791707343\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0791730087680084\n",
            "    mean_inference_ms: 1.0317945809826552\n",
            "    mean_raw_obs_processing_ms: 0.1375667318571157\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09787239791707343\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0791730087680084\n",
            "      mean_inference_ms: 1.0317945809826552\n",
            "      mean_raw_obs_processing_ms: 0.1375667318571157\n",
            "  time_since_restore: 289.26910400390625\n",
            "  time_this_iter_s: 7.3609912395477295\n",
            "  time_total_s: 289.26910400390625\n",
            "  timers:\n",
            "    learn_throughput: 865.309\n",
            "    learn_time_ms: 4622.628\n",
            "    load_throughput: 13117448.006\n",
            "    load_time_ms: 0.305\n",
            "    training_iteration_time_ms: 7331.271\n",
            "    update_time_ms: 4.787\n",
            "  timestamp: 1656950457\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 124000\n",
            "  training_iteration: 31\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:03 (running for 00:05:09.87)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         289.269</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:08 (running for 00:05:14.88)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         289.269</td><td style=\"text-align: right;\">124000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 128000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 128000\n",
            "    num_agent_steps_trained: 128000\n",
            "    num_env_steps_sampled: 128000\n",
            "    num_env_steps_trained: 128000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-01-09\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 897\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07984522843983448\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06019720356152643\n",
            "      mean_inference_ms: 0.8068578340035296\n",
            "      mean_raw_obs_processing_ms: 0.08383536719207321\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.57763690064894e-06\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4751887619495392\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.008753044530749321\n",
            "          model: {}\n",
            "          policy_loss: 0.0028732598293572664\n",
            "          total_loss: 4.920022487640381\n",
            "          vf_explained_var: -0.00038457513437606394\n",
            "          vf_loss: 4.917149543762207\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 128000\n",
            "    num_agent_steps_trained: 128000\n",
            "    num_env_steps_sampled: 128000\n",
            "    num_env_steps_trained: 128000\n",
            "  iterations_since_restore: 32\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 128000\n",
            "  num_agent_steps_trained: 128000\n",
            "  num_env_steps_sampled: 128000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 128000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.2470588235294\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09786163614162088\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07912658031863433\n",
            "    mean_inference_ms: 1.0310072709172848\n",
            "    mean_raw_obs_processing_ms: 0.13741611838071346\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09786163614162088\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07912658031863433\n",
            "      mean_inference_ms: 1.0310072709172848\n",
            "      mean_raw_obs_processing_ms: 0.13741611838071346\n",
            "  time_since_restore: 300.763112783432\n",
            "  time_this_iter_s: 11.494008779525757\n",
            "  time_total_s: 300.763112783432\n",
            "  timers:\n",
            "    learn_throughput: 866.711\n",
            "    learn_time_ms: 4615.147\n",
            "    load_throughput: 13221858.302\n",
            "    load_time_ms: 0.303\n",
            "    training_iteration_time_ms: 7315.579\n",
            "    update_time_ms: 5.034\n",
            "  timestamp: 1656950469\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 128000\n",
            "  training_iteration: 32\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:14 (running for 00:05:21.37)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         300.763</td><td style=\"text-align: right;\">128000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 132000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 132000\n",
            "    num_agent_steps_trained: 132000\n",
            "    num_env_steps_sampled: 132000\n",
            "    num_env_steps_trained: 132000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-01-16\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 917\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.57763690064894e-06\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.481453537940979\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0028875914867967367\n",
            "          model: {}\n",
            "          policy_loss: 0.004487794358283281\n",
            "          total_loss: 4.919431686401367\n",
            "          vf_explained_var: 0.0013924470404163003\n",
            "          vf_loss: 4.914943695068359\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 132000\n",
            "    num_agent_steps_trained: 132000\n",
            "    num_env_steps_sampled: 132000\n",
            "    num_env_steps_trained: 132000\n",
            "  iterations_since_restore: 33\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 132000\n",
            "  num_agent_steps_trained: 132000\n",
            "  num_env_steps_sampled: 132000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 132000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.44\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09786768303287531\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07911259240447731\n",
            "    mean_inference_ms: 1.030508103442382\n",
            "    mean_raw_obs_processing_ms: 0.13733001544685877\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09786768303287531\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07911259240447731\n",
            "      mean_inference_ms: 1.030508103442382\n",
            "      mean_raw_obs_processing_ms: 0.13733001544685877\n",
            "  time_since_restore: 308.1175148487091\n",
            "  time_this_iter_s: 7.3544020652771\n",
            "  time_total_s: 308.1175148487091\n",
            "  timers:\n",
            "    learn_throughput: 868.02\n",
            "    learn_time_ms: 4608.188\n",
            "    load_throughput: 13137992.169\n",
            "    load_time_ms: 0.304\n",
            "    training_iteration_time_ms: 7312.385\n",
            "    update_time_ms: 5.067\n",
            "  timestamp: 1656950476\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 132000\n",
            "  training_iteration: 33\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:21 (running for 00:05:28.59)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         308.118</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:26 (running for 00:05:33.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         308.118</td><td style=\"text-align: right;\">132000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 136000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 136000\n",
            "    num_agent_steps_trained: 136000\n",
            "    num_env_steps_sampled: 136000\n",
            "    num_env_steps_trained: 136000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-01-28\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 937\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07988052006317337\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.06018560816726095\n",
            "      mean_inference_ms: 0.8062818549389924\n",
            "      mean_raw_obs_processing_ms: 0.08378105329542815\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.28881845032447e-06\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.46948859095573425\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00441032275557518\n",
            "          model: {}\n",
            "          policy_loss: 0.004655099473893642\n",
            "          total_loss: 4.919093608856201\n",
            "          vf_explained_var: -0.009812358766794205\n",
            "          vf_loss: 4.914438247680664\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 136000\n",
            "    num_agent_steps_trained: 136000\n",
            "    num_env_steps_sampled: 136000\n",
            "    num_env_steps_trained: 136000\n",
            "  iterations_since_restore: 34\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 136000\n",
            "  num_agent_steps_trained: 136000\n",
            "  num_env_steps_sampled: 136000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 136000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.37647058823529\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09785246232953262\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07910268834085667\n",
            "    mean_inference_ms: 1.0299828723036315\n",
            "    mean_raw_obs_processing_ms: 0.13722594761036228\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09785246232953262\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07910268834085667\n",
            "      mean_inference_ms: 1.0299828723036315\n",
            "      mean_raw_obs_processing_ms: 0.13722594761036228\n",
            "  time_since_restore: 319.4807870388031\n",
            "  time_this_iter_s: 11.363272190093994\n",
            "  time_total_s: 319.4807870388031\n",
            "  timers:\n",
            "    learn_throughput: 869.009\n",
            "    learn_time_ms: 4602.943\n",
            "    load_throughput: 12654409.413\n",
            "    load_time_ms: 0.316\n",
            "    training_iteration_time_ms: 7302.233\n",
            "    update_time_ms: 4.695\n",
            "  timestamp: 1656950488\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 136000\n",
            "  training_iteration: 34\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:33 (running for 00:05:39.99)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         319.481</td><td style=\"text-align: right;\">136000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 140000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 140000\n",
            "    num_agent_steps_trained: 140000\n",
            "    num_env_steps_sampled: 140000\n",
            "    num_env_steps_trained: 140000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-01-35\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 957\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.144409225162235e-06\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.47442272305488586\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0024017030373215675\n",
            "          model: {}\n",
            "          policy_loss: 0.004491694737225771\n",
            "          total_loss: 4.918842315673828\n",
            "          vf_explained_var: -0.01900451071560383\n",
            "          vf_loss: 4.9143500328063965\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 140000\n",
            "    num_agent_steps_trained: 140000\n",
            "    num_env_steps_sampled: 140000\n",
            "    num_env_steps_trained: 140000\n",
            "  iterations_since_restore: 35\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 140000\n",
            "  num_agent_steps_trained: 140000\n",
            "  num_env_steps_sampled: 140000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 140000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.92999999999999\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0978224449901081\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07909251975066656\n",
            "    mean_inference_ms: 1.0293498989582952\n",
            "    mean_raw_obs_processing_ms: 0.13712745921108088\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0978224449901081\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07909251975066656\n",
            "      mean_inference_ms: 1.0293498989582952\n",
            "      mean_raw_obs_processing_ms: 0.13712745921108088\n",
            "  time_since_restore: 326.74597239494324\n",
            "  time_this_iter_s: 7.265185356140137\n",
            "  time_total_s: 326.74597239494324\n",
            "  timers:\n",
            "    learn_throughput: 869.408\n",
            "    learn_time_ms: 4600.833\n",
            "    load_throughput: 12561557.353\n",
            "    load_time_ms: 0.318\n",
            "    training_iteration_time_ms: 7295.982\n",
            "    update_time_ms: 4.769\n",
            "  timestamp: 1656950495\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 140000\n",
            "  training_iteration: 35\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:40 (running for 00:05:47.44)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         326.746</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:45 (running for 00:05:52.46)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         326.746</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 144000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 144000\n",
            "    num_agent_steps_trained: 144000\n",
            "    num_env_steps_sampled: 144000\n",
            "    num_env_steps_trained: 144000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-01-46\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 977\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07981728199434852\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0601206048733907\n",
            "      mean_inference_ms: 0.805416254415347\n",
            "      mean_raw_obs_processing_ms: 0.08372701381858708\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 5.722046125811175e-07\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4426387548446655\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0027947365306317806\n",
            "          model: {}\n",
            "          policy_loss: 0.004838276654481888\n",
            "          total_loss: 4.919158458709717\n",
            "          vf_explained_var: -0.07970884442329407\n",
            "          vf_loss: 4.914320945739746\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 144000\n",
            "    num_agent_steps_trained: 144000\n",
            "    num_env_steps_sampled: 144000\n",
            "    num_env_steps_trained: 144000\n",
            "  iterations_since_restore: 36\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 144000\n",
            "  num_agent_steps_trained: 144000\n",
            "  num_env_steps_sampled: 144000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 144000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.45625\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0977703787136522\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07909093585435172\n",
            "    mean_inference_ms: 1.0286355546233805\n",
            "    mean_raw_obs_processing_ms: 0.13704892837621163\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0977703787136522\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07909093585435172\n",
            "      mean_inference_ms: 1.0286355546233805\n",
            "      mean_raw_obs_processing_ms: 0.13704892837621163\n",
            "  time_since_restore: 338.1999201774597\n",
            "  time_this_iter_s: 11.45394778251648\n",
            "  time_total_s: 338.1999201774597\n",
            "  timers:\n",
            "    learn_throughput: 869.42\n",
            "    learn_time_ms: 4600.771\n",
            "    load_throughput: 12776799.939\n",
            "    load_time_ms: 0.313\n",
            "    training_iteration_time_ms: 7296.015\n",
            "    update_time_ms: 4.649\n",
            "  timestamp: 1656950506\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 144000\n",
            "  training_iteration: 36\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:52 (running for 00:05:58.91)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">           338.2</td><td style=\"text-align: right;\">144000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 148000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 148000\n",
            "    num_agent_steps_trained: 148000\n",
            "    num_env_steps_sampled: 148000\n",
            "    num_env_steps_trained: 148000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-01-54\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 997\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.8610230629055877e-07\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.42139238119125366\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004334441851824522\n",
            "          model: {}\n",
            "          policy_loss: 0.004626174457371235\n",
            "          total_loss: 4.918943405151367\n",
            "          vf_explained_var: -0.134904682636261\n",
            "          vf_loss: 4.914316654205322\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 148000\n",
            "    num_agent_steps_trained: 148000\n",
            "    num_env_steps_sampled: 148000\n",
            "    num_env_steps_trained: 148000\n",
            "  iterations_since_restore: 37\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 148000\n",
            "  num_agent_steps_trained: 148000\n",
            "  num_env_steps_sampled: 148000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 148000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.19090909090909\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0977162941716167\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07908413429462931\n",
            "    mean_inference_ms: 1.0279176509901382\n",
            "    mean_raw_obs_processing_ms: 0.1369772763718765\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0977162941716167\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07908413429462931\n",
            "      mean_inference_ms: 1.0279176509901382\n",
            "      mean_raw_obs_processing_ms: 0.1369772763718765\n",
            "  time_since_restore: 345.46252942085266\n",
            "  time_this_iter_s: 7.262609243392944\n",
            "  time_total_s: 345.46252942085266\n",
            "  timers:\n",
            "    learn_throughput: 868.694\n",
            "    learn_time_ms: 4604.614\n",
            "    load_throughput: 12495133.686\n",
            "    load_time_ms: 0.32\n",
            "    training_iteration_time_ms: 7298.871\n",
            "    update_time_ms: 4.646\n",
            "  timestamp: 1656950514\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 148000\n",
            "  training_iteration: 37\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:01:59 (running for 00:06:06.05)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         345.463</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:04 (running for 00:06:11.13)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         345.463</td><td style=\"text-align: right;\">148000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 152000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 152000\n",
            "    num_agent_steps_trained: 152000\n",
            "    num_env_steps_sampled: 152000\n",
            "    num_env_steps_trained: 152000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-02-06\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1017\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07979611598710629\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060080437469210844\n",
            "      mean_inference_ms: 0.804958219345156\n",
            "      mean_raw_obs_processing_ms: 0.08366028822496824\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.4305115314527939e-07\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4555450975894928\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0023863997776061296\n",
            "          model: {}\n",
            "          policy_loss: 0.004841443616896868\n",
            "          total_loss: 4.9191575050354\n",
            "          vf_explained_var: -0.2073093056678772\n",
            "          vf_loss: 4.914315700531006\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 152000\n",
            "    num_agent_steps_trained: 152000\n",
            "    num_env_steps_sampled: 152000\n",
            "    num_env_steps_trained: 152000\n",
            "  iterations_since_restore: 38\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 152000\n",
            "  num_agent_steps_trained: 152000\n",
            "  num_env_steps_sampled: 152000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 152000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.36874999999999\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09764459990444378\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07906196086086034\n",
            "    mean_inference_ms: 1.0270307425300078\n",
            "    mean_raw_obs_processing_ms: 0.1368703718086477\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09764459990444378\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07906196086086034\n",
            "      mean_inference_ms: 1.0270307425300078\n",
            "      mean_raw_obs_processing_ms: 0.1368703718086477\n",
            "  time_since_restore: 357.2523703575134\n",
            "  time_this_iter_s: 11.789840936660767\n",
            "  time_total_s: 357.2523703575134\n",
            "  timers:\n",
            "    learn_throughput: 860.076\n",
            "    learn_time_ms: 4650.751\n",
            "    load_throughput: 13103105.28\n",
            "    load_time_ms: 0.305\n",
            "    training_iteration_time_ms: 7346.166\n",
            "    update_time_ms: 4.603\n",
            "  timestamp: 1656950526\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 152000\n",
            "  training_iteration: 38\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:11 (running for 00:06:17.87)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         357.252</td><td style=\"text-align: right;\">152000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 156000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 156000\n",
            "    num_agent_steps_trained: 156000\n",
            "    num_env_steps_sampled: 156000\n",
            "    num_env_steps_trained: 156000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-02-13\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1037\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 7.152557657263969e-08\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.44187459349632263\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0037294356152415276\n",
            "          model: {}\n",
            "          policy_loss: 0.004156321287155151\n",
            "          total_loss: 4.918471813201904\n",
            "          vf_explained_var: -0.22580289840698242\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 156000\n",
            "    num_agent_steps_trained: 156000\n",
            "    num_env_steps_sampled: 156000\n",
            "    num_env_steps_trained: 156000\n",
            "  iterations_since_restore: 39\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 156000\n",
            "  num_agent_steps_trained: 156000\n",
            "  num_env_steps_sampled: 156000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 156000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.03999999999999\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09761235420887436\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07906780265415538\n",
            "    mean_inference_ms: 1.0263564157418932\n",
            "    mean_raw_obs_processing_ms: 0.13679994239374782\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09761235420887436\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07906780265415538\n",
            "      mean_inference_ms: 1.0263564157418932\n",
            "      mean_raw_obs_processing_ms: 0.13679994239374782\n",
            "  time_since_restore: 364.645663022995\n",
            "  time_this_iter_s: 7.393292665481567\n",
            "  time_total_s: 364.645663022995\n",
            "  timers:\n",
            "    learn_throughput: 860.522\n",
            "    learn_time_ms: 4648.344\n",
            "    load_throughput: 12162691.025\n",
            "    load_time_ms: 0.329\n",
            "    training_iteration_time_ms: 7349.684\n",
            "    update_time_ms: 4.304\n",
            "  timestamp: 1656950533\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 156000\n",
            "  training_iteration: 39\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:18 (running for 00:06:25.45)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         364.646</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:23 (running for 00:06:30.46)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         364.646</td><td style=\"text-align: right;\">156000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 160000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 160000\n",
            "    num_agent_steps_trained: 160000\n",
            "    num_env_steps_sampled: 160000\n",
            "    num_env_steps_trained: 160000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-02-24\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1057\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07972092073078767\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.060038292045623956\n",
            "      mean_inference_ms: 0.804437443687887\n",
            "      mean_raw_obs_processing_ms: 0.08352182273091371\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.5762788286319847e-08\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4294241666793823\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0024635910522192717\n",
            "          model: {}\n",
            "          policy_loss: 0.005305583123117685\n",
            "          total_loss: 4.919620513916016\n",
            "          vf_explained_var: -0.2257922738790512\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 160000\n",
            "    num_agent_steps_trained: 160000\n",
            "    num_env_steps_sampled: 160000\n",
            "    num_env_steps_trained: 160000\n",
            "  iterations_since_restore: 40\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 160000\n",
            "  num_agent_steps_trained: 160000\n",
            "  num_env_steps_sampled: 160000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 160000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.59411764705884\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09759262699677619\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07907295555385722\n",
            "    mean_inference_ms: 1.0258065286016695\n",
            "    mean_raw_obs_processing_ms: 0.13673659736491361\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09759262699677619\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07907295555385722\n",
            "      mean_inference_ms: 1.0258065286016695\n",
            "      mean_raw_obs_processing_ms: 0.13673659736491361\n",
            "  time_since_restore: 376.02095222473145\n",
            "  time_this_iter_s: 11.37528920173645\n",
            "  time_total_s: 376.02095222473145\n",
            "  timers:\n",
            "    learn_throughput: 860.267\n",
            "    learn_time_ms: 4649.719\n",
            "    load_throughput: 12622040.325\n",
            "    load_time_ms: 0.317\n",
            "    training_iteration_time_ms: 7348.911\n",
            "    update_time_ms: 4.317\n",
            "  timestamp: 1656950544\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 160000\n",
            "  training_iteration: 40\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:30 (running for 00:06:36.82)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         376.021</td><td style=\"text-align: right;\">160000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 164000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 164000\n",
            "    num_agent_steps_trained: 164000\n",
            "    num_env_steps_sampled: 164000\n",
            "    num_env_steps_trained: 164000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-02-32\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1077\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.7881394143159923e-08\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4394739270210266\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00274359411559999\n",
            "          model: {}\n",
            "          policy_loss: 0.004714519251137972\n",
            "          total_loss: 4.919029712677002\n",
            "          vf_explained_var: -0.22580397129058838\n",
            "          vf_loss: 4.914315223693848\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 164000\n",
            "    num_agent_steps_trained: 164000\n",
            "    num_env_steps_sampled: 164000\n",
            "    num_env_steps_trained: 164000\n",
            "  iterations_since_restore: 41\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 164000\n",
            "  num_agent_steps_trained: 164000\n",
            "  num_env_steps_sampled: 164000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 164000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.56\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09756045603079512\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07906738158767779\n",
            "    mean_inference_ms: 1.025303376200733\n",
            "    mean_raw_obs_processing_ms: 0.13666258143639096\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09756045603079512\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07906738158767779\n",
            "      mean_inference_ms: 1.025303376200733\n",
            "      mean_raw_obs_processing_ms: 0.13666258143639096\n",
            "  time_since_restore: 383.26698541641235\n",
            "  time_this_iter_s: 7.246033191680908\n",
            "  time_total_s: 383.26698541641235\n",
            "  timers:\n",
            "    learn_throughput: 861.767\n",
            "    learn_time_ms: 4641.624\n",
            "    load_throughput: 12648685.163\n",
            "    load_time_ms: 0.316\n",
            "    training_iteration_time_ms: 7337.166\n",
            "    update_time_ms: 4.327\n",
            "  timestamp: 1656950552\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 164000\n",
            "  training_iteration: 41\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:37 (running for 00:06:43.97)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         383.267</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:42 (running for 00:06:49.05)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         383.267</td><td style=\"text-align: right;\">164000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 168000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 168000\n",
            "    num_agent_steps_trained: 168000\n",
            "    num_env_steps_sampled: 168000\n",
            "    num_env_steps_trained: 168000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-02-43\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1097\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0796337911137364\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059995314589116985\n",
            "      mean_inference_ms: 0.8042699048135702\n",
            "      mean_raw_obs_processing_ms: 0.08349716288285096\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.940697071579962e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4324289560317993\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005617681425064802\n",
            "          model: {}\n",
            "          policy_loss: 0.003317250171676278\n",
            "          total_loss: 4.917634963989258\n",
            "          vf_explained_var: -0.16504165530204773\n",
            "          vf_loss: 4.914317607879639\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 168000\n",
            "    num_agent_steps_trained: 168000\n",
            "    num_env_steps_sampled: 168000\n",
            "    num_env_steps_trained: 168000\n",
            "  iterations_since_restore: 42\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 168000\n",
            "  num_agent_steps_trained: 168000\n",
            "  num_env_steps_sampled: 168000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 168000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.15625\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09753628710182274\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07906687663360232\n",
            "    mean_inference_ms: 1.0248632371846986\n",
            "    mean_raw_obs_processing_ms: 0.1365984233660513\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09753628710182274\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07906687663360232\n",
            "      mean_inference_ms: 1.0248632371846986\n",
            "      mean_raw_obs_processing_ms: 0.1365984233660513\n",
            "  time_since_restore: 394.5889570713043\n",
            "  time_this_iter_s: 11.321971654891968\n",
            "  time_total_s: 394.5889570713043\n",
            "  timers:\n",
            "    learn_throughput: 862.945\n",
            "    learn_time_ms: 4635.291\n",
            "    load_throughput: 12675442.732\n",
            "    load_time_ms: 0.316\n",
            "    training_iteration_time_ms: 7331.168\n",
            "    update_time_ms: 4.294\n",
            "  timestamp: 1656950563\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 168000\n",
            "  training_iteration: 42\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:48 (running for 00:06:55.32)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         394.589</td><td style=\"text-align: right;\">168000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 172000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 172000\n",
            "    num_agent_steps_trained: 172000\n",
            "    num_env_steps_sampled: 172000\n",
            "    num_env_steps_trained: 172000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-02-50\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1117\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.940697071579962e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.43140438199043274\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.006400581449270248\n",
            "          model: {}\n",
            "          policy_loss: 0.004763480741530657\n",
            "          total_loss: 4.919111728668213\n",
            "          vf_explained_var: -0.22580485045909882\n",
            "          vf_loss: 4.914348125457764\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 172000\n",
            "    num_agent_steps_trained: 172000\n",
            "    num_env_steps_sampled: 172000\n",
            "    num_env_steps_trained: 172000\n",
            "  iterations_since_restore: 43\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 172000\n",
            "  num_agent_steps_trained: 172000\n",
            "  num_env_steps_sampled: 172000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 172000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.86363636363636\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09751549946375128\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07907373543206526\n",
            "    mean_inference_ms: 1.0244411496541366\n",
            "    mean_raw_obs_processing_ms: 0.1365342310077651\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09751549946375128\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07907373543206526\n",
            "      mean_inference_ms: 1.0244411496541366\n",
            "      mean_raw_obs_processing_ms: 0.1365342310077651\n",
            "  time_since_restore: 401.81700372695923\n",
            "  time_this_iter_s: 7.228046655654907\n",
            "  time_total_s: 401.81700372695923\n",
            "  timers:\n",
            "    learn_throughput: 863.303\n",
            "    learn_time_ms: 4633.368\n",
            "    load_throughput: 12594561.97\n",
            "    load_time_ms: 0.318\n",
            "    training_iteration_time_ms: 7318.484\n",
            "    update_time_ms: 4.277\n",
            "  timestamp: 1656950570\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 172000\n",
            "  training_iteration: 43\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:02:55 (running for 00:07:02.73)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         401.817</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:01 (running for 00:07:07.75)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         401.817</td><td style=\"text-align: right;\">172000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 176000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 176000\n",
            "    num_agent_steps_trained: 176000\n",
            "    num_env_steps_sampled: 176000\n",
            "    num_env_steps_trained: 176000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-03-02\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1137\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0795628023520997\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059927344992218926\n",
            "      mean_inference_ms: 0.8036662613692636\n",
            "      mean_raw_obs_processing_ms: 0.08337027523705604\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.940697071579962e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.417690247297287\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00625174306333065\n",
            "          model: {}\n",
            "          policy_loss: 0.004466102924197912\n",
            "          total_loss: 4.91878080368042\n",
            "          vf_explained_var: -0.22580623626708984\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 176000\n",
            "    num_agent_steps_trained: 176000\n",
            "    num_env_steps_sampled: 176000\n",
            "    num_env_steps_trained: 176000\n",
            "  iterations_since_restore: 44\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 176000\n",
            "  num_agent_steps_trained: 176000\n",
            "  num_env_steps_sampled: 176000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 176000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.36875\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09747162521022858\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07905117337422697\n",
            "    mean_inference_ms: 1.0238829940624958\n",
            "    mean_raw_obs_processing_ms: 0.13645259265370938\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09747162521022858\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07905117337422697\n",
            "      mean_inference_ms: 1.0238829940624958\n",
            "      mean_raw_obs_processing_ms: 0.13645259265370938\n",
            "  time_since_restore: 413.0456347465515\n",
            "  time_this_iter_s: 11.228631019592285\n",
            "  time_total_s: 413.0456347465515\n",
            "  timers:\n",
            "    learn_throughput: 865.087\n",
            "    learn_time_ms: 4623.812\n",
            "    load_throughput: 12965391.036\n",
            "    load_time_ms: 0.309\n",
            "    training_iteration_time_ms: 7309.817\n",
            "    update_time_ms: 4.189\n",
            "  timestamp: 1656950582\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 176000\n",
            "  training_iteration: 44\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:07 (running for 00:07:13.95)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         413.046</td><td style=\"text-align: right;\">176000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 180000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 180000\n",
            "    num_agent_steps_trained: 180000\n",
            "    num_env_steps_sampled: 180000\n",
            "    num_env_steps_trained: 180000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-03-09\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1157\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.940697071579962e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4385053813457489\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0023276149295270443\n",
            "          model: {}\n",
            "          policy_loss: 0.005867674946784973\n",
            "          total_loss: 4.920183181762695\n",
            "          vf_explained_var: -0.2216850072145462\n",
            "          vf_loss: 4.914315700531006\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 180000\n",
            "    num_agent_steps_trained: 180000\n",
            "    num_env_steps_sampled: 180000\n",
            "    num_env_steps_trained: 180000\n",
            "  iterations_since_restore: 45\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 180000\n",
            "  num_agent_steps_trained: 180000\n",
            "  num_env_steps_sampled: 180000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 180000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.67999999999999\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09742090800279309\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07902809582364427\n",
            "    mean_inference_ms: 1.0232019497538536\n",
            "    mean_raw_obs_processing_ms: 0.13635666243534356\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09742090800279309\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07902809582364427\n",
            "      mean_inference_ms: 1.0232019497538536\n",
            "      mean_raw_obs_processing_ms: 0.13635666243534356\n",
            "  time_since_restore: 420.202210187912\n",
            "  time_this_iter_s: 7.156575441360474\n",
            "  time_total_s: 420.202210187912\n",
            "  timers:\n",
            "    learn_throughput: 866.45\n",
            "    learn_time_ms: 4616.538\n",
            "    load_throughput: 13214568.368\n",
            "    load_time_ms: 0.303\n",
            "    training_iteration_time_ms: 7298.903\n",
            "    update_time_ms: 4.168\n",
            "  timestamp: 1656950589\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 180000\n",
            "  training_iteration: 45\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:14 (running for 00:07:21.01)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         420.202</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:19 (running for 00:07:26.10)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         420.202</td><td style=\"text-align: right;\">180000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 184000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 184000\n",
            "    num_agent_steps_trained: 184000\n",
            "    num_env_steps_sampled: 184000\n",
            "    num_env_steps_trained: 184000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-03-20\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1177\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07953493431743672\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05992656581643814\n",
            "      mean_inference_ms: 0.8036853773233033\n",
            "      mean_raw_obs_processing_ms: 0.0833836355355648\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.470348535789981e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4280950725078583\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00889511127024889\n",
            "          model: {}\n",
            "          policy_loss: 0.0015331180766224861\n",
            "          total_loss: 4.915848731994629\n",
            "          vf_explained_var: -0.22581042349338531\n",
            "          vf_loss: 4.914315223693848\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 184000\n",
            "    num_agent_steps_trained: 184000\n",
            "    num_env_steps_sampled: 184000\n",
            "    num_env_steps_trained: 184000\n",
            "  iterations_since_restore: 46\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 184000\n",
            "  num_agent_steps_trained: 184000\n",
            "  num_env_steps_sampled: 184000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 184000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.95625\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09737246306269398\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07899790845995186\n",
            "    mean_inference_ms: 1.0224523720977934\n",
            "    mean_raw_obs_processing_ms: 0.13624095093531852\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09737246306269398\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07899790845995186\n",
            "      mean_inference_ms: 1.0224523720977934\n",
            "      mean_raw_obs_processing_ms: 0.13624095093531852\n",
            "  time_since_restore: 431.49993324279785\n",
            "  time_this_iter_s: 11.297723054885864\n",
            "  time_total_s: 431.49993324279785\n",
            "  timers:\n",
            "    learn_throughput: 868.91\n",
            "    learn_time_ms: 4603.469\n",
            "    load_throughput: 12510973.9\n",
            "    load_time_ms: 0.32\n",
            "    training_iteration_time_ms: 7277.033\n",
            "    update_time_ms: 3.869\n",
            "  timestamp: 1656950600\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 184000\n",
            "  training_iteration: 46\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:25 (running for 00:07:32.34)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">           431.5</td><td style=\"text-align: right;\">184000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 188000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 188000\n",
            "    num_agent_steps_trained: 188000\n",
            "    num_env_steps_sampled: 188000\n",
            "    num_env_steps_trained: 188000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-03-27\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1197\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.470348535789981e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4294184148311615\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002660686383023858\n",
            "          model: {}\n",
            "          policy_loss: 0.004783129785209894\n",
            "          total_loss: 4.91911506652832\n",
            "          vf_explained_var: -0.021234335377812386\n",
            "          vf_loss: 4.914332389831543\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 188000\n",
            "    num_agent_steps_trained: 188000\n",
            "    num_env_steps_sampled: 188000\n",
            "    num_env_steps_trained: 188000\n",
            "  iterations_since_restore: 47\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 188000\n",
            "  num_agent_steps_trained: 188000\n",
            "  num_env_steps_sampled: 188000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 188000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.41\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09732160076086677\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07896106854236663\n",
            "    mean_inference_ms: 1.0217699157325657\n",
            "    mean_raw_obs_processing_ms: 0.13611971471759862\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09732160076086677\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07896106854236663\n",
            "      mean_inference_ms: 1.0217699157325657\n",
            "      mean_raw_obs_processing_ms: 0.13611971471759862\n",
            "  time_since_restore: 438.751668214798\n",
            "  time_this_iter_s: 7.251734972000122\n",
            "  time_total_s: 438.751668214798\n",
            "  timers:\n",
            "    learn_throughput: 869.675\n",
            "    learn_time_ms: 4599.417\n",
            "    load_throughput: 12365283.019\n",
            "    load_time_ms: 0.323\n",
            "    training_iteration_time_ms: 7276.267\n",
            "    update_time_ms: 3.809\n",
            "  timestamp: 1656950607\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 188000\n",
            "  training_iteration: 47\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:33 (running for 00:07:39.77)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         438.752</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:38 (running for 00:07:44.79)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         438.752</td><td style=\"text-align: right;\">188000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 192000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 192000\n",
            "    num_agent_steps_trained: 192000\n",
            "    num_env_steps_sampled: 192000\n",
            "    num_env_steps_trained: 192000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-03-39\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1217\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07952205384994489\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05992338863035226\n",
            "      mean_inference_ms: 0.8028054246077792\n",
            "      mean_raw_obs_processing_ms: 0.08334622164700982\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.2351742678949904e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.38432249426841736\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002595336642116308\n",
            "          model: {}\n",
            "          policy_loss: 0.005142217967659235\n",
            "          total_loss: 4.919479846954346\n",
            "          vf_explained_var: -0.003076363354921341\n",
            "          vf_loss: 4.914337158203125\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 192000\n",
            "    num_agent_steps_trained: 192000\n",
            "    num_env_steps_sampled: 192000\n",
            "    num_env_steps_trained: 192000\n",
            "  iterations_since_restore: 48\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 192000\n",
            "  num_agent_steps_trained: 192000\n",
            "  num_env_steps_sampled: 192000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 192000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.7235294117647\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09727218181699808\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07892842587240109\n",
            "    mean_inference_ms: 1.0212073026964676\n",
            "    mean_raw_obs_processing_ms: 0.13600047045689656\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09727218181699808\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07892842587240109\n",
            "      mean_inference_ms: 1.0212073026964676\n",
            "      mean_raw_obs_processing_ms: 0.13600047045689656\n",
            "  time_since_restore: 450.07255506515503\n",
            "  time_this_iter_s: 11.320886850357056\n",
            "  time_total_s: 450.07255506515503\n",
            "  timers:\n",
            "    learn_throughput: 878.438\n",
            "    learn_time_ms: 4553.539\n",
            "    load_throughput: 12364371.73\n",
            "    load_time_ms: 0.324\n",
            "    training_iteration_time_ms: 7235.223\n",
            "    update_time_ms: 3.894\n",
            "  timestamp: 1656950619\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 192000\n",
            "  training_iteration: 48\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:44 (running for 00:07:51.09)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         450.073</td><td style=\"text-align: right;\">192000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 196000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 196000\n",
            "    num_agent_steps_trained: 196000\n",
            "    num_env_steps_sampled: 196000\n",
            "    num_env_steps_trained: 196000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-03-46\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1237\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.1175871339474952e-09\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.39522621035575867\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0035726255737245083\n",
            "          model: {}\n",
            "          policy_loss: 0.004864916671067476\n",
            "          total_loss: 4.919183731079102\n",
            "          vf_explained_var: -0.01611431874334812\n",
            "          vf_loss: 4.914318084716797\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 196000\n",
            "    num_agent_steps_trained: 196000\n",
            "    num_env_steps_sampled: 196000\n",
            "    num_env_steps_trained: 196000\n",
            "  iterations_since_restore: 49\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 196000\n",
            "  num_agent_steps_trained: 196000\n",
            "  num_env_steps_sampled: 196000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 196000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.77\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09721227514598779\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07890094062475282\n",
            "    mean_inference_ms: 1.0206638150125875\n",
            "    mean_raw_obs_processing_ms: 0.1358737379306114\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09721227514598779\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07890094062475282\n",
            "      mean_inference_ms: 1.0206638150125875\n",
            "      mean_raw_obs_processing_ms: 0.1358737379306114\n",
            "  time_since_restore: 457.3470997810364\n",
            "  time_this_iter_s: 7.274544715881348\n",
            "  time_total_s: 457.3470997810364\n",
            "  timers:\n",
            "    learn_throughput: 878.81\n",
            "    learn_time_ms: 4551.607\n",
            "    load_throughput: 12719648.218\n",
            "    load_time_ms: 0.314\n",
            "    training_iteration_time_ms: 7223.357\n",
            "    update_time_ms: 3.878\n",
            "  timestamp: 1656950626\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 196000\n",
            "  training_iteration: 49\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:51 (running for 00:07:58.27)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         457.347</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:03:56 (running for 00:08:03.35)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         457.347</td><td style=\"text-align: right;\">196000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 200000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 200000\n",
            "    num_agent_steps_trained: 200000\n",
            "    num_env_steps_sampled: 200000\n",
            "    num_env_steps_trained: 200000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-03-57\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1257\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07943804825241366\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059849387050704375\n",
            "      mean_inference_ms: 0.8020017083335894\n",
            "      mean_raw_obs_processing_ms: 0.08323523128715418\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 5.587935669737476e-10\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.41068148612976074\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0021282059606164694\n",
            "          model: {}\n",
            "          policy_loss: 0.005856421776115894\n",
            "          total_loss: 4.920177459716797\n",
            "          vf_explained_var: -0.002753367181867361\n",
            "          vf_loss: 4.914321422576904\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 200000\n",
            "    num_agent_steps_trained: 200000\n",
            "    num_env_steps_sampled: 200000\n",
            "    num_env_steps_trained: 200000\n",
            "  iterations_since_restore: 50\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 200000\n",
            "  num_agent_steps_trained: 200000\n",
            "  num_env_steps_sampled: 200000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 200000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.25\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09715465165967344\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07888191897532253\n",
            "    mean_inference_ms: 1.0202268369749412\n",
            "    mean_raw_obs_processing_ms: 0.135753092798005\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09715465165967344\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07888191897532253\n",
            "      mean_inference_ms: 1.0202268369749412\n",
            "      mean_raw_obs_processing_ms: 0.135753092798005\n",
            "  time_since_restore: 468.6420273780823\n",
            "  time_this_iter_s: 11.294927597045898\n",
            "  time_total_s: 468.6420273780823\n",
            "  timers:\n",
            "    learn_throughput: 878.395\n",
            "    learn_time_ms: 4553.762\n",
            "    load_throughput: 12701352.108\n",
            "    load_time_ms: 0.315\n",
            "    training_iteration_time_ms: 7220.808\n",
            "    update_time_ms: 3.856\n",
            "  timestamp: 1656950637\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 200000\n",
            "  training_iteration: 50\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:02 (running for 00:08:09.59)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         468.642</td><td style=\"text-align: right;\">200000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 204000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 204000\n",
            "    num_agent_steps_trained: 204000\n",
            "    num_env_steps_sampled: 204000\n",
            "    num_env_steps_trained: 204000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-04-05\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1277\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.793967834868738e-10\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.402426540851593\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002950503723695874\n",
            "          model: {}\n",
            "          policy_loss: 0.004204849246889353\n",
            "          total_loss: 4.918520927429199\n",
            "          vf_explained_var: -0.18620829284191132\n",
            "          vf_loss: 4.914315700531006\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 204000\n",
            "    num_agent_steps_trained: 204000\n",
            "    num_env_steps_sampled: 204000\n",
            "    num_env_steps_trained: 204000\n",
            "  iterations_since_restore: 51\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 204000\n",
            "  num_agent_steps_trained: 204000\n",
            "  num_env_steps_sampled: 204000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 204000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.31818181818181\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09710423290451546\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07887717716845605\n",
            "    mean_inference_ms: 1.0198450115360942\n",
            "    mean_raw_obs_processing_ms: 0.13564890705002303\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09710423290451546\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07887717716845605\n",
            "      mean_inference_ms: 1.0198450115360942\n",
            "      mean_raw_obs_processing_ms: 0.13564890705002303\n",
            "  time_since_restore: 475.93037366867065\n",
            "  time_this_iter_s: 7.288346290588379\n",
            "  time_total_s: 475.93037366867065\n",
            "  timers:\n",
            "    learn_throughput: 876.769\n",
            "    learn_time_ms: 4562.205\n",
            "    load_throughput: 12804102.877\n",
            "    load_time_ms: 0.312\n",
            "    training_iteration_time_ms: 7225.268\n",
            "    update_time_ms: 4.236\n",
            "  timestamp: 1656950645\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 204000\n",
            "  training_iteration: 51\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:10 (running for 00:08:17.07)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">          475.93</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:15 (running for 00:08:22.08)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">          475.93</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 208000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 208000\n",
            "    num_agent_steps_trained: 208000\n",
            "    num_env_steps_sampled: 208000\n",
            "    num_env_steps_trained: 208000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-04-16\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1297\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07949248134713535\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0598688763431842\n",
            "      mean_inference_ms: 0.802108756984764\n",
            "      mean_raw_obs_processing_ms: 0.08324476261734268\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.396983917434369e-10\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3916770815849304\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002068673726171255\n",
            "          model: {}\n",
            "          policy_loss: 0.005906359292566776\n",
            "          total_loss: 4.920223236083984\n",
            "          vf_explained_var: -0.1688050925731659\n",
            "          vf_loss: 4.914316654205322\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 208000\n",
            "    num_agent_steps_trained: 208000\n",
            "    num_env_steps_sampled: 208000\n",
            "    num_env_steps_trained: 208000\n",
            "  iterations_since_restore: 52\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 208000\n",
            "  num_agent_steps_trained: 208000\n",
            "  num_env_steps_sampled: 208000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 208000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.96875\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09707188325853411\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0788802404718809\n",
            "    mean_inference_ms: 1.019505637328926\n",
            "    mean_raw_obs_processing_ms: 0.1355622921577735\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09707188325853411\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0788802404718809\n",
            "      mean_inference_ms: 1.019505637328926\n",
            "      mean_raw_obs_processing_ms: 0.1355622921577735\n",
            "  time_since_restore: 487.4682059288025\n",
            "  time_this_iter_s: 11.537832260131836\n",
            "  time_total_s: 487.4682059288025\n",
            "  timers:\n",
            "    learn_throughput: 874.063\n",
            "    learn_time_ms: 4576.327\n",
            "    load_throughput: 12828579.293\n",
            "    load_time_ms: 0.312\n",
            "    training_iteration_time_ms: 7243.83\n",
            "    update_time_ms: 4.42\n",
            "  timestamp: 1656950656\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 208000\n",
            "  training_iteration: 52\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:21 (running for 00:08:28.63)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         487.468</td><td style=\"text-align: right;\">208000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 212000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 212000\n",
            "    num_agent_steps_trained: 212000\n",
            "    num_env_steps_sampled: 212000\n",
            "    num_env_steps_trained: 212000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-04-23\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1317\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 6.984919587171845e-11\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3884844481945038\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003763652639463544\n",
            "          model: {}\n",
            "          policy_loss: 0.004656955134123564\n",
            "          total_loss: 4.918973445892334\n",
            "          vf_explained_var: -0.07387477904558182\n",
            "          vf_loss: 4.914316654205322\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 212000\n",
            "    num_agent_steps_trained: 212000\n",
            "    num_env_steps_sampled: 212000\n",
            "    num_env_steps_trained: 212000\n",
            "  iterations_since_restore: 53\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 212000\n",
            "  num_agent_steps_trained: 212000\n",
            "  num_env_steps_sampled: 212000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 212000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.53636363636363\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09703386772606622\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07888104123789065\n",
            "    mean_inference_ms: 1.0191051705688703\n",
            "    mean_raw_obs_processing_ms: 0.1354866430733125\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09703386772606622\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07888104123789065\n",
            "      mean_inference_ms: 1.0191051705688703\n",
            "      mean_raw_obs_processing_ms: 0.1354866430733125\n",
            "  time_since_restore: 494.68789768218994\n",
            "  time_this_iter_s: 7.219691753387451\n",
            "  time_total_s: 494.68789768218994\n",
            "  timers:\n",
            "    learn_throughput: 874.386\n",
            "    learn_time_ms: 4574.64\n",
            "    load_throughput: 12910516.352\n",
            "    load_time_ms: 0.31\n",
            "    training_iteration_time_ms: 7243.512\n",
            "    update_time_ms: 4.705\n",
            "  timestamp: 1656950663\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 212000\n",
            "  training_iteration: 53\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:28 (running for 00:08:35.73)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         494.688</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:34 (running for 00:08:40.82)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         494.688</td><td style=\"text-align: right;\">212000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 216000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 216000\n",
            "    num_agent_steps_trained: 216000\n",
            "    num_env_steps_sampled: 216000\n",
            "    num_env_steps_trained: 216000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-04-35\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1337\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07941415421890971\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059814903697666476\n",
            "      mean_inference_ms: 0.8014214569268139\n",
            "      mean_raw_obs_processing_ms: 0.08311892736916573\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.4924597935859225e-11\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3772267997264862\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0026977567467838526\n",
            "          model: {}\n",
            "          policy_loss: 0.0055508678779006\n",
            "          total_loss: 4.919866561889648\n",
            "          vf_explained_var: -0.2035634070634842\n",
            "          vf_loss: 4.914315700531006\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 216000\n",
            "    num_agent_steps_trained: 216000\n",
            "    num_env_steps_sampled: 216000\n",
            "    num_env_steps_trained: 216000\n",
            "  iterations_since_restore: 54\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 216000\n",
            "  num_agent_steps_trained: 216000\n",
            "  num_env_steps_sampled: 216000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 216000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.50666666666666\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0970017648981195\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07888684560391375\n",
            "    mean_inference_ms: 1.0186974569275338\n",
            "    mean_raw_obs_processing_ms: 0.135420767197493\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0970017648981195\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07888684560391375\n",
            "      mean_inference_ms: 1.0186974569275338\n",
            "      mean_raw_obs_processing_ms: 0.135420767197493\n",
            "  time_since_restore: 505.85055780410767\n",
            "  time_this_iter_s: 11.162660121917725\n",
            "  time_total_s: 505.85055780410767\n",
            "  timers:\n",
            "    learn_throughput: 874.218\n",
            "    learn_time_ms: 4575.519\n",
            "    load_throughput: 13381094.273\n",
            "    load_time_ms: 0.299\n",
            "    training_iteration_time_ms: 7241.043\n",
            "    update_time_ms: 4.719\n",
            "  timestamp: 1656950675\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 216000\n",
            "  training_iteration: 54\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:40 (running for 00:08:46.92)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         505.851</td><td style=\"text-align: right;\">216000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 220000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 220000\n",
            "    num_agent_steps_trained: 220000\n",
            "    num_env_steps_sampled: 220000\n",
            "    num_env_steps_trained: 220000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-04-42\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1357\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.7462298967929613e-11\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3899616301059723\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004135363735258579\n",
            "          model: {}\n",
            "          policy_loss: 0.004024996422231197\n",
            "          total_loss: 4.918347358703613\n",
            "          vf_explained_var: -0.07059445977210999\n",
            "          vf_loss: 4.9143218994140625\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 220000\n",
            "    num_agent_steps_trained: 220000\n",
            "    num_env_steps_sampled: 220000\n",
            "    num_env_steps_trained: 220000\n",
            "  iterations_since_restore: 55\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 220000\n",
            "  num_agent_steps_trained: 220000\n",
            "  num_env_steps_sampled: 220000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 220000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.5\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09697367016952965\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0788843565105475\n",
            "    mean_inference_ms: 1.0182667908010155\n",
            "    mean_raw_obs_processing_ms: 0.13534937574680744\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09697367016952965\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0788843565105475\n",
            "      mean_inference_ms: 1.0182667908010155\n",
            "      mean_raw_obs_processing_ms: 0.13534937574680744\n",
            "  time_since_restore: 513.0643928050995\n",
            "  time_this_iter_s: 7.213835000991821\n",
            "  time_total_s: 513.0643928050995\n",
            "  timers:\n",
            "    learn_throughput: 873.355\n",
            "    learn_time_ms: 4580.037\n",
            "    load_throughput: 13004585.691\n",
            "    load_time_ms: 0.308\n",
            "    training_iteration_time_ms: 7246.737\n",
            "    update_time_ms: 4.751\n",
            "  timestamp: 1656950682\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 220000\n",
            "  training_iteration: 55\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:47 (running for 00:08:54.32)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         513.064</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:52 (running for 00:08:59.33)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         513.064</td><td style=\"text-align: right;\">220000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 224000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 224000\n",
            "    num_agent_steps_trained: 224000\n",
            "    num_env_steps_sampled: 224000\n",
            "    num_env_steps_trained: 224000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-04-53\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1377\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07936077662946639\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05977711704575219\n",
            "      mean_inference_ms: 0.8012616857163433\n",
            "      mean_raw_obs_processing_ms: 0.08303656292169811\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.731149483964806e-12\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.39165791869163513\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002731750952079892\n",
            "          model: {}\n",
            "          policy_loss: 0.005001429934054613\n",
            "          total_loss: 4.919318675994873\n",
            "          vf_explained_var: -0.2258032262325287\n",
            "          vf_loss: 4.914316654205322\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 224000\n",
            "    num_agent_steps_trained: 224000\n",
            "    num_env_steps_sampled: 224000\n",
            "    num_env_steps_trained: 224000\n",
            "  iterations_since_restore: 56\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 224000\n",
            "  num_agent_steps_trained: 224000\n",
            "  num_env_steps_sampled: 224000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 224000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.89375\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09695169243572548\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07888438637816143\n",
            "    mean_inference_ms: 1.0179008920904757\n",
            "    mean_raw_obs_processing_ms: 0.13528510864836085\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09695169243572548\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07888438637816143\n",
            "      mean_inference_ms: 1.0179008920904757\n",
            "      mean_raw_obs_processing_ms: 0.13528510864836085\n",
            "  time_since_restore: 524.3540480136871\n",
            "  time_this_iter_s: 11.289655208587646\n",
            "  time_total_s: 524.3540480136871\n",
            "  timers:\n",
            "    learn_throughput: 873.881\n",
            "    learn_time_ms: 4577.283\n",
            "    load_throughput: 13666679.7\n",
            "    load_time_ms: 0.293\n",
            "    training_iteration_time_ms: 7251.274\n",
            "    update_time_ms: 5.044\n",
            "  timestamp: 1656950693\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 224000\n",
            "  training_iteration: 56\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:04:58 (running for 00:09:05.65)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         524.354</td><td style=\"text-align: right;\">224000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 228000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 228000\n",
            "    num_agent_steps_trained: 228000\n",
            "    num_env_steps_sampled: 228000\n",
            "    num_env_steps_trained: 228000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-05-00\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1397\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.365574741982403e-12\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3766637444496155\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0035351691767573357\n",
            "          model: {}\n",
            "          policy_loss: 0.004787706770002842\n",
            "          total_loss: 4.9191083908081055\n",
            "          vf_explained_var: -0.19784224033355713\n",
            "          vf_loss: 4.914320945739746\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 228000\n",
            "    num_agent_steps_trained: 228000\n",
            "    num_env_steps_sampled: 228000\n",
            "    num_env_steps_trained: 228000\n",
            "  iterations_since_restore: 57\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 228000\n",
            "  num_agent_steps_trained: 228000\n",
            "  num_env_steps_sampled: 228000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 228000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.31\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0969217308968397\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07887402752658336\n",
            "    mean_inference_ms: 1.0174867727916606\n",
            "    mean_raw_obs_processing_ms: 0.1352068650688312\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0969217308968397\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07887402752658336\n",
            "      mean_inference_ms: 1.0174867727916606\n",
            "      mean_raw_obs_processing_ms: 0.1352068650688312\n",
            "  time_since_restore: 531.5291249752045\n",
            "  time_this_iter_s: 7.175076961517334\n",
            "  time_total_s: 531.5291249752045\n",
            "  timers:\n",
            "    learn_throughput: 874.775\n",
            "    learn_time_ms: 4572.604\n",
            "    load_throughput: 13849443.619\n",
            "    load_time_ms: 0.289\n",
            "    training_iteration_time_ms: 7243.459\n",
            "    update_time_ms: 5.428\n",
            "  timestamp: 1656950700\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 228000\n",
            "  training_iteration: 57\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:05 (running for 00:09:12.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         531.529</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:11 (running for 00:09:17.75)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         531.529</td><td style=\"text-align: right;\">228000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 232000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 232000\n",
            "    num_agent_steps_trained: 232000\n",
            "    num_env_steps_sampled: 232000\n",
            "    num_env_steps_trained: 232000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-05-12\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1417\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07946180832891554\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05992000325669309\n",
            "      mean_inference_ms: 0.8034627764872437\n",
            "      mean_raw_obs_processing_ms: 0.08322446502309769\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.1827873709912016e-12\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3841533660888672\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0026896458584815264\n",
            "          model: {}\n",
            "          policy_loss: 0.0045525929890573025\n",
            "          total_loss: 4.918872356414795\n",
            "          vf_explained_var: -0.19371305406093597\n",
            "          vf_loss: 4.914319038391113\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 232000\n",
            "    num_agent_steps_trained: 232000\n",
            "    num_env_steps_sampled: 232000\n",
            "    num_env_steps_trained: 232000\n",
            "  iterations_since_restore: 58\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 232000\n",
            "  num_agent_steps_trained: 232000\n",
            "  num_env_steps_sampled: 232000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 232000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 84.21176470588235\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09689695708797919\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07885174449004709\n",
            "    mean_inference_ms: 1.0170765611398631\n",
            "    mean_raw_obs_processing_ms: 0.13512779985612797\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09689695708797919\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07885174449004709\n",
            "      mean_inference_ms: 1.0170765611398631\n",
            "      mean_raw_obs_processing_ms: 0.13512779985612797\n",
            "  time_since_restore: 543.0602881908417\n",
            "  time_this_iter_s: 11.531163215637207\n",
            "  time_total_s: 543.0602881908417\n",
            "  timers:\n",
            "    learn_throughput: 876.791\n",
            "    learn_time_ms: 4562.089\n",
            "    load_throughput: 13273113.924\n",
            "    load_time_ms: 0.301\n",
            "    training_iteration_time_ms: 7227.261\n",
            "    update_time_ms: 5.462\n",
            "  timestamp: 1656950712\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 232000\n",
            "  training_iteration: 58\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:17 (running for 00:09:24.27)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">          543.06</td><td style=\"text-align: right;\">232000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 236000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 236000\n",
            "    num_agent_steps_trained: 236000\n",
            "    num_env_steps_sampled: 236000\n",
            "    num_env_steps_trained: 236000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-05-20\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1437\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.0913936854956008e-12\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.37196075916290283\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0040566339157521725\n",
            "          model: {}\n",
            "          policy_loss: 0.00468147499486804\n",
            "          total_loss: 4.918996334075928\n",
            "          vf_explained_var: -0.22580504417419434\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 236000\n",
            "    num_agent_steps_trained: 236000\n",
            "    num_env_steps_sampled: 236000\n",
            "    num_env_steps_trained: 236000\n",
            "  iterations_since_restore: 59\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 236000\n",
            "  num_agent_steps_trained: 236000\n",
            "  num_env_steps_sampled: 236000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 236000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 92.55454545454546\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09690905543809865\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07883736324638208\n",
            "    mean_inference_ms: 1.0178852710041404\n",
            "    mean_raw_obs_processing_ms: 0.1350823525798627\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09690905543809865\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07883736324638208\n",
            "      mean_inference_ms: 1.0178852710041404\n",
            "      mean_raw_obs_processing_ms: 0.1350823525798627\n",
            "  time_since_restore: 551.0177054405212\n",
            "  time_this_iter_s: 7.957417249679565\n",
            "  time_total_s: 551.0177054405212\n",
            "  timers:\n",
            "    learn_throughput: 879.115\n",
            "    learn_time_ms: 4550.031\n",
            "    load_throughput: 13308913.216\n",
            "    load_time_ms: 0.301\n",
            "    training_iteration_time_ms: 7295.482\n",
            "    update_time_ms: 5.46\n",
            "  timestamp: 1656950720\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 236000\n",
            "  training_iteration: 59\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:25 (running for 00:09:32.40)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         551.018</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:30 (running for 00:09:37.43)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         551.018</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 240000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 240000\n",
            "    num_agent_steps_trained: 240000\n",
            "    num_env_steps_sampled: 240000\n",
            "    num_env_steps_trained: 240000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-05-31\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1457\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07942277820556456\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05986741730534687\n",
            "      mean_inference_ms: 0.8029254648003195\n",
            "      mean_raw_obs_processing_ms: 0.0831475108849192\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 5.456968427478004e-13\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.34957170486450195\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002275810344144702\n",
            "          model: {}\n",
            "          policy_loss: 0.005532447248697281\n",
            "          total_loss: 4.919849872589111\n",
            "          vf_explained_var: -0.2258051186800003\n",
            "          vf_loss: 4.9143171310424805\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 240000\n",
            "    num_agent_steps_trained: 240000\n",
            "    num_env_steps_sampled: 240000\n",
            "    num_env_steps_trained: 240000\n",
            "  iterations_since_restore: 60\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 240000\n",
            "  num_agent_steps_trained: 240000\n",
            "  num_env_steps_sampled: 240000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 240000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.1625\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09692220073343347\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0788155814829339\n",
            "    mean_inference_ms: 1.0186744115028743\n",
            "    mean_raw_obs_processing_ms: 0.13504764380462872\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09692220073343347\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0788155814829339\n",
            "      mean_inference_ms: 1.0186744115028743\n",
            "      mean_raw_obs_processing_ms: 0.13504764380462872\n",
            "  time_since_restore: 562.2343537807465\n",
            "  time_this_iter_s: 11.21664834022522\n",
            "  time_total_s: 562.2343537807465\n",
            "  timers:\n",
            "    learn_throughput: 880.41\n",
            "    learn_time_ms: 4543.336\n",
            "    load_throughput: 12889686.54\n",
            "    load_time_ms: 0.31\n",
            "    training_iteration_time_ms: 7285.857\n",
            "    update_time_ms: 5.847\n",
            "  timestamp: 1656950731\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 240000\n",
            "  training_iteration: 60\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:36 (running for 00:09:43.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         562.234</td><td style=\"text-align: right;\">240000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 244000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 244000\n",
            "    num_agent_steps_trained: 244000\n",
            "    num_env_steps_sampled: 244000\n",
            "    num_env_steps_trained: 244000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-05-38\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1477\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.728484213739002e-13\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.35464951395988464\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.005821479484438896\n",
            "          model: {}\n",
            "          policy_loss: 0.004545664880424738\n",
            "          total_loss: 4.918862342834473\n",
            "          vf_explained_var: -0.09056544303894043\n",
            "          vf_loss: 4.914316654205322\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 244000\n",
            "    num_agent_steps_trained: 244000\n",
            "    num_env_steps_sampled: 244000\n",
            "    num_env_steps_trained: 244000\n",
            "  iterations_since_restore: 61\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 244000\n",
            "  num_agent_steps_trained: 244000\n",
            "  num_env_steps_sampled: 244000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 244000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.03\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09692751354607633\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0787896751286812\n",
            "    mean_inference_ms: 1.0193986515250004\n",
            "    mean_raw_obs_processing_ms: 0.1350099842261793\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09692751354607633\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0787896751286812\n",
            "      mean_inference_ms: 1.0193986515250004\n",
            "      mean_raw_obs_processing_ms: 0.1350099842261793\n",
            "  time_since_restore: 569.4028189182281\n",
            "  time_this_iter_s: 7.1684651374816895\n",
            "  time_total_s: 569.4028189182281\n",
            "  timers:\n",
            "    learn_throughput: 882.468\n",
            "    learn_time_ms: 4532.74\n",
            "    load_throughput: 12929420.469\n",
            "    load_time_ms: 0.309\n",
            "    training_iteration_time_ms: 7273.397\n",
            "    update_time_ms: 5.415\n",
            "  timestamp: 1656950738\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 244000\n",
            "  training_iteration: 61\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:43 (running for 00:09:50.69)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         569.403</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:49 (running for 00:09:55.77)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         569.403</td><td style=\"text-align: right;\">244000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 248000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 248000\n",
            "    num_agent_steps_trained: 248000\n",
            "    num_env_steps_sampled: 248000\n",
            "    num_env_steps_trained: 248000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-05-50\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1497\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07939544906718315\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059824556497848944\n",
            "      mean_inference_ms: 0.8025081860889728\n",
            "      mean_raw_obs_processing_ms: 0.08312082258347112\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.728484213739002e-13\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.36946144700050354\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003409299999475479\n",
            "          model: {}\n",
            "          policy_loss: 0.0046731517650187016\n",
            "          total_loss: 4.919002056121826\n",
            "          vf_explained_var: -0.17122671008110046\n",
            "          vf_loss: 4.914328098297119\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 248000\n",
            "    num_agent_steps_trained: 248000\n",
            "    num_env_steps_sampled: 248000\n",
            "    num_env_steps_trained: 248000\n",
            "  iterations_since_restore: 62\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 248000\n",
            "  num_agent_steps_trained: 248000\n",
            "  num_env_steps_sampled: 248000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 248000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.65294117647058\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09692692254749201\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07875802425351633\n",
            "    mean_inference_ms: 1.0201155447828523\n",
            "    mean_raw_obs_processing_ms: 0.13497527023520203\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09692692254749201\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07875802425351633\n",
            "      mean_inference_ms: 1.0201155447828523\n",
            "      mean_raw_obs_processing_ms: 0.13497527023520203\n",
            "  time_since_restore: 580.7042925357819\n",
            "  time_this_iter_s: 11.301473617553711\n",
            "  time_total_s: 580.7042925357819\n",
            "  timers:\n",
            "    learn_throughput: 884.324\n",
            "    learn_time_ms: 4523.228\n",
            "    load_throughput: 13263669.855\n",
            "    load_time_ms: 0.302\n",
            "    training_iteration_time_ms: 7257.796\n",
            "    update_time_ms: 5.472\n",
            "  timestamp: 1656950750\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 248000\n",
            "  training_iteration: 62\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:05:55 (running for 00:10:02.02)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         580.704</td><td style=\"text-align: right;\">248000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 252000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 252000\n",
            "    num_agent_steps_trained: 252000\n",
            "    num_env_steps_sampled: 252000\n",
            "    num_env_steps_trained: 252000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-05-57\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1517\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.364242106869501e-13\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3823523223400116\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0024776931386440992\n",
            "          model: {}\n",
            "          policy_loss: 0.0039681317284703255\n",
            "          total_loss: 4.918282508850098\n",
            "          vf_explained_var: -0.2087816745042801\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 252000\n",
            "    num_agent_steps_trained: 252000\n",
            "    num_env_steps_sampled: 252000\n",
            "    num_env_steps_trained: 252000\n",
            "  iterations_since_restore: 63\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 252000\n",
            "  num_agent_steps_trained: 252000\n",
            "  num_env_steps_sampled: 252000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 252000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.63\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09692748084788114\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07872842094782105\n",
            "    mean_inference_ms: 1.020868645816266\n",
            "    mean_raw_obs_processing_ms: 0.13493620844893617\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09692748084788114\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07872842094782105\n",
            "      mean_inference_ms: 1.020868645816266\n",
            "      mean_raw_obs_processing_ms: 0.13493620844893617\n",
            "  time_since_restore: 587.910612821579\n",
            "  time_this_iter_s: 7.206320285797119\n",
            "  time_total_s: 587.910612821579\n",
            "  timers:\n",
            "    learn_throughput: 885.022\n",
            "    learn_time_ms: 4519.66\n",
            "    load_throughput: 13054167.445\n",
            "    load_time_ms: 0.306\n",
            "    training_iteration_time_ms: 7256.444\n",
            "    update_time_ms: 5.612\n",
            "  timestamp: 1656950757\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 252000\n",
            "  training_iteration: 63\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:02 (running for 00:10:09.40)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         587.911</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:07 (running for 00:10:14.41)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         587.911</td><td style=\"text-align: right;\">252000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 256000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 256000\n",
            "    num_agent_steps_trained: 256000\n",
            "    num_env_steps_sampled: 256000\n",
            "    num_env_steps_trained: 256000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-06-08\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1537\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07941912848178803\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059823733767340806\n",
            "      mean_inference_ms: 0.8022027351789764\n",
            "      mean_raw_obs_processing_ms: 0.083116086482689\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 6.821210534347505e-14\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.36776241660118103\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0028468987438827753\n",
            "          model: {}\n",
            "          policy_loss: 0.005396449007093906\n",
            "          total_loss: 4.919712066650391\n",
            "          vf_explained_var: -0.19266140460968018\n",
            "          vf_loss: 4.914315700531006\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 256000\n",
            "    num_agent_steps_trained: 256000\n",
            "    num_env_steps_sampled: 256000\n",
            "    num_env_steps_trained: 256000\n",
            "  iterations_since_restore: 64\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 256000\n",
            "  num_agent_steps_trained: 256000\n",
            "  num_env_steps_sampled: 256000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 256000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.5875\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09689082990257102\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07868208401531523\n",
            "    mean_inference_ms: 1.0204529480061084\n",
            "    mean_raw_obs_processing_ms: 0.1348703897566395\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09689082990257102\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07868208401531523\n",
            "      mean_inference_ms: 1.0204529480061084\n",
            "      mean_raw_obs_processing_ms: 0.1348703897566395\n",
            "  time_since_restore: 599.2496654987335\n",
            "  time_this_iter_s: 11.339052677154541\n",
            "  time_total_s: 599.2496654987335\n",
            "  timers:\n",
            "    learn_throughput: 883.672\n",
            "    learn_time_ms: 4526.566\n",
            "    load_throughput: 12620141.417\n",
            "    load_time_ms: 0.317\n",
            "    training_iteration_time_ms: 7267.635\n",
            "    update_time_ms: 6.092\n",
            "  timestamp: 1656950768\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 256000\n",
            "  training_iteration: 64\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:14 (running for 00:10:20.78)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">          599.25</td><td style=\"text-align: right;\">256000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 260000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 260000\n",
            "    num_agent_steps_trained: 260000\n",
            "    num_env_steps_sampled: 260000\n",
            "    num_env_steps_trained: 260000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-06-16\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1557\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.4106052671737525e-14\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.38953348994255066\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0014975022058933973\n",
            "          model: {}\n",
            "          policy_loss: 0.005884040612727404\n",
            "          total_loss: 4.920200347900391\n",
            "          vf_explained_var: -0.1666463166475296\n",
            "          vf_loss: 4.914316177368164\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 260000\n",
            "    num_agent_steps_trained: 260000\n",
            "    num_env_steps_sampled: 260000\n",
            "    num_env_steps_trained: 260000\n",
            "  iterations_since_restore: 65\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 260000\n",
            "  num_agent_steps_trained: 260000\n",
            "  num_env_steps_sampled: 260000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 260000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.48181818181818\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09686261868232539\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07864056884036988\n",
            "    mean_inference_ms: 1.0200997441920487\n",
            "    mean_raw_obs_processing_ms: 0.13480563585558059\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09686261868232539\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07864056884036988\n",
            "      mean_inference_ms: 1.0200997441920487\n",
            "      mean_raw_obs_processing_ms: 0.13480563585558059\n",
            "  time_since_restore: 606.5402915477753\n",
            "  time_this_iter_s: 7.290626049041748\n",
            "  time_total_s: 606.5402915477753\n",
            "  timers:\n",
            "    learn_throughput: 882.695\n",
            "    learn_time_ms: 4531.576\n",
            "    load_throughput: 12626790.096\n",
            "    load_time_ms: 0.317\n",
            "    training_iteration_time_ms: 7275.14\n",
            "    update_time_ms: 6.335\n",
            "  timestamp: 1656950776\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 260000\n",
            "  training_iteration: 65\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:21 (running for 00:10:27.95)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">          606.54</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:26 (running for 00:10:33.02)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">          606.54</td><td style=\"text-align: right;\">260000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 264000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 264000\n",
            "    num_agent_steps_trained: 264000\n",
            "    num_env_steps_sampled: 264000\n",
            "    num_env_steps_trained: 264000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-06-27\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1577\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07937219362661088\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0597788573933548\n",
            "      mean_inference_ms: 0.8017032959337685\n",
            "      mean_raw_obs_processing_ms: 0.08305305087621617\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.7053026335868762e-14\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.4018782377243042\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0025865978095680475\n",
            "          model: {}\n",
            "          policy_loss: 0.005621250253170729\n",
            "          total_loss: 4.9199395179748535\n",
            "          vf_explained_var: 0.021994760259985924\n",
            "          vf_loss: 4.914317607879639\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 264000\n",
            "    num_agent_steps_trained: 264000\n",
            "    num_env_steps_sampled: 264000\n",
            "    num_env_steps_trained: 264000\n",
            "  iterations_since_restore: 66\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 264000\n",
            "  num_agent_steps_trained: 264000\n",
            "  num_env_steps_sampled: 264000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 264000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.56875\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09684636548585857\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07860463630674247\n",
            "    mean_inference_ms: 1.0198675326016688\n",
            "    mean_raw_obs_processing_ms: 0.13476405723714519\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09684636548585857\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07860463630674247\n",
            "      mean_inference_ms: 1.0198675326016688\n",
            "      mean_raw_obs_processing_ms: 0.13476405723714519\n",
            "  time_since_restore: 617.8807420730591\n",
            "  time_this_iter_s: 11.340450525283813\n",
            "  time_total_s: 617.8807420730591\n",
            "  timers:\n",
            "    learn_throughput: 881.986\n",
            "    learn_time_ms: 4535.22\n",
            "    load_throughput: 12518442.024\n",
            "    load_time_ms: 0.32\n",
            "    training_iteration_time_ms: 7284.791\n",
            "    update_time_ms: 6.079\n",
            "  timestamp: 1656950787\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 264000\n",
            "  training_iteration: 66\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:32 (running for 00:10:39.32)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         617.881</td><td style=\"text-align: right;\">264000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 268000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 268000\n",
            "    num_agent_steps_trained: 268000\n",
            "    num_env_steps_sampled: 268000\n",
            "    num_env_steps_trained: 268000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-06-34\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1597\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.526513167934381e-15\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.40977227687835693\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0036095562390983105\n",
            "          model: {}\n",
            "          policy_loss: 0.004668208304792643\n",
            "          total_loss: 4.918991565704346\n",
            "          vf_explained_var: -0.21112574636936188\n",
            "          vf_loss: 4.914323806762695\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 268000\n",
            "    num_agent_steps_trained: 268000\n",
            "    num_env_steps_sampled: 268000\n",
            "    num_env_steps_trained: 268000\n",
            "  iterations_since_restore: 67\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 268000\n",
            "  num_agent_steps_trained: 268000\n",
            "  num_env_steps_sampled: 268000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 268000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.55\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09682866394793997\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0785705261228701\n",
            "    mean_inference_ms: 1.019648111372789\n",
            "    mean_raw_obs_processing_ms: 0.13472449897375122\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09682866394793997\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0785705261228701\n",
            "      mean_inference_ms: 1.019648111372789\n",
            "      mean_raw_obs_processing_ms: 0.13472449897375122\n",
            "  time_since_restore: 625.069605588913\n",
            "  time_this_iter_s: 7.188863515853882\n",
            "  time_total_s: 625.069605588913\n",
            "  timers:\n",
            "    learn_throughput: 881.819\n",
            "    learn_time_ms: 4536.081\n",
            "    load_throughput: 12755429.18\n",
            "    load_time_ms: 0.314\n",
            "    training_iteration_time_ms: 7286.352\n",
            "    update_time_ms: 5.772\n",
            "  timestamp: 1656950794\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 268000\n",
            "  training_iteration: 67\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:39 (running for 00:10:46.67)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">          625.07</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:44 (running for 00:10:51.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">          625.07</td><td style=\"text-align: right;\">268000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 272000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 272000\n",
            "    num_agent_steps_trained: 272000\n",
            "    num_env_steps_sampled: 272000\n",
            "    num_env_steps_trained: 272000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-06-46\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1617\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07934239007335589\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0597450534380212\n",
            "      mean_inference_ms: 0.8013605051823539\n",
            "      mean_raw_obs_processing_ms: 0.08298746189032391\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.2632565839671906e-15\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3790442645549774\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002588688163086772\n",
            "          model: {}\n",
            "          policy_loss: 0.0053580706007778645\n",
            "          total_loss: 4.919672966003418\n",
            "          vf_explained_var: -0.10625197738409042\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 272000\n",
            "    num_agent_steps_trained: 272000\n",
            "    num_env_steps_sampled: 272000\n",
            "    num_env_steps_trained: 272000\n",
            "  iterations_since_restore: 68\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 272000\n",
            "  num_agent_steps_trained: 272000\n",
            "  num_env_steps_sampled: 272000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 272000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.78125\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09681067774296812\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07854103782747043\n",
            "    mean_inference_ms: 1.0194030936986236\n",
            "    mean_raw_obs_processing_ms: 0.1346872974349943\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09681067774296812\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07854103782747043\n",
            "      mean_inference_ms: 1.0194030936986236\n",
            "      mean_raw_obs_processing_ms: 0.1346872974349943\n",
            "  time_since_restore: 636.3012871742249\n",
            "  time_this_iter_s: 11.23168158531189\n",
            "  time_total_s: 636.3012871742249\n",
            "  timers:\n",
            "    learn_throughput: 880.568\n",
            "    learn_time_ms: 4542.522\n",
            "    load_throughput: 12618243.081\n",
            "    load_time_ms: 0.317\n",
            "    training_iteration_time_ms: 7292.31\n",
            "    update_time_ms: 5.989\n",
            "  timestamp: 1656950806\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 272000\n",
            "  training_iteration: 68\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:51 (running for 00:10:58.01)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         636.301</td><td style=\"text-align: right;\">272000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 276000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 276000\n",
            "    num_agent_steps_trained: 276000\n",
            "    num_env_steps_sampled: 276000\n",
            "    num_env_steps_trained: 276000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-06-53\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1637\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.1316282919835953e-15\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3872796297073364\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0031199699733406305\n",
            "          model: {}\n",
            "          policy_loss: 0.004347098991274834\n",
            "          total_loss: 4.91867208480835\n",
            "          vf_explained_var: 0.017898723483085632\n",
            "          vf_loss: 4.91432523727417\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 276000\n",
            "    num_agent_steps_trained: 276000\n",
            "    num_env_steps_sampled: 276000\n",
            "    num_env_steps_trained: 276000\n",
            "  iterations_since_restore: 69\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 276000\n",
            "  num_agent_steps_trained: 276000\n",
            "  num_env_steps_sampled: 276000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 276000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.87\n",
            "    ram_util_percent: 22.0\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09679502873791854\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07852186387132865\n",
            "    mean_inference_ms: 1.0191837436714173\n",
            "    mean_raw_obs_processing_ms: 0.13466844335889191\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09679502873791854\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07852186387132865\n",
            "      mean_inference_ms: 1.0191837436714173\n",
            "      mean_raw_obs_processing_ms: 0.13466844335889191\n",
            "  time_since_restore: 643.5563130378723\n",
            "  time_this_iter_s: 7.255025863647461\n",
            "  time_total_s: 643.5563130378723\n",
            "  timers:\n",
            "    learn_throughput: 879.399\n",
            "    learn_time_ms: 4548.562\n",
            "    load_throughput: 12698468.059\n",
            "    load_time_ms: 0.315\n",
            "    training_iteration_time_ms: 7222.157\n",
            "    update_time_ms: 6.399\n",
            "  timestamp: 1656950813\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 276000\n",
            "  training_iteration: 69\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:06:58 (running for 00:11:05.07)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         643.556</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:03 (running for 00:11:10.15)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         643.556</td><td style=\"text-align: right;\">276000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 280000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 280000\n",
            "    num_agent_steps_trained: 280000\n",
            "    num_env_steps_sampled: 280000\n",
            "    num_env_steps_trained: 280000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-07-04\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1657\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07926078014107399\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05969637554344563\n",
            "      mean_inference_ms: 0.8012927138878284\n",
            "      mean_raw_obs_processing_ms: 0.08292639925133236\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.0658141459917976e-15\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.37774914503097534\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0069656530395150185\n",
            "          model: {}\n",
            "          policy_loss: 0.0022351716179400682\n",
            "          total_loss: 4.916550636291504\n",
            "          vf_explained_var: 0.04363124445080757\n",
            "          vf_loss: 4.914315223693848\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 280000\n",
            "    num_agent_steps_trained: 280000\n",
            "    num_env_steps_sampled: 280000\n",
            "    num_env_steps_trained: 280000\n",
            "  iterations_since_restore: 70\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 280000\n",
            "  num_agent_steps_trained: 280000\n",
            "  num_env_steps_sampled: 280000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 280000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.16470588235293\n",
            "    ram_util_percent: 22.094117647058823\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09676841495958158\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07849885332904812\n",
            "    mean_inference_ms: 1.0189008041775482\n",
            "    mean_raw_obs_processing_ms: 0.13464026592901138\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09676841495958158\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07849885332904812\n",
            "      mean_inference_ms: 1.0189008041775482\n",
            "      mean_raw_obs_processing_ms: 0.13464026592901138\n",
            "  time_since_restore: 654.7672619819641\n",
            "  time_this_iter_s: 11.210948944091797\n",
            "  time_total_s: 654.7672619819641\n",
            "  timers:\n",
            "    learn_throughput: 879.748\n",
            "    learn_time_ms: 4546.759\n",
            "    load_throughput: 12841344.049\n",
            "    load_time_ms: 0.311\n",
            "    training_iteration_time_ms: 7217.325\n",
            "    update_time_ms: 6.012\n",
            "  timestamp: 1656950824\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 280000\n",
            "  training_iteration: 70\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:09 (running for 00:11:16.31)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         654.767</td><td style=\"text-align: right;\">280000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 284000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 284000\n",
            "    num_agent_steps_trained: 284000\n",
            "    num_env_steps_sampled: 284000\n",
            "    num_env_steps_trained: 284000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-07-11\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1677\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.0658141459917976e-15\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3781697452068329\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.002632077084854245\n",
            "          model: {}\n",
            "          policy_loss: 0.004849774297326803\n",
            "          total_loss: 4.919166564941406\n",
            "          vf_explained_var: -0.21242766082286835\n",
            "          vf_loss: 4.914316177368164\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 284000\n",
            "    num_agent_steps_trained: 284000\n",
            "    num_env_steps_sampled: 284000\n",
            "    num_env_steps_trained: 284000\n",
            "  iterations_since_restore: 71\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 284000\n",
            "  num_agent_steps_trained: 284000\n",
            "  num_env_steps_sampled: 284000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 284000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.04\n",
            "    ram_util_percent: 22.09\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09673268262711876\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07846871086634591\n",
            "    mean_inference_ms: 1.0185115176905633\n",
            "    mean_raw_obs_processing_ms: 0.13460078129483324\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09673268262711876\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07846871086634591\n",
            "      mean_inference_ms: 1.0185115176905633\n",
            "      mean_raw_obs_processing_ms: 0.13460078129483324\n",
            "  time_since_restore: 661.922158241272\n",
            "  time_this_iter_s: 7.154896259307861\n",
            "  time_total_s: 661.922158241272\n",
            "  timers:\n",
            "    learn_throughput: 880.0\n",
            "    learn_time_ms: 4545.455\n",
            "    load_throughput: 12565320.551\n",
            "    load_time_ms: 0.318\n",
            "    training_iteration_time_ms: 7216.284\n",
            "    update_time_ms: 6.016\n",
            "  timestamp: 1656950831\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 284000\n",
            "  training_iteration: 71\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:16 (running for 00:11:23.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         661.922</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:21 (running for 00:11:28.70)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         661.922</td><td style=\"text-align: right;\">284000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 288000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 288000\n",
            "    num_agent_steps_trained: 288000\n",
            "    num_env_steps_sampled: 288000\n",
            "    num_env_steps_trained: 288000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-07-22\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1697\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07924503066614379\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05969529373488952\n",
            "      mean_inference_ms: 0.8012367799619705\n",
            "      mean_raw_obs_processing_ms: 0.08290250919728742\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 5.329070729958988e-16\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3835255205631256\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004084481857717037\n",
            "          model: {}\n",
            "          policy_loss: 0.0030170204117894173\n",
            "          total_loss: 4.917335033416748\n",
            "          vf_explained_var: -0.20859768986701965\n",
            "          vf_loss: 4.914317607879639\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 288000\n",
            "    num_agent_steps_trained: 288000\n",
            "    num_env_steps_sampled: 288000\n",
            "    num_env_steps_trained: 288000\n",
            "  iterations_since_restore: 72\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 288000\n",
            "  num_agent_steps_trained: 288000\n",
            "  num_env_steps_sampled: 288000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 288000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.5625\n",
            "    ram_util_percent: 22.081250000000004\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09668893157771045\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07844077364367193\n",
            "    mean_inference_ms: 1.0180985057059857\n",
            "    mean_raw_obs_processing_ms: 0.13456158129864235\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09668893157771045\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07844077364367193\n",
            "      mean_inference_ms: 1.0180985057059857\n",
            "      mean_raw_obs_processing_ms: 0.13456158129864235\n",
            "  time_since_restore: 673.1416556835175\n",
            "  time_this_iter_s: 11.219497442245483\n",
            "  time_total_s: 673.1416556835175\n",
            "  timers:\n",
            "    learn_throughput: 881.698\n",
            "    learn_time_ms: 4536.702\n",
            "    load_throughput: 12185659.5\n",
            "    load_time_ms: 0.328\n",
            "    training_iteration_time_ms: 7203.85\n",
            "    update_time_ms: 5.583\n",
            "  timestamp: 1656950842\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 288000\n",
            "  training_iteration: 72\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:28 (running for 00:11:34.90)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         673.142</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 292000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 292000\n",
            "    num_agent_steps_trained: 292000\n",
            "    num_env_steps_sampled: 292000\n",
            "    num_env_steps_trained: 292000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-07-30\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1717\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.664535364979494e-16\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.36085501313209534\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.00352852838113904\n",
            "          model: {}\n",
            "          policy_loss: 0.004040622618049383\n",
            "          total_loss: 4.918361663818359\n",
            "          vf_explained_var: -0.22009681165218353\n",
            "          vf_loss: 4.914320945739746\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 292000\n",
            "    num_agent_steps_trained: 292000\n",
            "    num_env_steps_sampled: 292000\n",
            "    num_env_steps_trained: 292000\n",
            "  iterations_since_restore: 73\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 292000\n",
            "  num_agent_steps_trained: 292000\n",
            "  num_env_steps_sampled: 292000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 292000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.51\n",
            "    ram_util_percent: 22.07\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09665412651419442\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07840711837120073\n",
            "    mean_inference_ms: 1.0176515948834917\n",
            "    mean_raw_obs_processing_ms: 0.13452243534110195\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09665412651419442\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07840711837120073\n",
            "      mean_inference_ms: 1.0176515948834917\n",
            "      mean_raw_obs_processing_ms: 0.13452243534110195\n",
            "  time_since_restore: 680.3574991226196\n",
            "  time_this_iter_s: 7.215843439102173\n",
            "  time_total_s: 680.3574991226196\n",
            "  timers:\n",
            "    learn_throughput: 880.566\n",
            "    learn_time_ms: 4542.531\n",
            "    load_throughput: 12276610.566\n",
            "    load_time_ms: 0.326\n",
            "    training_iteration_time_ms: 7204.879\n",
            "    update_time_ms: 5.201\n",
            "  timestamp: 1656950850\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 292000\n",
            "  training_iteration: 73\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:35 (running for 00:11:41.99)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         680.357</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:40 (running for 00:11:47.07)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         680.357</td><td style=\"text-align: right;\">292000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 296000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 296000\n",
            "    num_agent_steps_trained: 296000\n",
            "    num_env_steps_sampled: 296000\n",
            "    num_env_steps_trained: 296000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-07-41\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1737\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0792049660411855\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05966815413546086\n",
            "      mean_inference_ms: 0.8012238378984898\n",
            "      mean_raw_obs_processing_ms: 0.08288012089657051\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.332267682489747e-16\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3697853684425354\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0066597783006727695\n",
            "          model: {}\n",
            "          policy_loss: 0.00308613502420485\n",
            "          total_loss: 4.917401313781738\n",
            "          vf_explained_var: -0.19660061597824097\n",
            "          vf_loss: 4.914315223693848\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 296000\n",
            "    num_agent_steps_trained: 296000\n",
            "    num_env_steps_sampled: 296000\n",
            "    num_env_steps_trained: 296000\n",
            "  iterations_since_restore: 74\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 296000\n",
            "  num_agent_steps_trained: 296000\n",
            "  num_env_steps_sampled: 296000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 296000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.28125\n",
            "    ram_util_percent: 22.043750000000003\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09662168801498758\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07836679510309996\n",
            "    mean_inference_ms: 1.0171721274558547\n",
            "    mean_raw_obs_processing_ms: 0.13446914466371812\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09662168801498758\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07836679510309996\n",
            "      mean_inference_ms: 1.0171721274558547\n",
            "      mean_raw_obs_processing_ms: 0.13446914466371812\n",
            "  time_since_restore: 691.6693480014801\n",
            "  time_this_iter_s: 11.311848878860474\n",
            "  time_total_s: 691.6693480014801\n",
            "  timers:\n",
            "    learn_throughput: 880.643\n",
            "    learn_time_ms: 4542.137\n",
            "    load_throughput: 12524048.97\n",
            "    load_time_ms: 0.319\n",
            "    training_iteration_time_ms: 7200.368\n",
            "    update_time_ms: 4.807\n",
            "  timestamp: 1656950861\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 296000\n",
            "  training_iteration: 74\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:46 (running for 00:11:53.33)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         691.669</td><td style=\"text-align: right;\">296000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 300000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 300000\n",
            "    num_agent_steps_trained: 300000\n",
            "    num_env_steps_sampled: 300000\n",
            "    num_env_steps_trained: 300000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-07-48\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1757\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.332267682489747e-16\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3704898953437805\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0034287553280591965\n",
            "          model: {}\n",
            "          policy_loss: 0.004895394667983055\n",
            "          total_loss: 4.919214248657227\n",
            "          vf_explained_var: -0.18252097070217133\n",
            "          vf_loss: 4.914318561553955\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 300000\n",
            "    num_agent_steps_trained: 300000\n",
            "    num_env_steps_sampled: 300000\n",
            "    num_env_steps_trained: 300000\n",
            "  iterations_since_restore: 75\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 300000\n",
            "  num_agent_steps_trained: 300000\n",
            "  num_env_steps_sampled: 300000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 300000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.05454545454545\n",
            "    ram_util_percent: 22.027272727272727\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0965991543866593\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07832881700318103\n",
            "    mean_inference_ms: 1.016748243761121\n",
            "    mean_raw_obs_processing_ms: 0.1344168305242649\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0965991543866593\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07832881700318103\n",
            "      mean_inference_ms: 1.016748243761121\n",
            "      mean_raw_obs_processing_ms: 0.1344168305242649\n",
            "  time_since_restore: 698.8712482452393\n",
            "  time_this_iter_s: 7.201900243759155\n",
            "  time_total_s: 698.8712482452393\n",
            "  timers:\n",
            "    learn_throughput: 882.038\n",
            "    learn_time_ms: 4534.953\n",
            "    load_throughput: 12696546.087\n",
            "    load_time_ms: 0.315\n",
            "    training_iteration_time_ms: 7191.768\n",
            "    update_time_ms: 4.956\n",
            "  timestamp: 1656950868\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 300000\n",
            "  training_iteration: 75\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:53 (running for 00:12:00.73)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         698.871</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:07:58 (running for 00:12:05.75)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         698.871</td><td style=\"text-align: right;\">300000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 304000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 304000\n",
            "    num_agent_steps_trained: 304000\n",
            "    num_env_steps_sampled: 304000\n",
            "    num_env_steps_trained: 304000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-08-00\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1777\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.079161902661437\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05962682525670391\n",
            "      mean_inference_ms: 0.8009295051945001\n",
            "      mean_raw_obs_processing_ms: 0.0828217700578282\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 6.661338412448735e-17\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.351176917552948\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0030181787442415953\n",
            "          model: {}\n",
            "          policy_loss: 0.005281004123389721\n",
            "          total_loss: 4.919599533081055\n",
            "          vf_explained_var: -0.18258020281791687\n",
            "          vf_loss: 4.914318561553955\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 304000\n",
            "    num_agent_steps_trained: 304000\n",
            "    num_env_steps_sampled: 304000\n",
            "    num_env_steps_trained: 304000\n",
            "  iterations_since_restore: 76\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 304000\n",
            "  num_agent_steps_trained: 304000\n",
            "  num_env_steps_sampled: 304000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 304000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.1875\n",
            "    ram_util_percent: 22.0125\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09657568130833914\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07829830535963966\n",
            "    mean_inference_ms: 1.01635602602701\n",
            "    mean_raw_obs_processing_ms: 0.13436300826150543\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09657568130833914\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07829830535963966\n",
            "      mean_inference_ms: 1.01635602602701\n",
            "      mean_raw_obs_processing_ms: 0.13436300826150543\n",
            "  time_since_restore: 710.0681970119476\n",
            "  time_this_iter_s: 11.196948766708374\n",
            "  time_total_s: 710.0681970119476\n",
            "  timers:\n",
            "    learn_throughput: 883.658\n",
            "    learn_time_ms: 4526.636\n",
            "    load_throughput: 12703275.536\n",
            "    load_time_ms: 0.315\n",
            "    training_iteration_time_ms: 7175.659\n",
            "    update_time_ms: 4.919\n",
            "  timestamp: 1656950880\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 304000\n",
            "  training_iteration: 76\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:05 (running for 00:12:11.92)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         710.068</td><td style=\"text-align: right;\">304000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 308000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 308000\n",
            "    num_agent_steps_trained: 308000\n",
            "    num_env_steps_sampled: 308000\n",
            "    num_env_steps_trained: 308000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-08-07\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1797\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.3306692062243676e-17\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3489927351474762\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004848539363592863\n",
            "          model: {}\n",
            "          policy_loss: 0.004899309482425451\n",
            "          total_loss: 4.919227600097656\n",
            "          vf_explained_var: -0.12926089763641357\n",
            "          vf_loss: 4.914328098297119\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 308000\n",
            "    num_agent_steps_trained: 308000\n",
            "    num_env_steps_sampled: 308000\n",
            "    num_env_steps_trained: 308000\n",
            "  iterations_since_restore: 77\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 308000\n",
            "  num_agent_steps_trained: 308000\n",
            "  num_env_steps_sampled: 308000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 308000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.78999999999999\n",
            "    ram_util_percent: 22.09\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09656542228463284\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07826788123693185\n",
            "    mean_inference_ms: 1.016004402025512\n",
            "    mean_raw_obs_processing_ms: 0.13430377153395467\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09656542228463284\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07826788123693185\n",
            "      mean_inference_ms: 1.016004402025512\n",
            "      mean_raw_obs_processing_ms: 0.13430377153395467\n",
            "  time_since_restore: 717.2341809272766\n",
            "  time_this_iter_s: 7.1659839153289795\n",
            "  time_total_s: 717.2341809272766\n",
            "  timers:\n",
            "    learn_throughput: 884.053\n",
            "    learn_time_ms: 4524.618\n",
            "    load_throughput: 12851180.391\n",
            "    load_time_ms: 0.311\n",
            "    training_iteration_time_ms: 7173.353\n",
            "    update_time_ms: 4.918\n",
            "  timestamp: 1656950887\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 308000\n",
            "  training_iteration: 77\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:12 (running for 00:12:18.98)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         717.234</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:17 (running for 00:12:24.06)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         717.234</td><td style=\"text-align: right;\">308000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 312000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 312000\n",
            "    num_agent_steps_trained: 312000\n",
            "    num_env_steps_sampled: 312000\n",
            "    num_env_steps_trained: 312000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-08-18\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1817\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07914671335763332\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05962277783512917\n",
            "      mean_inference_ms: 0.8008764655127415\n",
            "      mean_raw_obs_processing_ms: 0.08281269928170339\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.6653346031121838e-17\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.34512460231781006\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.006146396975964308\n",
            "          model: {}\n",
            "          policy_loss: 0.003693596227094531\n",
            "          total_loss: 4.918008804321289\n",
            "          vf_explained_var: -0.18975193798542023\n",
            "          vf_loss: 4.914315223693848\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 312000\n",
            "    num_agent_steps_trained: 312000\n",
            "    num_env_steps_sampled: 312000\n",
            "    num_env_steps_trained: 312000\n",
            "  iterations_since_restore: 78\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 312000\n",
            "  num_agent_steps_trained: 312000\n",
            "  num_env_steps_sampled: 312000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 312000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.9\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0965441023956957\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07824127227170129\n",
            "    mean_inference_ms: 1.0156957050779107\n",
            "    mean_raw_obs_processing_ms: 0.13424671159309157\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0965441023956957\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07824127227170129\n",
            "      mean_inference_ms: 1.0156957050779107\n",
            "      mean_raw_obs_processing_ms: 0.13424671159309157\n",
            "  time_since_restore: 728.5224719047546\n",
            "  time_this_iter_s: 11.288290977478027\n",
            "  time_total_s: 728.5224719047546\n",
            "  timers:\n",
            "    learn_throughput: 883.819\n",
            "    learn_time_ms: 4525.811\n",
            "    load_throughput: 12909522.93\n",
            "    load_time_ms: 0.31\n",
            "    training_iteration_time_ms: 7174.104\n",
            "    update_time_ms: 4.654\n",
            "  timestamp: 1656950898\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 312000\n",
            "  training_iteration: 78\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:23 (running for 00:12:30.30)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         728.522</td><td style=\"text-align: right;\">312000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 316000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 316000\n",
            "    num_agent_steps_trained: 316000\n",
            "    num_env_steps_sampled: 316000\n",
            "    num_env_steps_trained: 316000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-08-25\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1837\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.6653346031121838e-17\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.33211857080459595\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0045454241335392\n",
            "          model: {}\n",
            "          policy_loss: 0.0045309122651815414\n",
            "          total_loss: 4.918846130371094\n",
            "          vf_explained_var: -0.2258097380399704\n",
            "          vf_loss: 4.914315223693848\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 316000\n",
            "    num_agent_steps_trained: 316000\n",
            "    num_env_steps_sampled: 316000\n",
            "    num_env_steps_trained: 316000\n",
            "  iterations_since_restore: 79\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 316000\n",
            "  num_agent_steps_trained: 316000\n",
            "  num_env_steps_sampled: 316000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 316000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.0\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09652578563914282\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07821816461871343\n",
            "    mean_inference_ms: 1.0154077783677669\n",
            "    mean_raw_obs_processing_ms: 0.1341989202710485\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09652578563914282\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07821816461871343\n",
            "      mean_inference_ms: 1.0154077783677669\n",
            "      mean_raw_obs_processing_ms: 0.1341989202710485\n",
            "  time_since_restore: 735.6799948215485\n",
            "  time_this_iter_s: 7.157522916793823\n",
            "  time_total_s: 735.6799948215485\n",
            "  timers:\n",
            "    learn_throughput: 885.122\n",
            "    learn_time_ms: 4519.15\n",
            "    load_throughput: 13022755.569\n",
            "    load_time_ms: 0.307\n",
            "    training_iteration_time_ms: 7164.299\n",
            "    update_time_ms: 4.209\n",
            "  timestamp: 1656950905\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 316000\n",
            "  training_iteration: 79\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:30 (running for 00:12:37.64)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          735.68</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:35 (running for 00:12:42.65)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">          735.68</td><td style=\"text-align: right;\">316000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 320000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 320000\n",
            "    num_agent_steps_trained: 320000\n",
            "    num_env_steps_sampled: 320000\n",
            "    num_env_steps_trained: 320000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-08-36\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1857\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07910706775481115\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059576423980420234\n",
            "      mean_inference_ms: 0.8004800574665316\n",
            "      mean_raw_obs_processing_ms: 0.08274782205392224\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.326673015560919e-18\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3189188539981842\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004800818394869566\n",
            "          model: {}\n",
            "          policy_loss: 0.0027960152365267277\n",
            "          total_loss: 4.917113780975342\n",
            "          vf_explained_var: -0.22477629780769348\n",
            "          vf_loss: 4.914317607879639\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 320000\n",
            "    num_agent_steps_trained: 320000\n",
            "    num_env_steps_sampled: 320000\n",
            "    num_env_steps_trained: 320000\n",
            "  iterations_since_restore: 80\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 320000\n",
            "  num_agent_steps_trained: 320000\n",
            "  num_env_steps_sampled: 320000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 320000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 82.29375\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09650437697849423\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07820003568607384\n",
            "    mean_inference_ms: 1.0151485189036615\n",
            "    mean_raw_obs_processing_ms: 0.13415957367891873\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09650437697849423\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07820003568607384\n",
            "      mean_inference_ms: 1.0151485189036615\n",
            "      mean_raw_obs_processing_ms: 0.13415957367891873\n",
            "  time_since_restore: 746.8695023059845\n",
            "  time_this_iter_s: 11.189507484436035\n",
            "  time_total_s: 746.8695023059845\n",
            "  timers:\n",
            "    learn_throughput: 885.755\n",
            "    learn_time_ms: 4515.922\n",
            "    load_throughput: 13136963.433\n",
            "    load_time_ms: 0.304\n",
            "    training_iteration_time_ms: 7168.231\n",
            "    update_time_ms: 4.222\n",
            "  timestamp: 1656950916\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 320000\n",
            "  training_iteration: 80\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:42 (running for 00:12:48.87)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">          746.87</td><td style=\"text-align: right;\">320000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 324000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 324000\n",
            "    num_agent_steps_trained: 324000\n",
            "    num_env_steps_sampled: 324000\n",
            "    num_env_steps_trained: 324000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-08-44\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1877\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.1633365077804595e-18\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3129802644252777\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0019026780501008034\n",
            "          model: {}\n",
            "          policy_loss: 0.005356702022254467\n",
            "          total_loss: 4.919671535491943\n",
            "          vf_explained_var: -0.22513139247894287\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 324000\n",
            "    num_agent_steps_trained: 324000\n",
            "    num_env_steps_sampled: 324000\n",
            "    num_env_steps_trained: 324000\n",
            "  iterations_since_restore: 81\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 324000\n",
            "  num_agent_steps_trained: 324000\n",
            "  num_env_steps_sampled: 324000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 324000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.09090909090908\n",
            "    ram_util_percent: 22.099999999999998\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09649457471433831\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07818449428533493\n",
            "    mean_inference_ms: 1.0149149451022748\n",
            "    mean_raw_obs_processing_ms: 0.13412278902442776\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09649457471433831\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07818449428533493\n",
            "      mean_inference_ms: 1.0149149451022748\n",
            "      mean_raw_obs_processing_ms: 0.13412278902442776\n",
            "  time_since_restore: 754.0681376457214\n",
            "  time_this_iter_s: 7.1986353397369385\n",
            "  time_total_s: 754.0681376457214\n",
            "  timers:\n",
            "    learn_throughput: 886.05\n",
            "    learn_time_ms: 4514.42\n",
            "    load_throughput: 13331121.176\n",
            "    load_time_ms: 0.3\n",
            "    training_iteration_time_ms: 7172.592\n",
            "    update_time_ms: 4.2\n",
            "  timestamp: 1656950924\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 324000\n",
            "  training_iteration: 81\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:49 (running for 00:12:55.92)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         754.068</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:08:54 (running for 00:13:01.01)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         754.068</td><td style=\"text-align: right;\">324000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 328000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 328000\n",
            "    num_agent_steps_trained: 328000\n",
            "    num_env_steps_sampled: 328000\n",
            "    num_env_steps_trained: 328000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-08-55\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1897\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07911402971640938\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05957171835922507\n",
            "      mean_inference_ms: 0.8000512222770981\n",
            "      mean_raw_obs_processing_ms: 0.08272154415917925\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.0816682538902298e-18\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.32043567299842834\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0027105279732495546\n",
            "          model: {}\n",
            "          policy_loss: 0.005187149625271559\n",
            "          total_loss: 4.919509410858154\n",
            "          vf_explained_var: 0.003229932626709342\n",
            "          vf_loss: 4.9143218994140625\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 328000\n",
            "    num_agent_steps_trained: 328000\n",
            "    num_env_steps_sampled: 328000\n",
            "    num_env_steps_trained: 328000\n",
            "  iterations_since_restore: 82\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 328000\n",
            "  num_agent_steps_trained: 328000\n",
            "  num_env_steps_sampled: 328000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 328000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.65\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0964803053245528\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07817201128447988\n",
            "    mean_inference_ms: 1.0146790277797997\n",
            "    mean_raw_obs_processing_ms: 0.13409292548568977\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0964803053245528\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07817201128447988\n",
            "      mean_inference_ms: 1.0146790277797997\n",
            "      mean_raw_obs_processing_ms: 0.13409292548568977\n",
            "  time_since_restore: 765.3120269775391\n",
            "  time_this_iter_s: 11.243889331817627\n",
            "  time_total_s: 765.3120269775391\n",
            "  timers:\n",
            "    learn_throughput: 884.805\n",
            "    learn_time_ms: 4520.769\n",
            "    load_throughput: 13073494.896\n",
            "    load_time_ms: 0.306\n",
            "    training_iteration_time_ms: 7181.026\n",
            "    update_time_ms: 4.229\n",
            "  timestamp: 1656950935\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 328000\n",
            "  training_iteration: 82\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:00 (running for 00:13:07.20)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         765.312</td><td style=\"text-align: right;\">328000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 332000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 332000\n",
            "    num_agent_steps_trained: 332000\n",
            "    num_env_steps_sampled: 332000\n",
            "    num_env_steps_trained: 332000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-09-02\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1917\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.0408341269451149e-18\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.33085209131240845\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0031553253065794706\n",
            "          model: {}\n",
            "          policy_loss: 0.0041215019300580025\n",
            "          total_loss: 4.91843843460083\n",
            "          vf_explained_var: -0.2258075326681137\n",
            "          vf_loss: 4.9143171310424805\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 332000\n",
            "    num_agent_steps_trained: 332000\n",
            "    num_env_steps_sampled: 332000\n",
            "    num_env_steps_trained: 332000\n",
            "  iterations_since_restore: 83\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 332000\n",
            "  num_agent_steps_trained: 332000\n",
            "  num_env_steps_sampled: 332000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 332000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.53\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09647356010809968\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0781641781314136\n",
            "    mean_inference_ms: 1.0144851078276285\n",
            "    mean_raw_obs_processing_ms: 0.13406499198300043\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09647356010809968\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0781641781314136\n",
            "      mean_inference_ms: 1.0144851078276285\n",
            "      mean_raw_obs_processing_ms: 0.13406499198300043\n",
            "  time_since_restore: 772.5670373439789\n",
            "  time_this_iter_s: 7.255010366439819\n",
            "  time_total_s: 772.5670373439789\n",
            "  timers:\n",
            "    learn_throughput: 884.959\n",
            "    learn_time_ms: 4519.982\n",
            "    load_throughput: 13124631.151\n",
            "    load_time_ms: 0.305\n",
            "    training_iteration_time_ms: 7184.837\n",
            "    update_time_ms: 4.215\n",
            "  timestamp: 1656950942\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 332000\n",
            "  training_iteration: 83\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:07 (running for 00:13:14.62)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         772.567</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:12 (running for 00:13:19.65)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         772.567</td><td style=\"text-align: right;\">332000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 336000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 336000\n",
            "    num_agent_steps_trained: 336000\n",
            "    num_env_steps_sampled: 336000\n",
            "    num_env_steps_trained: 336000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-09-14\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1937\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07908800720349454\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05954927340888953\n",
            "      mean_inference_ms: 0.7999664134587945\n",
            "      mean_raw_obs_processing_ms: 0.08270102100911542\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 5.204170634725574e-19\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3222007751464844\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003533013863489032\n",
            "          model: {}\n",
            "          policy_loss: 0.004441332537680864\n",
            "          total_loss: 4.918761730194092\n",
            "          vf_explained_var: 0.01385981123894453\n",
            "          vf_loss: 4.91431999206543\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 336000\n",
            "    num_agent_steps_trained: 336000\n",
            "    num_env_steps_sampled: 336000\n",
            "    num_env_steps_trained: 336000\n",
            "  iterations_since_restore: 84\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 336000\n",
            "  num_agent_steps_trained: 336000\n",
            "  num_env_steps_sampled: 336000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 336000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.9375\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09647002921875124\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07815472044329058\n",
            "    mean_inference_ms: 1.0143163229165892\n",
            "    mean_raw_obs_processing_ms: 0.13403621758943354\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09647002921875124\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07815472044329058\n",
            "      mean_inference_ms: 1.0143163229165892\n",
            "      mean_raw_obs_processing_ms: 0.13403621758943354\n",
            "  time_since_restore: 783.8750250339508\n",
            "  time_this_iter_s: 11.307987689971924\n",
            "  time_total_s: 783.8750250339508\n",
            "  timers:\n",
            "    learn_throughput: 885.641\n",
            "    learn_time_ms: 4516.502\n",
            "    load_throughput: 12736063.159\n",
            "    load_time_ms: 0.314\n",
            "    training_iteration_time_ms: 7186.377\n",
            "    update_time_ms: 4.175\n",
            "  timestamp: 1656950954\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 336000\n",
            "  training_iteration: 84\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:19 (running for 00:13:25.96)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         783.875</td><td style=\"text-align: right;\">336000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 340000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 340000\n",
            "    num_agent_steps_trained: 340000\n",
            "    num_env_steps_sampled: 340000\n",
            "    num_env_steps_trained: 340000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-09-21\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1957\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.602085317362787e-19\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3068374991416931\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004845800809562206\n",
            "          model: {}\n",
            "          policy_loss: 0.0034956687595695257\n",
            "          total_loss: 4.917812347412109\n",
            "          vf_explained_var: 0.01957375928759575\n",
            "          vf_loss: 4.914316654205322\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 340000\n",
            "    num_agent_steps_trained: 340000\n",
            "    num_env_steps_sampled: 340000\n",
            "    num_env_steps_trained: 340000\n",
            "  iterations_since_restore: 85\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 340000\n",
            "  num_agent_steps_trained: 340000\n",
            "  num_env_steps_sampled: 340000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 340000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.36363636363636\n",
            "    ram_util_percent: 22.099999999999998\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09646878137909067\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0781505052076349\n",
            "    mean_inference_ms: 1.014146388916239\n",
            "    mean_raw_obs_processing_ms: 0.1340066692860392\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09646878137909067\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0781505052076349\n",
            "      mean_inference_ms: 1.014146388916239\n",
            "      mean_raw_obs_processing_ms: 0.1340066692860392\n",
            "  time_since_restore: 791.1793124675751\n",
            "  time_this_iter_s: 7.304287433624268\n",
            "  time_total_s: 791.1793124675751\n",
            "  timers:\n",
            "    learn_throughput: 884.052\n",
            "    learn_time_ms: 4524.622\n",
            "    load_throughput: 12785563.176\n",
            "    load_time_ms: 0.313\n",
            "    training_iteration_time_ms: 7196.28\n",
            "    update_time_ms: 3.757\n",
            "  timestamp: 1656950961\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 340000\n",
            "  training_iteration: 85\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:26 (running for 00:13:33.15)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         791.179</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:31 (running for 00:13:38.24)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         791.179</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 344000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 344000\n",
            "    num_agent_steps_trained: 344000\n",
            "    num_env_steps_sampled: 344000\n",
            "    num_env_steps_trained: 344000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-09-32\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1977\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07906068456371383\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.059528013355085486\n",
            "      mean_inference_ms: 0.79983288699388\n",
            "      mean_raw_obs_processing_ms: 0.08273475314529889\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.3010426586813936e-19\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.30896854400634766\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0016234925715252757\n",
            "          model: {}\n",
            "          policy_loss: 0.005120451562106609\n",
            "          total_loss: 4.919440746307373\n",
            "          vf_explained_var: 0.0020736795850098133\n",
            "          vf_loss: 4.91431999206543\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 344000\n",
            "    num_agent_steps_trained: 344000\n",
            "    num_env_steps_sampled: 344000\n",
            "    num_env_steps_trained: 344000\n",
            "  iterations_since_restore: 86\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 344000\n",
            "  num_agent_steps_trained: 344000\n",
            "  num_env_steps_sampled: 344000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 344000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.56875\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09646189248087497\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07813990459733644\n",
            "    mean_inference_ms: 1.0139530794686438\n",
            "    mean_raw_obs_processing_ms: 0.13397852395641482\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09646189248087497\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07813990459733644\n",
            "      mean_inference_ms: 1.0139530794686438\n",
            "      mean_raw_obs_processing_ms: 0.13397852395641482\n",
            "  time_since_restore: 802.5580468177795\n",
            "  time_this_iter_s: 11.378734350204468\n",
            "  time_total_s: 802.5580468177795\n",
            "  timers:\n",
            "    learn_throughput: 881.175\n",
            "    learn_time_ms: 4539.393\n",
            "    load_throughput: 12550281.269\n",
            "    load_time_ms: 0.319\n",
            "    training_iteration_time_ms: 7211.541\n",
            "    update_time_ms: 4.177\n",
            "  timestamp: 1656950972\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 344000\n",
            "  training_iteration: 86\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:37 (running for 00:13:44.56)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         802.558</td><td style=\"text-align: right;\">344000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 348000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 348000\n",
            "    num_agent_steps_trained: 348000\n",
            "    num_env_steps_sampled: 348000\n",
            "    num_env_steps_trained: 348000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-09-40\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 1997\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 6.505213293406968e-20\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.30704355239868164\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0030796476639807224\n",
            "          model: {}\n",
            "          policy_loss: 0.004681925754994154\n",
            "          total_loss: 4.919000148773193\n",
            "          vf_explained_var: -0.20142002403736115\n",
            "          vf_loss: 4.914317607879639\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 348000\n",
            "    num_agent_steps_trained: 348000\n",
            "    num_env_steps_sampled: 348000\n",
            "    num_env_steps_trained: 348000\n",
            "  iterations_since_restore: 87\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 348000\n",
            "  num_agent_steps_trained: 348000\n",
            "  num_env_steps_sampled: 348000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 348000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.22\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09645554265741384\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07813027801278316\n",
            "    mean_inference_ms: 1.0137994363341545\n",
            "    mean_raw_obs_processing_ms: 0.13395520405914219\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09645554265741384\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07813027801278316\n",
            "      mean_inference_ms: 1.0137994363341545\n",
            "      mean_raw_obs_processing_ms: 0.13395520405914219\n",
            "  time_since_restore: 809.8258445262909\n",
            "  time_this_iter_s: 7.2677977085113525\n",
            "  time_total_s: 809.8258445262909\n",
            "  timers:\n",
            "    learn_throughput: 879.493\n",
            "    learn_time_ms: 4548.073\n",
            "    load_throughput: 12688864.015\n",
            "    load_time_ms: 0.315\n",
            "    training_iteration_time_ms: 7221.601\n",
            "    update_time_ms: 4.586\n",
            "  timestamp: 1656950980\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 348000\n",
            "  training_iteration: 87\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:45 (running for 00:13:51.99)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         809.826</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:50 (running for 00:13:57.01)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         809.826</td><td style=\"text-align: right;\">348000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 352000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 352000\n",
            "    num_agent_steps_trained: 352000\n",
            "    num_env_steps_sampled: 352000\n",
            "    num_env_steps_trained: 352000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-09-51\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2017\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07906256222123377\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05952190796498399\n",
            "      mean_inference_ms: 0.7998404646102217\n",
            "      mean_raw_obs_processing_ms: 0.0827272803319134\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.252606646703484e-20\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3058015704154968\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0024550401140004396\n",
            "          model: {}\n",
            "          policy_loss: 0.004971771966665983\n",
            "          total_loss: 4.9192891120910645\n",
            "          vf_explained_var: 0.014258154667913914\n",
            "          vf_loss: 4.914317607879639\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 352000\n",
            "    num_agent_steps_trained: 352000\n",
            "    num_env_steps_sampled: 352000\n",
            "    num_env_steps_trained: 352000\n",
            "  iterations_since_restore: 88\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 352000\n",
            "  num_agent_steps_trained: 352000\n",
            "  num_env_steps_sampled: 352000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 352000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.62352941176471\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09644667990284543\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07812212339270347\n",
            "    mean_inference_ms: 1.0136289021106548\n",
            "    mean_raw_obs_processing_ms: 0.13394830577227118\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09644667990284543\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07812212339270347\n",
            "      mean_inference_ms: 1.0136289021106548\n",
            "      mean_raw_obs_processing_ms: 0.13394830577227118\n",
            "  time_since_restore: 821.226943731308\n",
            "  time_this_iter_s: 11.40109920501709\n",
            "  time_total_s: 821.226943731308\n",
            "  timers:\n",
            "    learn_throughput: 878.001\n",
            "    learn_time_ms: 4555.801\n",
            "    load_throughput: 12781666.921\n",
            "    load_time_ms: 0.313\n",
            "    training_iteration_time_ms: 7232.671\n",
            "    update_time_ms: 4.548\n",
            "  timestamp: 1656950991\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 352000\n",
            "  training_iteration: 88\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:09:56 (running for 00:14:03.43)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         821.227</td><td style=\"text-align: right;\">352000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 356000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 356000\n",
            "    num_agent_steps_trained: 356000\n",
            "    num_env_steps_sampled: 356000\n",
            "    num_env_steps_trained: 356000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-09-58\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2037\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.626303323351742e-20\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.29909202456474304\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0019434463465586305\n",
            "          model: {}\n",
            "          policy_loss: 0.00494446000084281\n",
            "          total_loss: 4.9192633628845215\n",
            "          vf_explained_var: 0.009139835834503174\n",
            "          vf_loss: 4.914318561553955\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 356000\n",
            "    num_agent_steps_trained: 356000\n",
            "    num_env_steps_sampled: 356000\n",
            "    num_env_steps_trained: 356000\n",
            "  iterations_since_restore: 89\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 356000\n",
            "  num_agent_steps_trained: 356000\n",
            "  num_env_steps_sampled: 356000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 356000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.06\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0964271533497001\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07811034551423929\n",
            "    mean_inference_ms: 1.0134495608136256\n",
            "    mean_raw_obs_processing_ms: 0.13393643646168502\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0964271533497001\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07811034551423929\n",
            "      mean_inference_ms: 1.0134495608136256\n",
            "      mean_raw_obs_processing_ms: 0.13393643646168502\n",
            "  time_since_restore: 828.4270389080048\n",
            "  time_this_iter_s: 7.200095176696777\n",
            "  time_total_s: 828.4270389080048\n",
            "  timers:\n",
            "    learn_throughput: 876.943\n",
            "    learn_time_ms: 4561.298\n",
            "    load_throughput: 12550281.269\n",
            "    load_time_ms: 0.319\n",
            "    training_iteration_time_ms: 7236.785\n",
            "    update_time_ms: 4.573\n",
            "  timestamp: 1656950998\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 356000\n",
            "  training_iteration: 89\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:03 (running for 00:14:10.51)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         828.427</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:08 (running for 00:14:15.58)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         828.427</td><td style=\"text-align: right;\">356000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 360000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 360000\n",
            "    num_agent_steps_trained: 360000\n",
            "    num_env_steps_sampled: 360000\n",
            "    num_env_steps_trained: 360000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-10-10\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2057\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07905172886659972\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05952077017341846\n",
            "      mean_inference_ms: 0.8000620120640087\n",
            "      mean_raw_obs_processing_ms: 0.0827358201636474\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 8.13151661675871e-21\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.3184168040752411\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0012393230572342873\n",
            "          model: {}\n",
            "          policy_loss: 0.005298600532114506\n",
            "          total_loss: 4.919614315032959\n",
            "          vf_explained_var: -0.06187180429697037\n",
            "          vf_loss: 4.914315700531006\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 360000\n",
            "    num_agent_steps_trained: 360000\n",
            "    num_env_steps_sampled: 360000\n",
            "    num_env_steps_trained: 360000\n",
            "  iterations_since_restore: 90\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 360000\n",
            "  num_agent_steps_trained: 360000\n",
            "  num_env_steps_sampled: 360000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 360000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.85624999999999\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09640683845617921\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0780950655981716\n",
            "    mean_inference_ms: 1.0132905026099275\n",
            "    mean_raw_obs_processing_ms: 0.13392136908104932\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09640683845617921\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0780950655981716\n",
            "      mean_inference_ms: 1.0132905026099275\n",
            "      mean_raw_obs_processing_ms: 0.13392136908104932\n",
            "  time_since_restore: 839.8626675605774\n",
            "  time_this_iter_s: 11.435628652572632\n",
            "  time_total_s: 839.8626675605774\n",
            "  timers:\n",
            "    learn_throughput: 875.079\n",
            "    learn_time_ms: 4571.014\n",
            "    load_throughput: 11800813.111\n",
            "    load_time_ms: 0.339\n",
            "    training_iteration_time_ms: 7248.973\n",
            "    update_time_ms: 4.596\n",
            "  timestamp: 1656951010\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 360000\n",
            "  training_iteration: 90\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:15 (running for 00:14:21.98)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         839.863</td><td style=\"text-align: right;\">360000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 364000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 364000\n",
            "    num_agent_steps_trained: 364000\n",
            "    num_env_steps_sampled: 364000\n",
            "    num_env_steps_trained: 364000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-10-17\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2077\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 4.065758308379355e-21\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.29793018102645874\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.004635763820260763\n",
            "          model: {}\n",
            "          policy_loss: 0.0030243447981774807\n",
            "          total_loss: 4.917344570159912\n",
            "          vf_explained_var: -0.2137368768453598\n",
            "          vf_loss: 4.91431999206543\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 364000\n",
            "    num_agent_steps_trained: 364000\n",
            "    num_env_steps_sampled: 364000\n",
            "    num_env_steps_trained: 364000\n",
            "  iterations_since_restore: 91\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 364000\n",
            "  num_agent_steps_trained: 364000\n",
            "  num_env_steps_sampled: 364000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 364000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.13636363636364\n",
            "    ram_util_percent: 22.099999999999998\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09639154427853297\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07808397249068735\n",
            "    mean_inference_ms: 1.013171883229855\n",
            "    mean_raw_obs_processing_ms: 0.13390907064170918\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09639154427853297\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07808397249068735\n",
            "      mean_inference_ms: 1.013171883229855\n",
            "      mean_raw_obs_processing_ms: 0.13390907064170918\n",
            "  time_since_restore: 847.1759655475616\n",
            "  time_this_iter_s: 7.313297986984253\n",
            "  time_total_s: 847.1759655475616\n",
            "  timers:\n",
            "    learn_throughput: 872.899\n",
            "    learn_time_ms: 4582.431\n",
            "    load_throughput: 11422396.514\n",
            "    load_time_ms: 0.35\n",
            "    training_iteration_time_ms: 7260.635\n",
            "    update_time_ms: 4.642\n",
            "  timestamp: 1656951017\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 364000\n",
            "  training_iteration: 91\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:22 (running for 00:14:29.48)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         847.176</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:27 (running for 00:14:34.49)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         847.176</td><td style=\"text-align: right;\">364000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 368000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 368000\n",
            "    num_agent_steps_trained: 368000\n",
            "    num_env_steps_sampled: 368000\n",
            "    num_env_steps_trained: 368000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-10-28\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2097\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0790612922500653\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0595160572461651\n",
            "      mean_inference_ms: 0.7999581245982916\n",
            "      mean_raw_obs_processing_ms: 0.0827335962889905\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.0328791541896775e-21\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.28156739473342896\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0017927230801433325\n",
            "          model: {}\n",
            "          policy_loss: 0.005207296460866928\n",
            "          total_loss: 4.919534206390381\n",
            "          vf_explained_var: -0.006058581173419952\n",
            "          vf_loss: 4.914327144622803\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 368000\n",
            "    num_agent_steps_trained: 368000\n",
            "    num_env_steps_sampled: 368000\n",
            "    num_env_steps_trained: 368000\n",
            "  iterations_since_restore: 92\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 368000\n",
            "  num_agent_steps_trained: 368000\n",
            "  num_env_steps_sampled: 368000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 368000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.98125\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09638209744280683\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07807389049859763\n",
            "    mean_inference_ms: 1.0130826773162938\n",
            "    mean_raw_obs_processing_ms: 0.13389957287600032\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09638209744280683\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07807389049859763\n",
            "      mean_inference_ms: 1.0130826773162938\n",
            "      mean_raw_obs_processing_ms: 0.13389957287600032\n",
            "  time_since_restore: 858.6098380088806\n",
            "  time_this_iter_s: 11.43387246131897\n",
            "  time_total_s: 858.6098380088806\n",
            "  timers:\n",
            "    learn_throughput: 871.911\n",
            "    learn_time_ms: 4587.624\n",
            "    load_throughput: 11525979.665\n",
            "    load_time_ms: 0.347\n",
            "    training_iteration_time_ms: 7274.901\n",
            "    update_time_ms: 4.651\n",
            "  timestamp: 1656951028\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 368000\n",
            "  training_iteration: 92\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:34 (running for 00:14:40.97)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">          858.61</td><td style=\"text-align: right;\">368000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 372000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 372000\n",
            "    num_agent_steps_trained: 372000\n",
            "    num_env_steps_sampled: 372000\n",
            "    num_env_steps_trained: 372000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-10-36\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2117\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.0164395770948388e-21\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.2828233242034912\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003893558867275715\n",
            "          model: {}\n",
            "          policy_loss: 0.003146987408399582\n",
            "          total_loss: 4.91746187210083\n",
            "          vf_explained_var: 0.005692868493497372\n",
            "          vf_loss: 4.9143147468566895\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 372000\n",
            "    num_agent_steps_trained: 372000\n",
            "    num_env_steps_sampled: 372000\n",
            "    num_env_steps_trained: 372000\n",
            "  iterations_since_restore: 93\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 372000\n",
            "  num_agent_steps_trained: 372000\n",
            "  num_env_steps_sampled: 372000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 372000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 89.8909090909091\n",
            "    ram_util_percent: 22.099999999999998\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09637890935541409\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07806118090209738\n",
            "    mean_inference_ms: 1.0130170093355548\n",
            "    mean_raw_obs_processing_ms: 0.13387917987559889\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09637890935541409\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07806118090209738\n",
            "      mean_inference_ms: 1.0130170093355548\n",
            "      mean_raw_obs_processing_ms: 0.13387917987559889\n",
            "  time_since_restore: 865.8700079917908\n",
            "  time_this_iter_s: 7.260169982910156\n",
            "  time_total_s: 865.8700079917908\n",
            "  timers:\n",
            "    learn_throughput: 871.747\n",
            "    learn_time_ms: 4588.485\n",
            "    load_throughput: 11355906.322\n",
            "    load_time_ms: 0.352\n",
            "    training_iteration_time_ms: 7275.312\n",
            "    update_time_ms: 4.687\n",
            "  timestamp: 1656951036\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 372000\n",
            "  training_iteration: 93\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:41 (running for 00:14:48.06)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">          865.87</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:46 (running for 00:14:53.14)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">          865.87</td><td style=\"text-align: right;\">372000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 376000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 376000\n",
            "    num_agent_steps_trained: 376000\n",
            "    num_env_steps_sampled: 376000\n",
            "    num_env_steps_trained: 376000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-10-47\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2137\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07904890738715523\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05952869576821242\n",
            "      mean_inference_ms: 0.8000156629289498\n",
            "      mean_raw_obs_processing_ms: 0.08272684181048211\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 5.082197885474194e-22\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.2805462181568146\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.003366242628544569\n",
            "          model: {}\n",
            "          policy_loss: 0.004014816600829363\n",
            "          total_loss: 4.918332099914551\n",
            "          vf_explained_var: 0.007555810734629631\n",
            "          vf_loss: 4.914317607879639\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 376000\n",
            "    num_agent_steps_trained: 376000\n",
            "    num_env_steps_sampled: 376000\n",
            "    num_env_steps_trained: 376000\n",
            "  iterations_since_restore: 94\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 376000\n",
            "  num_agent_steps_trained: 376000\n",
            "  num_env_steps_sampled: 376000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 376000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.6625\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0963826543685066\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07805518717984732\n",
            "    mean_inference_ms: 1.0129638061896333\n",
            "    mean_raw_obs_processing_ms: 0.1338604633846266\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0963826543685066\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07805518717984732\n",
            "      mean_inference_ms: 1.0129638061896333\n",
            "      mean_raw_obs_processing_ms: 0.1338604633846266\n",
            "  time_since_restore: 877.3036108016968\n",
            "  time_this_iter_s: 11.433602809906006\n",
            "  time_total_s: 877.3036108016968\n",
            "  timers:\n",
            "    learn_throughput: 870.023\n",
            "    learn_time_ms: 4597.58\n",
            "    load_throughput: 11212468.088\n",
            "    load_time_ms: 0.357\n",
            "    training_iteration_time_ms: 7284.494\n",
            "    update_time_ms: 5.07\n",
            "  timestamp: 1656951047\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 376000\n",
            "  training_iteration: 94\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:10:52 (running for 00:14:59.53)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         877.304</td><td style=\"text-align: right;\">376000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 380000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 380000\n",
            "    num_agent_steps_trained: 380000\n",
            "    num_env_steps_sampled: 380000\n",
            "    num_env_steps_trained: 380000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-10-55\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2157\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 2.541098942737097e-22\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.2750912606716156\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0015254507306963205\n",
            "          model: {}\n",
            "          policy_loss: 0.005581064149737358\n",
            "          total_loss: 4.9198994636535645\n",
            "          vf_explained_var: -0.011025396175682545\n",
            "          vf_loss: 4.914318084716797\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 380000\n",
            "    num_agent_steps_trained: 380000\n",
            "    num_env_steps_sampled: 380000\n",
            "    num_env_steps_trained: 380000\n",
            "  iterations_since_restore: 95\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 380000\n",
            "  num_agent_steps_trained: 380000\n",
            "  num_env_steps_sampled: 380000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 380000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 90.27000000000001\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09638921417289986\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07804961620983118\n",
            "    mean_inference_ms: 1.0129267464685654\n",
            "    mean_raw_obs_processing_ms: 0.13385197800799897\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09638921417289986\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07804961620983118\n",
            "      mean_inference_ms: 1.0129267464685654\n",
            "      mean_raw_obs_processing_ms: 0.13385197800799897\n",
            "  time_since_restore: 884.609457731247\n",
            "  time_this_iter_s: 7.305846929550171\n",
            "  time_total_s: 884.609457731247\n",
            "  timers:\n",
            "    learn_throughput: 870.894\n",
            "    learn_time_ms: 4592.981\n",
            "    load_throughput: 11583275.338\n",
            "    load_time_ms: 0.345\n",
            "    training_iteration_time_ms: 7284.522\n",
            "    update_time_ms: 5.079\n",
            "  timestamp: 1656951055\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 380000\n",
            "  training_iteration: 95\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:00 (running for 00:15:07.01)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         884.609</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:05 (running for 00:15:12.02)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         884.609</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 384000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 384000\n",
            "    num_agent_steps_trained: 384000\n",
            "    num_env_steps_sampled: 384000\n",
            "    num_env_steps_trained: 384000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-11-06\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2177\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07904588740444775\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05951218357701429\n",
            "      mean_inference_ms: 0.7998817767069627\n",
            "      mean_raw_obs_processing_ms: 0.08270518367075569\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.2705494713685484e-22\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.2919977903366089\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0020432656165212393\n",
            "          model: {}\n",
            "          policy_loss: 0.0040583196096122265\n",
            "          total_loss: 4.918373107910156\n",
            "          vf_explained_var: -0.15740500390529633\n",
            "          vf_loss: 4.914315700531006\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 384000\n",
            "    num_agent_steps_trained: 384000\n",
            "    num_env_steps_sampled: 384000\n",
            "    num_env_steps_trained: 384000\n",
            "  iterations_since_restore: 96\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 384000\n",
            "  num_agent_steps_trained: 384000\n",
            "  num_env_steps_sampled: 384000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 384000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.29411764705881\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0963935142603089\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07804308153914602\n",
            "    mean_inference_ms: 1.0128834974085261\n",
            "    mean_raw_obs_processing_ms: 0.13384199379121298\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0963935142603089\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07804308153914602\n",
            "      mean_inference_ms: 1.0128834974085261\n",
            "      mean_raw_obs_processing_ms: 0.13384199379121298\n",
            "  time_since_restore: 895.9822618961334\n",
            "  time_this_iter_s: 11.372804164886475\n",
            "  time_total_s: 895.9822618961334\n",
            "  timers:\n",
            "    learn_throughput: 871.111\n",
            "    learn_time_ms: 4591.836\n",
            "    load_throughput: 11612137.32\n",
            "    load_time_ms: 0.344\n",
            "    training_iteration_time_ms: 7284.555\n",
            "    update_time_ms: 4.737\n",
            "  timestamp: 1656951066\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 384000\n",
            "  training_iteration: 96\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:11 (running for 00:15:18.43)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         895.982</td><td style=\"text-align: right;\">384000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 388000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 388000\n",
            "    num_agent_steps_trained: 388000\n",
            "    num_env_steps_sampled: 388000\n",
            "    num_env_steps_trained: 388000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-11-13\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2197\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 6.352747356842742e-23\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.2978801727294922\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0024237215984612703\n",
            "          model: {}\n",
            "          policy_loss: 0.004370624665170908\n",
            "          total_loss: 4.918689727783203\n",
            "          vf_explained_var: 0.015312669798731804\n",
            "          vf_loss: 4.914318561553955\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 388000\n",
            "    num_agent_steps_trained: 388000\n",
            "    num_env_steps_sampled: 388000\n",
            "    num_env_steps_trained: 388000\n",
            "  iterations_since_restore: 97\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 388000\n",
            "  num_agent_steps_trained: 388000\n",
            "  num_env_steps_sampled: 388000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 388000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.33\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09640017725791657\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07804079903736748\n",
            "    mean_inference_ms: 1.0128605257205465\n",
            "    mean_raw_obs_processing_ms: 0.13383447799917808\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09640017725791657\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07804079903736748\n",
            "      mean_inference_ms: 1.0128605257205465\n",
            "      mean_raw_obs_processing_ms: 0.13383447799917808\n",
            "  time_since_restore: 903.2204778194427\n",
            "  time_this_iter_s: 7.238215923309326\n",
            "  time_total_s: 903.2204778194427\n",
            "  timers:\n",
            "    learn_throughput: 872.786\n",
            "    learn_time_ms: 4583.026\n",
            "    load_throughput: 11362828.31\n",
            "    load_time_ms: 0.352\n",
            "    training_iteration_time_ms: 7281.778\n",
            "    update_time_ms: 4.428\n",
            "  timestamp: 1656951073\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 388000\n",
            "  training_iteration: 97\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:18 (running for 00:15:25.53)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">          903.22</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:23 (running for 00:15:30.60)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">          903.22</td><td style=\"text-align: right;\">388000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 392000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 392000\n",
            "    num_agent_steps_trained: 392000\n",
            "    num_env_steps_sampled: 392000\n",
            "    num_env_steps_trained: 392000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-11-25\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2217\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07904429182943004\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05951602483432252\n",
            "      mean_inference_ms: 0.8002594710026228\n",
            "      mean_raw_obs_processing_ms: 0.08269046732544058\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 3.176373678421371e-23\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.27490338683128357\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0015059186844155192\n",
            "          model: {}\n",
            "          policy_loss: 0.004739123862236738\n",
            "          total_loss: 4.919055461883545\n",
            "          vf_explained_var: -0.2258063703775406\n",
            "          vf_loss: 4.914316177368164\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 392000\n",
            "    num_agent_steps_trained: 392000\n",
            "    num_env_steps_sampled: 392000\n",
            "    num_env_steps_trained: 392000\n",
            "  iterations_since_restore: 98\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 392000\n",
            "  num_agent_steps_trained: 392000\n",
            "  num_env_steps_sampled: 392000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 392000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.73125\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09640215618098939\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.0780354461470346\n",
            "    mean_inference_ms: 1.0127764298165198\n",
            "    mean_raw_obs_processing_ms: 0.1338214270235852\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09640215618098939\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0780354461470346\n",
            "      mean_inference_ms: 1.0127764298165198\n",
            "      mean_raw_obs_processing_ms: 0.1338214270235852\n",
            "  time_since_restore: 914.6329629421234\n",
            "  time_this_iter_s: 11.412485122680664\n",
            "  time_total_s: 914.6329629421234\n",
            "  timers:\n",
            "    learn_throughput: 872.64\n",
            "    learn_time_ms: 4583.792\n",
            "    load_throughput: 11375154.926\n",
            "    load_time_ms: 0.352\n",
            "    training_iteration_time_ms: 7275.7\n",
            "    update_time_ms: 4.719\n",
            "  timestamp: 1656951085\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 392000\n",
            "  training_iteration: 98\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:30 (running for 00:15:36.98)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         914.633</td><td style=\"text-align: right;\">392000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 396000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 396000\n",
            "    num_agent_steps_trained: 396000\n",
            "    num_env_steps_sampled: 396000\n",
            "    num_env_steps_trained: 396000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-11-32\n",
            "  done: false\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2237\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 1.5881868392106856e-23\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.30046510696411133\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0019985290709882975\n",
            "          model: {}\n",
            "          policy_loss: 0.005714588798582554\n",
            "          total_loss: 4.920039653778076\n",
            "          vf_explained_var: 0.01987658254802227\n",
            "          vf_loss: 4.91432523727417\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 396000\n",
            "    num_agent_steps_trained: 396000\n",
            "    num_env_steps_sampled: 396000\n",
            "    num_env_steps_trained: 396000\n",
            "  iterations_since_restore: 99\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 396000\n",
            "  num_agent_steps_trained: 396000\n",
            "  num_env_steps_sampled: 396000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 396000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 88.58181818181818\n",
            "    ram_util_percent: 22.099999999999998\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.09640102585441582\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07803359581756598\n",
            "    mean_inference_ms: 1.0126894062066958\n",
            "    mean_raw_obs_processing_ms: 0.13380028451909326\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09640102585441582\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07803359581756598\n",
            "      mean_inference_ms: 1.0126894062066958\n",
            "      mean_raw_obs_processing_ms: 0.13380028451909326\n",
            "  time_since_restore: 921.9329912662506\n",
            "  time_this_iter_s: 7.300028324127197\n",
            "  time_total_s: 921.9329912662506\n",
            "  timers:\n",
            "    learn_throughput: 870.953\n",
            "    learn_time_ms: 4592.672\n",
            "    load_throughput: 11522021.839\n",
            "    load_time_ms: 0.347\n",
            "    training_iteration_time_ms: 7285.881\n",
            "    update_time_ms: 5.334\n",
            "  timestamp: 1656951092\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 396000\n",
            "  training_iteration: 99\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:37 (running for 00:15:44.47)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         921.933</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:42 (running for 00:15:49.49)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>RUNNING </td><td>172.28.0.2:622</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         921.933</td><td style=\"text-align: right;\">396000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for PPO_CartPole-v0_c870f_00000:\n",
            "  agent_timesteps_total: 400000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 400000\n",
            "    num_agent_steps_trained: 400000\n",
            "    num_env_steps_sampled: 400000\n",
            "    num_env_steps_trained: 400000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-11-43\n",
            "  done: true\n",
            "  episode_len_mean: 200.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 200.0\n",
            "  episode_reward_min: 200.0\n",
            "  episodes_this_iter: 20\n",
            "  episodes_total: 2257\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.07902815071812883\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.05949973726108511\n",
            "      mean_inference_ms: 0.7999399497409472\n",
            "      mean_raw_obs_processing_ms: 0.08265780696936846\n",
            "    timesteps_this_iter: 4000\n",
            "  experiment_id: 4393e3cab67945d999b9f89b5220132f\n",
            "  hostname: fb50265e9b74\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_kl_coeff: 7.940934196053428e-24\n",
            "          cur_lr: 4.999999873689376e-05\n",
            "          entropy: 0.277404248714447\n",
            "          entropy_coeff: 0.0\n",
            "          kl: 0.0023934359196573496\n",
            "          model: {}\n",
            "          policy_loss: 0.003867305815219879\n",
            "          total_loss: 4.918182373046875\n",
            "          vf_explained_var: 0.033973727375268936\n",
            "          vf_loss: 4.914315223693848\n",
            "        num_agent_steps_trained: 128.0\n",
            "    num_agent_steps_sampled: 400000\n",
            "    num_agent_steps_trained: 400000\n",
            "    num_env_steps_sampled: 400000\n",
            "    num_env_steps_trained: 400000\n",
            "  iterations_since_restore: 100\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 400000\n",
            "  num_agent_steps_trained: 400000\n",
            "  num_env_steps_sampled: 400000\n",
            "  num_env_steps_sampled_this_iter: 4000\n",
            "  num_env_steps_trained: 400000\n",
            "  num_env_steps_trained_this_iter: 4000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 81.7125\n",
            "    ram_util_percent: 22.1\n",
            "  pid: 622\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.0963951280209924\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.07803464596879602\n",
            "    mean_inference_ms: 1.0125740751240622\n",
            "    mean_raw_obs_processing_ms: 0.13377306311804682\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 200.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 200.0\n",
            "    episode_reward_min: 200.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0963951280209924\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07803464596879602\n",
            "      mean_inference_ms: 1.0125740751240622\n",
            "      mean_raw_obs_processing_ms: 0.13377306311804682\n",
            "  time_since_restore: 933.1945621967316\n",
            "  time_this_iter_s: 11.261570930480957\n",
            "  time_total_s: 933.1945621967316\n",
            "  timers:\n",
            "    learn_throughput: 871.404\n",
            "    learn_time_ms: 4590.291\n",
            "    load_throughput: 12263150.355\n",
            "    load_time_ms: 0.326\n",
            "    training_iteration_time_ms: 7280.036\n",
            "    update_time_ms: 5.576\n",
            "  timestamp: 1656951103\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 400000\n",
            "  training_iteration: 100\n",
            "  trial_id: c870f_00000\n",
            "  warmup_time: 9.875841856002808\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:11:43 (running for 00:15:50.67)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/80 CPUs, 0/2 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /root/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>PPO_CartPole-v0_c870f_00000</td><td>TERMINATED</td><td>172.28.0.2:622</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         933.195</td><td style=\"text-align: right;\">400000</td><td style=\"text-align: right;\">     200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               200</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-04 16:11:44,399\tINFO tune.py:748 -- Total run time: 951.18 seconds (950.63 seconds for the tuning loop).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UAfXqnzB_Z8R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(dfs.values())[0])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "WEJFihr0_bdB",
        "outputId": "480384c7-9211-459e-a0fa-c55d1fa20f1d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
              "0                 74.0                 9.0            21.818681   \n",
              "1                200.0                 9.0            42.830000   \n",
              "2                200.0                 9.0            69.520000   \n",
              "3                200.0                 9.0            97.460000   \n",
              "4                200.0                 9.0           125.450000   \n",
              "..                 ...                 ...                  ...   \n",
              "95               200.0               200.0           200.000000   \n",
              "96               200.0               200.0           200.000000   \n",
              "97               200.0               200.0           200.000000   \n",
              "98               200.0               200.0           200.000000   \n",
              "99               200.0               200.0           200.000000   \n",
              "\n",
              "    episode_len_mean  episodes_this_iter  num_healthy_workers  \\\n",
              "0          21.818681                 182                    2   \n",
              "1          42.830000                  88                    2   \n",
              "2          69.520000                  36                    2   \n",
              "3          97.460000                  26                    2   \n",
              "4         125.450000                  20                    2   \n",
              "..               ...                 ...                  ...   \n",
              "95        200.000000                  20                    2   \n",
              "96        200.000000                  20                    2   \n",
              "97        200.000000                  20                    2   \n",
              "98        200.000000                  20                    2   \n",
              "99        200.000000                  20                    2   \n",
              "\n",
              "    num_agent_steps_sampled  num_agent_steps_trained  num_env_steps_sampled  \\\n",
              "0                      4000                     4000                   4000   \n",
              "1                      8000                     8000                   8000   \n",
              "2                     12000                    12000                  12000   \n",
              "3                     16000                    16000                  16000   \n",
              "4                     20000                    20000                  20000   \n",
              "..                      ...                      ...                    ...   \n",
              "95                   384000                   384000                 384000   \n",
              "96                   388000                   388000                 388000   \n",
              "97                   392000                   392000                 392000   \n",
              "98                   396000                   396000                 396000   \n",
              "99                   400000                   400000                 400000   \n",
              "\n",
              "    num_env_steps_trained  ...  \\\n",
              "0                    4000  ...   \n",
              "1                    8000  ...   \n",
              "2                   12000  ...   \n",
              "3                   16000  ...   \n",
              "4                   20000  ...   \n",
              "..                    ...  ...   \n",
              "95                 384000  ...   \n",
              "96                 388000  ...   \n",
              "97                 392000  ...   \n",
              "98                 396000  ...   \n",
              "99                 400000  ...   \n",
              "\n",
              "    info/learner/default_policy/num_agent_steps_trained  \\\n",
              "0                                               128.0     \n",
              "1                                               128.0     \n",
              "2                                               128.0     \n",
              "3                                               128.0     \n",
              "4                                               128.0     \n",
              "..                                                ...     \n",
              "95                                              128.0     \n",
              "96                                              128.0     \n",
              "97                                              128.0     \n",
              "98                                              128.0     \n",
              "99                                              128.0     \n",
              "\n",
              "    info/learner/default_policy/learner_stats/cur_kl_coeff  \\\n",
              "0                                        2.000000e-01        \n",
              "1                                        3.000000e-01        \n",
              "2                                        3.000000e-01        \n",
              "3                                        3.000000e-01        \n",
              "4                                        1.500000e-01        \n",
              "..                                                ...        \n",
              "95                                       1.270549e-22        \n",
              "96                                       6.352747e-23        \n",
              "97                                       3.176374e-23        \n",
              "98                                       1.588187e-23        \n",
              "99                                       7.940934e-24        \n",
              "\n",
              "    info/learner/default_policy/learner_stats/cur_lr  \\\n",
              "0                                            0.00005   \n",
              "1                                            0.00005   \n",
              "2                                            0.00005   \n",
              "3                                            0.00005   \n",
              "4                                            0.00005   \n",
              "..                                               ...   \n",
              "95                                           0.00005   \n",
              "96                                           0.00005   \n",
              "97                                           0.00005   \n",
              "98                                           0.00005   \n",
              "99                                           0.00005   \n",
              "\n",
              "    info/learner/default_policy/learner_stats/total_loss  \\\n",
              "0                                            8.772823      \n",
              "1                                            8.754086      \n",
              "2                                            9.313850      \n",
              "3                                            9.413007      \n",
              "4                                            9.507740      \n",
              "..                                                ...      \n",
              "95                                           4.918373      \n",
              "96                                           4.918690      \n",
              "97                                           4.919055      \n",
              "98                                           4.920040      \n",
              "99                                           4.918182      \n",
              "\n",
              "    info/learner/default_policy/learner_stats/policy_loss  \\\n",
              "0                                           -0.045288       \n",
              "1                                           -0.030288       \n",
              "2                                           -0.021817       \n",
              "3                                           -0.013853       \n",
              "4                                           -0.011806       \n",
              "..                                                ...       \n",
              "95                                           0.004058       \n",
              "96                                           0.004371       \n",
              "97                                           0.004739       \n",
              "98                                           0.005715       \n",
              "99                                           0.003867       \n",
              "\n",
              "    info/learner/default_policy/learner_stats/vf_loss  \\\n",
              "0                                            8.812579   \n",
              "1                                            8.779249   \n",
              "2                                            9.333462   \n",
              "3                                            9.425679   \n",
              "4                                            9.518681   \n",
              "..                                                ...   \n",
              "95                                           4.914316   \n",
              "96                                           4.914319   \n",
              "97                                           4.914316   \n",
              "98                                           4.914325   \n",
              "99                                           4.914315   \n",
              "\n",
              "    info/learner/default_policy/learner_stats/vf_explained_var  \\\n",
              "0                                            0.005504            \n",
              "1                                            0.042217            \n",
              "2                                            0.082484            \n",
              "3                                            0.071468            \n",
              "4                                            0.021475            \n",
              "..                                                ...            \n",
              "95                                          -0.157405            \n",
              "96                                           0.015313            \n",
              "97                                          -0.225806            \n",
              "98                                           0.019877            \n",
              "99                                           0.033974            \n",
              "\n",
              "   info/learner/default_policy/learner_stats/kl  \\\n",
              "0                                      0.027662   \n",
              "1                                      0.017079   \n",
              "2                                      0.007356   \n",
              "3                                      0.003936   \n",
              "4                                      0.005764   \n",
              "..                                          ...   \n",
              "95                                     0.002043   \n",
              "96                                     0.002424   \n",
              "97                                     0.001506   \n",
              "98                                     0.001999   \n",
              "99                                     0.002393   \n",
              "\n",
              "   info/learner/default_policy/learner_stats/entropy  \\\n",
              "0                                           0.666665   \n",
              "1                                           0.614745   \n",
              "2                                           0.573693   \n",
              "3                                           0.568746   \n",
              "4                                           0.567672   \n",
              "..                                               ...   \n",
              "95                                          0.291998   \n",
              "96                                          0.297880   \n",
              "97                                          0.274903   \n",
              "98                                          0.300465   \n",
              "99                                          0.277404   \n",
              "\n",
              "   info/learner/default_policy/learner_stats/entropy_coeff  \n",
              "0                                                 0.0       \n",
              "1                                                 0.0       \n",
              "2                                                 0.0       \n",
              "3                                                 0.0       \n",
              "4                                                 0.0       \n",
              "..                                                ...       \n",
              "95                                                0.0       \n",
              "96                                                0.0       \n",
              "97                                                0.0       \n",
              "98                                                0.0       \n",
              "99                                                0.0       \n",
              "\n",
              "[100 rows x 75 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1cdc7583-2759-4f00-87a2-c0f9013ec0f4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_reward_max</th>\n",
              "      <th>episode_reward_min</th>\n",
              "      <th>episode_reward_mean</th>\n",
              "      <th>episode_len_mean</th>\n",
              "      <th>episodes_this_iter</th>\n",
              "      <th>num_healthy_workers</th>\n",
              "      <th>num_agent_steps_sampled</th>\n",
              "      <th>num_agent_steps_trained</th>\n",
              "      <th>num_env_steps_sampled</th>\n",
              "      <th>num_env_steps_trained</th>\n",
              "      <th>...</th>\n",
              "      <th>info/learner/default_policy/num_agent_steps_trained</th>\n",
              "      <th>info/learner/default_policy/learner_stats/cur_kl_coeff</th>\n",
              "      <th>info/learner/default_policy/learner_stats/cur_lr</th>\n",
              "      <th>info/learner/default_policy/learner_stats/total_loss</th>\n",
              "      <th>info/learner/default_policy/learner_stats/policy_loss</th>\n",
              "      <th>info/learner/default_policy/learner_stats/vf_loss</th>\n",
              "      <th>info/learner/default_policy/learner_stats/vf_explained_var</th>\n",
              "      <th>info/learner/default_policy/learner_stats/kl</th>\n",
              "      <th>info/learner/default_policy/learner_stats/entropy</th>\n",
              "      <th>info/learner/default_policy/learner_stats/entropy_coeff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>74.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>21.818681</td>\n",
              "      <td>21.818681</td>\n",
              "      <td>182</td>\n",
              "      <td>2</td>\n",
              "      <td>4000</td>\n",
              "      <td>4000</td>\n",
              "      <td>4000</td>\n",
              "      <td>4000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>2.000000e-01</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>8.772823</td>\n",
              "      <td>-0.045288</td>\n",
              "      <td>8.812579</td>\n",
              "      <td>0.005504</td>\n",
              "      <td>0.027662</td>\n",
              "      <td>0.666665</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>200.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>42.830000</td>\n",
              "      <td>42.830000</td>\n",
              "      <td>88</td>\n",
              "      <td>2</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>3.000000e-01</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>8.754086</td>\n",
              "      <td>-0.030288</td>\n",
              "      <td>8.779249</td>\n",
              "      <td>0.042217</td>\n",
              "      <td>0.017079</td>\n",
              "      <td>0.614745</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>200.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>69.520000</td>\n",
              "      <td>69.520000</td>\n",
              "      <td>36</td>\n",
              "      <td>2</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000</td>\n",
              "      <td>12000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>3.000000e-01</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>9.313850</td>\n",
              "      <td>-0.021817</td>\n",
              "      <td>9.333462</td>\n",
              "      <td>0.082484</td>\n",
              "      <td>0.007356</td>\n",
              "      <td>0.573693</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>200.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>97.460000</td>\n",
              "      <td>97.460000</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>16000</td>\n",
              "      <td>16000</td>\n",
              "      <td>16000</td>\n",
              "      <td>16000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>3.000000e-01</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>9.413007</td>\n",
              "      <td>-0.013853</td>\n",
              "      <td>9.425679</td>\n",
              "      <td>0.071468</td>\n",
              "      <td>0.003936</td>\n",
              "      <td>0.568746</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>200.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>125.450000</td>\n",
              "      <td>125.450000</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>20000</td>\n",
              "      <td>20000</td>\n",
              "      <td>20000</td>\n",
              "      <td>20000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1.500000e-01</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>9.507740</td>\n",
              "      <td>-0.011806</td>\n",
              "      <td>9.518681</td>\n",
              "      <td>0.021475</td>\n",
              "      <td>0.005764</td>\n",
              "      <td>0.567672</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>384000</td>\n",
              "      <td>384000</td>\n",
              "      <td>384000</td>\n",
              "      <td>384000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1.270549e-22</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>4.918373</td>\n",
              "      <td>0.004058</td>\n",
              "      <td>4.914316</td>\n",
              "      <td>-0.157405</td>\n",
              "      <td>0.002043</td>\n",
              "      <td>0.291998</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>388000</td>\n",
              "      <td>388000</td>\n",
              "      <td>388000</td>\n",
              "      <td>388000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>6.352747e-23</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>4.918690</td>\n",
              "      <td>0.004371</td>\n",
              "      <td>4.914319</td>\n",
              "      <td>0.015313</td>\n",
              "      <td>0.002424</td>\n",
              "      <td>0.297880</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>392000</td>\n",
              "      <td>392000</td>\n",
              "      <td>392000</td>\n",
              "      <td>392000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>3.176374e-23</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>4.919055</td>\n",
              "      <td>0.004739</td>\n",
              "      <td>4.914316</td>\n",
              "      <td>-0.225806</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.274903</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>396000</td>\n",
              "      <td>396000</td>\n",
              "      <td>396000</td>\n",
              "      <td>396000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>1.588187e-23</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>4.920040</td>\n",
              "      <td>0.005715</td>\n",
              "      <td>4.914325</td>\n",
              "      <td>0.019877</td>\n",
              "      <td>0.001999</td>\n",
              "      <td>0.300465</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>200.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>200.000000</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>400000</td>\n",
              "      <td>400000</td>\n",
              "      <td>400000</td>\n",
              "      <td>400000</td>\n",
              "      <td>...</td>\n",
              "      <td>128.0</td>\n",
              "      <td>7.940934e-24</td>\n",
              "      <td>0.00005</td>\n",
              "      <td>4.918182</td>\n",
              "      <td>0.003867</td>\n",
              "      <td>4.914315</td>\n",
              "      <td>0.033974</td>\n",
              "      <td>0.002393</td>\n",
              "      <td>0.277404</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 75 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cdc7583-2759-4f00-87a2-c0f9013ec0f4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1cdc7583-2759-4f00-87a2-c0f9013ec0f4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1cdc7583-2759-4f00-87a2-c0f9013ec0f4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('ppo.csv') "
      ],
      "metadata": {
        "id": "05WgK27i_r5i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = analysis.get_best_checkpoint(\n",
        "    metric=\"episode_reward_mean\", \n",
        "    mode=\"max\", \n",
        "    trial=analysis.trials[0]\n",
        ")"
      ],
      "metadata": {
        "id": "qCaFjh0uAq_Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray.rllib.agents.ppo as ppo"
      ],
      "metadata": {
        "id": "WcYjsEoy4lYF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = ppo.PPOTrainer(config={\"env\":\"CartPole-v0\",\"evaluation_interval\":2,\"evaluation_num_episodes\": 20})\n",
        "agent.restore(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDJRCOak4u8z",
        "outputId": "c3f2acb9-771f-4eab-adf5-49f3d8851d77"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-04 16:18:31,251\tWARNING logger.py:337 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
            "2022-07-04 16:18:31,258\tINFO trainer.py:2333 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "2022-07-04 16:18:31,261\tWARNING deprecation.py:47 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
            "2022-07-04 16:18:31,263\tINFO ppo.py:415 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
            "2022-07-04 16:18:31,264\tINFO trainer.py:906 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2022-07-04 16:18:46,431\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
            "2022-07-04 16:18:47,254\tINFO trainable.py:163 -- Trainable.setup took 15.999 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2022-07-04 16:18:47,256\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
            "2022-07-04 16:18:47,323\tINFO trainable.py:589 -- Restored on 172.28.0.2 from checkpoint: /root/ray_results/PPO/PPO_CartPole-v0_c870f_00000_0_2022-07-04_15-55-53/checkpoint_000100/checkpoint-100\n",
            "2022-07-04 16:18:47,327\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 933.1945621967316, '_episodes_total': 2257}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "metadata": {
        "id": "1yh2PoDQFJZ_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "metadata": {
        "id": "FCeEKsp28XZv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rendering Dependencies\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# Gym Dependencies\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym[box2d] > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Czb4iJmkBO8a",
        "outputId": "392f384a-7149-4edb-b1ae-d35bed431b54"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-63.1.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 27.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed setuptools-63.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Google Colab needs to render the environment to a virtual display\n",
        "# we will record this as a video and play it after the training has finished\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[-1]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "EMp9pBMiBVeb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game = \"CartPole-v0\"\n",
        "\n",
        "nr_of_runs = 1\n",
        "current_run = 1\n",
        "\n",
        "if current_run == nr_of_runs:\n",
        "  env = wrap_env(gym.make(game))\n",
        "else:\n",
        "  env = gym.make(game)\n",
        "observation = env.reset()\n",
        "timestep = 0\n",
        "\n",
        "while current_run < nr_of_runs+1:\n",
        "    # render the current frame to the video recorder of Google Colab\n",
        "    if current_run == nr_of_runs:\n",
        "      env.render()\n",
        "    \n",
        "    # your agent goes here \n",
        "    # action_space.sample() results in a random action being picked\n",
        "    action = agent.compute_action(observation)\n",
        "    \n",
        "    # apply the action to the real environment and forward the game\n",
        "    observation, reward, done, info = env.step(action) \n",
        "    timestep += 1\n",
        "    \n",
        "    if done:\n",
        "      # for Monte Carlo Method you will need to update your value matrix here\n",
        "      # Temporal Difference Learning updates the value matrix in every step\n",
        "      \n",
        "      # this test ends after 'nr_of_runs' (default = 10)\n",
        "      # change the variable at the top if you want to train longer (recommended) \n",
        "      print(\"run: \"+ str(current_run) + \" took \" + str(timestep) + \" timesteps\")\n",
        "      current_run += 1\n",
        "      timestep = 0\n",
        "      if current_run == nr_of_runs:\n",
        "        # record the last run\n",
        "        env.close()\n",
        "        env = wrap_env(gym.make(game))\n",
        "      else:\n",
        "        env.close()\n",
        "        env = gym.make(game)\n",
        "      \n",
        "      observation = env.reset()\n",
        "show_video()  #only shows the last run\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "VFJrKFeWBsHK",
        "outputId": "43f1bc8a-e6ec-4957-ad71-66d17cd952db"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run: 1 took 200 timesteps\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}