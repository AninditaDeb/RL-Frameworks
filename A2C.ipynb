{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install lz4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmHSKZHWAGD5",
        "outputId": "09307dc9-6417-46ca-fca1-35c56660d271"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting lz4\n",
            "  Downloading lz4-4.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "Successfully installed lz4-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ray"
      ],
      "metadata": {
        "id": "DmeqIoXewIuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48e2f0e6-75b7-40d9-a5ab-f62e563e7615"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray\n",
            "  Downloading ray-1.13.0-cp37-cp37m-manylinux2014_x86_64.whl (54.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 54.5 MB 141 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray) (3.17.3)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray) (7.1.2)\n",
            "Collecting aiosignal\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray) (21.4.0)\n",
            "Collecting virtualenv\n",
            "  Downloading virtualenv-20.15.1-py2.py3-none-any.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray) (3.13)\n",
            "Collecting frozenlist\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 57.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray) (3.7.1)\n",
            "Collecting grpcio<=1.43.0,>=1.28.1\n",
            "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 42.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray) (1.0.4)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray) (4.11.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray) (2022.6.15)\n",
            "Collecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 59.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: platformdirs, frozenlist, distlib, virtualenv, grpcio, aiosignal, ray\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.46.3\n",
            "    Uninstalling grpcio-1.46.3:\n",
            "      Successfully uninstalled grpcio-1.46.3\n",
            "Successfully installed aiosignal-1.2.0 distlib-0.3.4 frozenlist-1.3.0 grpcio-1.43.0 platformdirs-2.5.2 ray-1.13.0 virtualenv-20.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "ray.init(num_gpus=2, num_cpus=80,ignore_reinit_error=True)"
      ],
      "metadata": {
        "id": "gpU0Ekk4uhnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239e3e76-b9ba-4453-d602-fdbde680e123"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RayContext(dashboard_url='', python_version='3.7.13', ray_version='1.13.0', ray_commit='e4ce38d001dbbe09cd21c497fedd03d692b2be3e', address_info={'node_ip_address': '172.28.0.2', 'raylet_ip_address': '172.28.0.2', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-07-04_16-52-01_640759_59/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-07-04_16-52-01_640759_59/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-07-04_16-52-01_640759_59', 'metrics_export_port': 64302, 'gcs_address': '172.28.0.2:60225', 'address': '172.28.0.2:60225', 'node_id': 'f4d32b7f1dbe896f8f0e21a0484977290ee3acc42b18974156604a37'})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray.rllib.agents.a3c as a3c"
      ],
      "metadata": {
        "id": "qSARSd4Xuil9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install \"ray[tune]\""
      ],
      "metadata": {
        "id": "bpXL8Y6F4ATY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ray import tune"
      ],
      "metadata": {
        "id": "H51SznLXxUEA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Execute this, only when you want to save Trained Checkpoints**"
      ],
      "metadata": {
        "id": "3lcYMGAW5o6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "metadata": {
        "id": "y19SgT-QJfoN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopper(trial_id, result):\n",
        "    return result[\"training_iteration\"] >= 100"
      ],
      "metadata": {
        "id": "MrLYLk62Jtk_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analysis = tune.run(\n",
        "    \"A2C\",\n",
        "    mode=\"max\",\n",
        "    checkpoint_at_end=True,\n",
        "    stop=stopper,\n",
        "    config={\"env\":\"CartPole-v0\",\"evaluation_interval\":2,\"evaluation_num_episodes\": 20},\n",
        ")\n",
        "\n",
        "dfs = analysis.trial_dataframes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gWVdjMBdJyRE",
        "outputId": "235378ee-c0a8-428d-a3a6-636e0c9f2683"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-04 16:57:30,501\tINFO logger.py:630 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
            "2022-07-04 16:57:30,507\tWARNING callback.py:106 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n",
            "\u001b[2m\u001b[36m(A2CTrainer pid=483)\u001b[0m 2022-07-04 16:57:37,148\tINFO trainer.py:2333 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "\u001b[2m\u001b[36m(A2CTrainer pid=483)\u001b[0m 2022-07-04 16:57:37,149\tWARNING deprecation.py:47 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
            "\u001b[2m\u001b[36m(A2CTrainer pid=483)\u001b[0m 2022-07-04 16:57:37,150\tINFO trainer.py:906 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "\u001b[2m\u001b[36m(A2CTrainer pid=483)\u001b[0m 2022-07-04 16:57:48,778\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:57:49 (running for 00:00:19.10)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(A2CTrainer pid=483)\u001b[0m 2022-07-04 16:57:49,605\tINFO trainable.py:163 -- Trainable.setup took 12.457 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "\u001b[2m\u001b[36m(A2CTrainer pid=483)\u001b[0m 2022-07-04 16:57:49,606\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:57:54 (running for 00:00:24.12)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:57:59 (running for 00:00:29.13)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 8000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_agent_steps_trained: 8000\n",
            "    num_env_steps_sampled: 8000\n",
            "    num_env_steps_trained: 8000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-57-59\n",
            "  done: false\n",
            "  episode_len_mean: 26.210526315789473\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 87.0\n",
            "  episode_reward_mean: 26.210526315789473\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 304\n",
            "  episodes_total: 304\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 135.5211944580078\n",
            "          policy_loss: 990.509521484375\n",
            "          var_gnorm: 22.649471282958984\n",
            "          vf_explained_var: -0.0013632774353027344\n",
            "          vf_loss: 7529.3291015625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 8000\n",
            "    num_agent_steps_trained: 8000\n",
            "    num_env_steps_sampled: 8000\n",
            "    num_env_steps_trained: 8000\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 8000\n",
            "  num_agent_steps_trained: 8000\n",
            "  num_env_steps_sampled: 8000\n",
            "  num_env_steps_sampled_this_iter: 8000\n",
            "  num_env_steps_trained: 8000\n",
            "  num_env_steps_trained_this_iter: 8000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 91.88666666666668\n",
            "    ram_util_percent: 21.93333333333333\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1454028313571959\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11671676528668817\n",
            "    mean_inference_ms: 1.4422462169670451\n",
            "    mean_raw_obs_processing_ms: 0.3189991873760457\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 26.210526315789473\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 87.0\n",
            "    episode_reward_mean: 26.210526315789473\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 304\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 30\n",
            "      - 18\n",
            "      - 42\n",
            "      - 21\n",
            "      - 12\n",
            "      - 18\n",
            "      - 14\n",
            "      - 17\n",
            "      - 12\n",
            "      - 41\n",
            "      - 26\n",
            "      - 15\n",
            "      - 26\n",
            "      - 16\n",
            "      - 14\n",
            "      - 15\n",
            "      - 13\n",
            "      - 24\n",
            "      - 18\n",
            "      - 32\n",
            "      - 17\n",
            "      - 14\n",
            "      - 57\n",
            "      - 17\n",
            "      - 23\n",
            "      - 10\n",
            "      - 15\n",
            "      - 18\n",
            "      - 18\n",
            "      - 12\n",
            "      - 13\n",
            "      - 11\n",
            "      - 16\n",
            "      - 14\n",
            "      - 15\n",
            "      - 11\n",
            "      - 43\n",
            "      - 27\n",
            "      - 40\n",
            "      - 21\n",
            "      - 18\n",
            "      - 17\n",
            "      - 10\n",
            "      - 19\n",
            "      - 47\n",
            "      - 16\n",
            "      - 25\n",
            "      - 22\n",
            "      - 30\n",
            "      - 19\n",
            "      - 24\n",
            "      - 23\n",
            "      - 40\n",
            "      - 30\n",
            "      - 12\n",
            "      - 21\n",
            "      - 27\n",
            "      - 21\n",
            "      - 46\n",
            "      - 62\n",
            "      - 21\n",
            "      - 33\n",
            "      - 14\n",
            "      - 14\n",
            "      - 12\n",
            "      - 16\n",
            "      - 79\n",
            "      - 21\n",
            "      - 14\n",
            "      - 19\n",
            "      - 12\n",
            "      - 12\n",
            "      - 15\n",
            "      - 11\n",
            "      - 26\n",
            "      - 14\n",
            "      - 21\n",
            "      - 11\n",
            "      - 20\n",
            "      - 44\n",
            "      - 12\n",
            "      - 39\n",
            "      - 31\n",
            "      - 27\n",
            "      - 21\n",
            "      - 33\n",
            "      - 35\n",
            "      - 16\n",
            "      - 13\n",
            "      - 10\n",
            "      - 52\n",
            "      - 19\n",
            "      - 38\n",
            "      - 12\n",
            "      - 80\n",
            "      - 54\n",
            "      - 24\n",
            "      - 18\n",
            "      - 52\n",
            "      - 15\n",
            "      - 23\n",
            "      - 58\n",
            "      - 15\n",
            "      - 13\n",
            "      - 26\n",
            "      - 14\n",
            "      - 14\n",
            "      - 56\n",
            "      - 18\n",
            "      - 34\n",
            "      - 20\n",
            "      - 11\n",
            "      - 20\n",
            "      - 25\n",
            "      - 34\n",
            "      - 24\n",
            "      - 11\n",
            "      - 24\n",
            "      - 15\n",
            "      - 45\n",
            "      - 55\n",
            "      - 39\n",
            "      - 20\n",
            "      - 49\n",
            "      - 22\n",
            "      - 57\n",
            "      - 36\n",
            "      - 41\n",
            "      - 12\n",
            "      - 15\n",
            "      - 27\n",
            "      - 28\n",
            "      - 12\n",
            "      - 24\n",
            "      - 31\n",
            "      - 15\n",
            "      - 31\n",
            "      - 50\n",
            "      - 25\n",
            "      - 32\n",
            "      - 45\n",
            "      - 24\n",
            "      - 23\n",
            "      - 21\n",
            "      - 16\n",
            "      - 18\n",
            "      - 51\n",
            "      - 21\n",
            "      - 14\n",
            "      - 34\n",
            "      - 35\n",
            "      - 15\n",
            "      - 45\n",
            "      - 14\n",
            "      - 20\n",
            "      - 20\n",
            "      - 36\n",
            "      - 16\n",
            "      - 9\n",
            "      - 13\n",
            "      - 13\n",
            "      - 37\n",
            "      - 11\n",
            "      - 22\n",
            "      - 16\n",
            "      - 19\n",
            "      - 19\n",
            "      - 85\n",
            "      - 18\n",
            "      - 37\n",
            "      - 51\n",
            "      - 12\n",
            "      - 20\n",
            "      - 20\n",
            "      - 13\n",
            "      - 19\n",
            "      - 19\n",
            "      - 35\n",
            "      - 19\n",
            "      - 15\n",
            "      - 19\n",
            "      - 35\n",
            "      - 43\n",
            "      - 15\n",
            "      - 24\n",
            "      - 37\n",
            "      - 21\n",
            "      - 20\n",
            "      - 36\n",
            "      - 11\n",
            "      - 14\n",
            "      - 22\n",
            "      - 26\n",
            "      - 46\n",
            "      - 13\n",
            "      - 20\n",
            "      - 12\n",
            "      - 20\n",
            "      - 21\n",
            "      - 15\n",
            "      - 24\n",
            "      - 21\n",
            "      - 22\n",
            "      - 41\n",
            "      - 34\n",
            "      - 22\n",
            "      - 40\n",
            "      - 20\n",
            "      - 23\n",
            "      - 52\n",
            "      - 33\n",
            "      - 29\n",
            "      - 14\n",
            "      - 11\n",
            "      - 63\n",
            "      - 16\n",
            "      - 49\n",
            "      - 12\n",
            "      - 16\n",
            "      - 22\n",
            "      - 10\n",
            "      - 32\n",
            "      - 9\n",
            "      - 20\n",
            "      - 16\n",
            "      - 18\n",
            "      - 19\n",
            "      - 14\n",
            "      - 30\n",
            "      - 19\n",
            "      - 34\n",
            "      - 26\n",
            "      - 10\n",
            "      - 17\n",
            "      - 49\n",
            "      - 38\n",
            "      - 51\n",
            "      - 32\n",
            "      - 14\n",
            "      - 18\n",
            "      - 19\n",
            "      - 33\n",
            "      - 18\n",
            "      - 32\n",
            "      - 25\n",
            "      - 17\n",
            "      - 17\n",
            "      - 21\n",
            "      - 27\n",
            "      - 15\n",
            "      - 14\n",
            "      - 53\n",
            "      - 30\n",
            "      - 61\n",
            "      - 87\n",
            "      - 25\n",
            "      - 79\n",
            "      - 25\n",
            "      - 30\n",
            "      - 41\n",
            "      - 74\n",
            "      - 35\n",
            "      - 22\n",
            "      - 27\n",
            "      - 29\n",
            "      - 44\n",
            "      - 18\n",
            "      - 38\n",
            "      - 20\n",
            "      - 26\n",
            "      - 24\n",
            "      - 14\n",
            "      - 19\n",
            "      - 19\n",
            "      - 18\n",
            "      - 20\n",
            "      - 26\n",
            "      - 24\n",
            "      - 30\n",
            "      - 14\n",
            "      - 23\n",
            "      - 32\n",
            "      - 14\n",
            "      - 16\n",
            "      - 65\n",
            "      - 16\n",
            "      - 15\n",
            "      - 25\n",
            "      - 22\n",
            "      - 57\n",
            "      - 59\n",
            "      - 28\n",
            "      - 29\n",
            "      - 26\n",
            "      - 13\n",
            "      - 18\n",
            "      - 27\n",
            "      - 39\n",
            "      - 40\n",
            "      - 27\n",
            "      - 24\n",
            "      - 56\n",
            "      - 44\n",
            "      - 27\n",
            "      episode_reward:\n",
            "      - 30.0\n",
            "      - 18.0\n",
            "      - 42.0\n",
            "      - 21.0\n",
            "      - 12.0\n",
            "      - 18.0\n",
            "      - 14.0\n",
            "      - 17.0\n",
            "      - 12.0\n",
            "      - 41.0\n",
            "      - 26.0\n",
            "      - 15.0\n",
            "      - 26.0\n",
            "      - 16.0\n",
            "      - 14.0\n",
            "      - 15.0\n",
            "      - 13.0\n",
            "      - 24.0\n",
            "      - 18.0\n",
            "      - 32.0\n",
            "      - 17.0\n",
            "      - 14.0\n",
            "      - 57.0\n",
            "      - 17.0\n",
            "      - 23.0\n",
            "      - 10.0\n",
            "      - 15.0\n",
            "      - 18.0\n",
            "      - 18.0\n",
            "      - 12.0\n",
            "      - 13.0\n",
            "      - 11.0\n",
            "      - 16.0\n",
            "      - 14.0\n",
            "      - 15.0\n",
            "      - 11.0\n",
            "      - 43.0\n",
            "      - 27.0\n",
            "      - 40.0\n",
            "      - 21.0\n",
            "      - 18.0\n",
            "      - 17.0\n",
            "      - 10.0\n",
            "      - 19.0\n",
            "      - 47.0\n",
            "      - 16.0\n",
            "      - 25.0\n",
            "      - 22.0\n",
            "      - 30.0\n",
            "      - 19.0\n",
            "      - 24.0\n",
            "      - 23.0\n",
            "      - 40.0\n",
            "      - 30.0\n",
            "      - 12.0\n",
            "      - 21.0\n",
            "      - 27.0\n",
            "      - 21.0\n",
            "      - 46.0\n",
            "      - 62.0\n",
            "      - 21.0\n",
            "      - 33.0\n",
            "      - 14.0\n",
            "      - 14.0\n",
            "      - 12.0\n",
            "      - 16.0\n",
            "      - 79.0\n",
            "      - 21.0\n",
            "      - 14.0\n",
            "      - 19.0\n",
            "      - 12.0\n",
            "      - 12.0\n",
            "      - 15.0\n",
            "      - 11.0\n",
            "      - 26.0\n",
            "      - 14.0\n",
            "      - 21.0\n",
            "      - 11.0\n",
            "      - 20.0\n",
            "      - 44.0\n",
            "      - 12.0\n",
            "      - 39.0\n",
            "      - 31.0\n",
            "      - 27.0\n",
            "      - 21.0\n",
            "      - 33.0\n",
            "      - 35.0\n",
            "      - 16.0\n",
            "      - 13.0\n",
            "      - 10.0\n",
            "      - 52.0\n",
            "      - 19.0\n",
            "      - 38.0\n",
            "      - 12.0\n",
            "      - 80.0\n",
            "      - 54.0\n",
            "      - 24.0\n",
            "      - 18.0\n",
            "      - 52.0\n",
            "      - 15.0\n",
            "      - 23.0\n",
            "      - 58.0\n",
            "      - 15.0\n",
            "      - 13.0\n",
            "      - 26.0\n",
            "      - 14.0\n",
            "      - 14.0\n",
            "      - 56.0\n",
            "      - 18.0\n",
            "      - 34.0\n",
            "      - 20.0\n",
            "      - 11.0\n",
            "      - 20.0\n",
            "      - 25.0\n",
            "      - 34.0\n",
            "      - 24.0\n",
            "      - 11.0\n",
            "      - 24.0\n",
            "      - 15.0\n",
            "      - 45.0\n",
            "      - 55.0\n",
            "      - 39.0\n",
            "      - 20.0\n",
            "      - 49.0\n",
            "      - 22.0\n",
            "      - 57.0\n",
            "      - 36.0\n",
            "      - 41.0\n",
            "      - 12.0\n",
            "      - 15.0\n",
            "      - 27.0\n",
            "      - 28.0\n",
            "      - 12.0\n",
            "      - 24.0\n",
            "      - 31.0\n",
            "      - 15.0\n",
            "      - 31.0\n",
            "      - 50.0\n",
            "      - 25.0\n",
            "      - 32.0\n",
            "      - 45.0\n",
            "      - 24.0\n",
            "      - 23.0\n",
            "      - 21.0\n",
            "      - 16.0\n",
            "      - 18.0\n",
            "      - 51.0\n",
            "      - 21.0\n",
            "      - 14.0\n",
            "      - 34.0\n",
            "      - 35.0\n",
            "      - 15.0\n",
            "      - 45.0\n",
            "      - 14.0\n",
            "      - 20.0\n",
            "      - 20.0\n",
            "      - 36.0\n",
            "      - 16.0\n",
            "      - 9.0\n",
            "      - 13.0\n",
            "      - 13.0\n",
            "      - 37.0\n",
            "      - 11.0\n",
            "      - 22.0\n",
            "      - 16.0\n",
            "      - 19.0\n",
            "      - 19.0\n",
            "      - 85.0\n",
            "      - 18.0\n",
            "      - 37.0\n",
            "      - 51.0\n",
            "      - 12.0\n",
            "      - 20.0\n",
            "      - 20.0\n",
            "      - 13.0\n",
            "      - 19.0\n",
            "      - 19.0\n",
            "      - 35.0\n",
            "      - 19.0\n",
            "      - 15.0\n",
            "      - 19.0\n",
            "      - 35.0\n",
            "      - 43.0\n",
            "      - 15.0\n",
            "      - 24.0\n",
            "      - 37.0\n",
            "      - 21.0\n",
            "      - 20.0\n",
            "      - 36.0\n",
            "      - 11.0\n",
            "      - 14.0\n",
            "      - 22.0\n",
            "      - 26.0\n",
            "      - 46.0\n",
            "      - 13.0\n",
            "      - 20.0\n",
            "      - 12.0\n",
            "      - 20.0\n",
            "      - 21.0\n",
            "      - 15.0\n",
            "      - 24.0\n",
            "      - 21.0\n",
            "      - 22.0\n",
            "      - 41.0\n",
            "      - 34.0\n",
            "      - 22.0\n",
            "      - 40.0\n",
            "      - 20.0\n",
            "      - 23.0\n",
            "      - 52.0\n",
            "      - 33.0\n",
            "      - 29.0\n",
            "      - 14.0\n",
            "      - 11.0\n",
            "      - 63.0\n",
            "      - 16.0\n",
            "      - 49.0\n",
            "      - 12.0\n",
            "      - 16.0\n",
            "      - 22.0\n",
            "      - 10.0\n",
            "      - 32.0\n",
            "      - 9.0\n",
            "      - 20.0\n",
            "      - 16.0\n",
            "      - 18.0\n",
            "      - 19.0\n",
            "      - 14.0\n",
            "      - 30.0\n",
            "      - 19.0\n",
            "      - 34.0\n",
            "      - 26.0\n",
            "      - 10.0\n",
            "      - 17.0\n",
            "      - 49.0\n",
            "      - 38.0\n",
            "      - 51.0\n",
            "      - 32.0\n",
            "      - 14.0\n",
            "      - 18.0\n",
            "      - 19.0\n",
            "      - 33.0\n",
            "      - 18.0\n",
            "      - 32.0\n",
            "      - 25.0\n",
            "      - 17.0\n",
            "      - 17.0\n",
            "      - 21.0\n",
            "      - 27.0\n",
            "      - 15.0\n",
            "      - 14.0\n",
            "      - 53.0\n",
            "      - 30.0\n",
            "      - 61.0\n",
            "      - 87.0\n",
            "      - 25.0\n",
            "      - 79.0\n",
            "      - 25.0\n",
            "      - 30.0\n",
            "      - 41.0\n",
            "      - 74.0\n",
            "      - 35.0\n",
            "      - 22.0\n",
            "      - 27.0\n",
            "      - 29.0\n",
            "      - 44.0\n",
            "      - 18.0\n",
            "      - 38.0\n",
            "      - 20.0\n",
            "      - 26.0\n",
            "      - 24.0\n",
            "      - 14.0\n",
            "      - 19.0\n",
            "      - 19.0\n",
            "      - 18.0\n",
            "      - 20.0\n",
            "      - 26.0\n",
            "      - 24.0\n",
            "      - 30.0\n",
            "      - 14.0\n",
            "      - 23.0\n",
            "      - 32.0\n",
            "      - 14.0\n",
            "      - 16.0\n",
            "      - 65.0\n",
            "      - 16.0\n",
            "      - 15.0\n",
            "      - 25.0\n",
            "      - 22.0\n",
            "      - 57.0\n",
            "      - 59.0\n",
            "      - 28.0\n",
            "      - 29.0\n",
            "      - 26.0\n",
            "      - 13.0\n",
            "      - 18.0\n",
            "      - 27.0\n",
            "      - 39.0\n",
            "      - 40.0\n",
            "      - 27.0\n",
            "      - 24.0\n",
            "      - 56.0\n",
            "      - 44.0\n",
            "      - 27.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1454028313571959\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11671676528668817\n",
            "      mean_inference_ms: 1.4422462169670451\n",
            "      mean_raw_obs_processing_ms: 0.3189991873760457\n",
            "  time_since_restore: 10.06491732597351\n",
            "  time_this_iter_s: 10.06491732597351\n",
            "  time_total_s: 10.06491732597351\n",
            "  timers:\n",
            "    learn_throughput: 29520.617\n",
            "    learn_time_ms: 6.775\n",
            "    load_throughput: 1042581.158\n",
            "    load_time_ms: 0.192\n",
            "    training_iteration_time_ms: 241.476\n",
            "    update_time_ms: 4.26\n",
            "  timestamp: 1656953879\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 8000\n",
            "  training_iteration: 1\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:04 (running for 00:00:34.28)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         10.0649</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 26.2105</td><td style=\"text-align: right;\">                  87</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           26.2105</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 9800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 9800\n",
            "    num_agent_steps_trained: 9800\n",
            "    num_env_steps_sampled: 9800\n",
            "    num_env_steps_trained: 9800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-58-09\n",
            "  done: false\n",
            "  episode_len_mean: 29.08\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 69.0\n",
            "  episode_reward_mean: 29.08\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 60\n",
            "  episodes_total: 364\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 34.55\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 68.0\n",
            "    episode_reward_mean: 34.55\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 28\n",
            "      - 24\n",
            "      - 25\n",
            "      - 16\n",
            "      - 43\n",
            "      - 49\n",
            "      - 18\n",
            "      - 41\n",
            "      - 49\n",
            "      - 13\n",
            "      - 20\n",
            "      - 57\n",
            "      - 62\n",
            "      - 24\n",
            "      - 17\n",
            "      - 34\n",
            "      - 68\n",
            "      - 47\n",
            "      - 21\n",
            "      - 35\n",
            "      episode_reward:\n",
            "      - 28.0\n",
            "      - 24.0\n",
            "      - 25.0\n",
            "      - 16.0\n",
            "      - 43.0\n",
            "      - 49.0\n",
            "      - 18.0\n",
            "      - 41.0\n",
            "      - 49.0\n",
            "      - 13.0\n",
            "      - 20.0\n",
            "      - 57.0\n",
            "      - 62.0\n",
            "      - 24.0\n",
            "      - 17.0\n",
            "      - 34.0\n",
            "      - 68.0\n",
            "      - 47.0\n",
            "      - 21.0\n",
            "      - 35.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1035903336793849\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07849762454236012\n",
            "      mean_inference_ms: 1.0236014459002576\n",
            "      mean_raw_obs_processing_ms: 0.12147575717588144\n",
            "    timesteps_this_iter: 691\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 134.57012939453125\n",
            "          policy_loss: 1118.689453125\n",
            "          var_gnorm: 22.654884338378906\n",
            "          vf_explained_var: -0.009659767150878906\n",
            "          vf_loss: 9555.7578125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 9800\n",
            "    num_agent_steps_trained: 9800\n",
            "    num_env_steps_sampled: 9800\n",
            "    num_env_steps_trained: 9800\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 9800\n",
            "  num_agent_steps_trained: 9800\n",
            "  num_env_steps_sampled: 9800\n",
            "  num_env_steps_sampled_this_iter: 1800\n",
            "  num_env_steps_trained: 9800\n",
            "  num_env_steps_trained_this_iter: 1800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 70.64285714285714\n",
            "    ram_util_percent: 21.750000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14486327404990684\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11595033714113763\n",
            "    mean_inference_ms: 1.4241500788295605\n",
            "    mean_raw_obs_processing_ms: 0.32611832731084744\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 29.08\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 69.0\n",
            "    episode_reward_mean: 29.08\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 60\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 29\n",
            "      - 44\n",
            "      - 18\n",
            "      - 38\n",
            "      - 20\n",
            "      - 26\n",
            "      - 24\n",
            "      - 14\n",
            "      - 19\n",
            "      - 19\n",
            "      - 18\n",
            "      - 20\n",
            "      - 26\n",
            "      - 24\n",
            "      - 30\n",
            "      - 14\n",
            "      - 23\n",
            "      - 32\n",
            "      - 14\n",
            "      - 16\n",
            "      - 65\n",
            "      - 16\n",
            "      - 15\n",
            "      - 25\n",
            "      - 22\n",
            "      - 57\n",
            "      - 59\n",
            "      - 28\n",
            "      - 29\n",
            "      - 26\n",
            "      - 13\n",
            "      - 18\n",
            "      - 27\n",
            "      - 39\n",
            "      - 40\n",
            "      - 27\n",
            "      - 24\n",
            "      - 56\n",
            "      - 44\n",
            "      - 27\n",
            "      - 44\n",
            "      - 11\n",
            "      - 41\n",
            "      - 38\n",
            "      - 21\n",
            "      - 25\n",
            "      - 69\n",
            "      - 25\n",
            "      - 23\n",
            "      - 27\n",
            "      - 57\n",
            "      - 26\n",
            "      - 34\n",
            "      - 17\n",
            "      - 12\n",
            "      - 40\n",
            "      - 9\n",
            "      - 53\n",
            "      - 56\n",
            "      - 14\n",
            "      - 17\n",
            "      - 31\n",
            "      - 14\n",
            "      - 23\n",
            "      - 54\n",
            "      - 26\n",
            "      - 34\n",
            "      - 47\n",
            "      - 40\n",
            "      - 21\n",
            "      - 14\n",
            "      - 43\n",
            "      - 24\n",
            "      - 46\n",
            "      - 18\n",
            "      - 12\n",
            "      - 31\n",
            "      - 29\n",
            "      - 27\n",
            "      - 24\n",
            "      - 21\n",
            "      - 34\n",
            "      - 51\n",
            "      - 41\n",
            "      - 23\n",
            "      - 33\n",
            "      - 31\n",
            "      - 26\n",
            "      - 14\n",
            "      - 23\n",
            "      - 29\n",
            "      - 56\n",
            "      - 11\n",
            "      - 9\n",
            "      - 43\n",
            "      - 24\n",
            "      - 22\n",
            "      - 17\n",
            "      - 35\n",
            "      - 23\n",
            "      episode_reward:\n",
            "      - 29.0\n",
            "      - 44.0\n",
            "      - 18.0\n",
            "      - 38.0\n",
            "      - 20.0\n",
            "      - 26.0\n",
            "      - 24.0\n",
            "      - 14.0\n",
            "      - 19.0\n",
            "      - 19.0\n",
            "      - 18.0\n",
            "      - 20.0\n",
            "      - 26.0\n",
            "      - 24.0\n",
            "      - 30.0\n",
            "      - 14.0\n",
            "      - 23.0\n",
            "      - 32.0\n",
            "      - 14.0\n",
            "      - 16.0\n",
            "      - 65.0\n",
            "      - 16.0\n",
            "      - 15.0\n",
            "      - 25.0\n",
            "      - 22.0\n",
            "      - 57.0\n",
            "      - 59.0\n",
            "      - 28.0\n",
            "      - 29.0\n",
            "      - 26.0\n",
            "      - 13.0\n",
            "      - 18.0\n",
            "      - 27.0\n",
            "      - 39.0\n",
            "      - 40.0\n",
            "      - 27.0\n",
            "      - 24.0\n",
            "      - 56.0\n",
            "      - 44.0\n",
            "      - 27.0\n",
            "      - 44.0\n",
            "      - 11.0\n",
            "      - 41.0\n",
            "      - 38.0\n",
            "      - 21.0\n",
            "      - 25.0\n",
            "      - 69.0\n",
            "      - 25.0\n",
            "      - 23.0\n",
            "      - 27.0\n",
            "      - 57.0\n",
            "      - 26.0\n",
            "      - 34.0\n",
            "      - 17.0\n",
            "      - 12.0\n",
            "      - 40.0\n",
            "      - 9.0\n",
            "      - 53.0\n",
            "      - 56.0\n",
            "      - 14.0\n",
            "      - 17.0\n",
            "      - 31.0\n",
            "      - 14.0\n",
            "      - 23.0\n",
            "      - 54.0\n",
            "      - 26.0\n",
            "      - 34.0\n",
            "      - 47.0\n",
            "      - 40.0\n",
            "      - 21.0\n",
            "      - 14.0\n",
            "      - 43.0\n",
            "      - 24.0\n",
            "      - 46.0\n",
            "      - 18.0\n",
            "      - 12.0\n",
            "      - 31.0\n",
            "      - 29.0\n",
            "      - 27.0\n",
            "      - 24.0\n",
            "      - 21.0\n",
            "      - 34.0\n",
            "      - 51.0\n",
            "      - 41.0\n",
            "      - 23.0\n",
            "      - 33.0\n",
            "      - 31.0\n",
            "      - 26.0\n",
            "      - 14.0\n",
            "      - 23.0\n",
            "      - 29.0\n",
            "      - 56.0\n",
            "      - 11.0\n",
            "      - 9.0\n",
            "      - 43.0\n",
            "      - 24.0\n",
            "      - 22.0\n",
            "      - 17.0\n",
            "      - 35.0\n",
            "      - 23.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14486327404990684\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11595033714113763\n",
            "      mean_inference_ms: 1.4241500788295605\n",
            "      mean_raw_obs_processing_ms: 0.32611832731084744\n",
            "  time_since_restore: 20.078953742980957\n",
            "  time_this_iter_s: 10.014036417007446\n",
            "  time_total_s: 20.078953742980957\n",
            "  timers:\n",
            "    learn_throughput: 30591.465\n",
            "    learn_time_ms: 6.538\n",
            "    load_throughput: 991444.037\n",
            "    load_time_ms: 0.202\n",
            "    training_iteration_time_ms: 225.364\n",
            "    update_time_ms: 3.946\n",
            "  timestamp: 1656953889\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 9800\n",
            "  training_iteration: 2\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:14 (running for 00:00:44.27)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          20.079</td><td style=\"text-align: right;\">9800</td><td style=\"text-align: right;\">   29.08</td><td style=\"text-align: right;\">                  69</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             29.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:19 (running for 00:00:49.37)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          20.079</td><td style=\"text-align: right;\">9800</td><td style=\"text-align: right;\">   29.08</td><td style=\"text-align: right;\">                  69</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">             29.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 19000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 19000\n",
            "    num_agent_steps_trained: 19000\n",
            "    num_env_steps_sampled: 19000\n",
            "    num_env_steps_trained: 19000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-58-19\n",
            "  done: false\n",
            "  episode_len_mean: 47.31443298969072\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 47.31443298969072\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 194\n",
            "  episodes_total: 558\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 120.0523681640625\n",
            "          policy_loss: 1036.0479736328125\n",
            "          var_gnorm: 22.71255874633789\n",
            "          vf_explained_var: 0.004441201686859131\n",
            "          vf_loss: 8769.5673828125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 19000\n",
            "    num_agent_steps_trained: 19000\n",
            "    num_env_steps_sampled: 19000\n",
            "    num_env_steps_trained: 19000\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 19000\n",
            "  num_agent_steps_trained: 19000\n",
            "  num_env_steps_sampled: 19000\n",
            "  num_env_steps_sampled_this_iter: 9200\n",
            "  num_env_steps_trained: 19000\n",
            "  num_env_steps_trained_this_iter: 9200\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.83333333333334\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1432230783169548\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11377799432271733\n",
            "    mean_inference_ms: 1.374330505516538\n",
            "    mean_raw_obs_processing_ms: 0.28622027227461005\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 47.31443298969072\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 47.31443298969072\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 194\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 58\n",
            "      - 21\n",
            "      - 53\n",
            "      - 70\n",
            "      - 28\n",
            "      - 62\n",
            "      - 37\n",
            "      - 15\n",
            "      - 28\n",
            "      - 26\n",
            "      - 23\n",
            "      - 73\n",
            "      - 11\n",
            "      - 37\n",
            "      - 75\n",
            "      - 35\n",
            "      - 22\n",
            "      - 68\n",
            "      - 18\n",
            "      - 42\n",
            "      - 54\n",
            "      - 73\n",
            "      - 40\n",
            "      - 89\n",
            "      - 27\n",
            "      - 66\n",
            "      - 54\n",
            "      - 106\n",
            "      - 51\n",
            "      - 20\n",
            "      - 29\n",
            "      - 69\n",
            "      - 62\n",
            "      - 33\n",
            "      - 60\n",
            "      - 49\n",
            "      - 31\n",
            "      - 51\n",
            "      - 43\n",
            "      - 66\n",
            "      - 26\n",
            "      - 46\n",
            "      - 38\n",
            "      - 20\n",
            "      - 41\n",
            "      - 61\n",
            "      - 103\n",
            "      - 135\n",
            "      - 36\n",
            "      - 17\n",
            "      - 94\n",
            "      - 47\n",
            "      - 45\n",
            "      - 21\n",
            "      - 25\n",
            "      - 25\n",
            "      - 73\n",
            "      - 39\n",
            "      - 26\n",
            "      - 20\n",
            "      - 76\n",
            "      - 32\n",
            "      - 50\n",
            "      - 41\n",
            "      - 26\n",
            "      - 45\n",
            "      - 46\n",
            "      - 32\n",
            "      - 61\n",
            "      - 63\n",
            "      - 59\n",
            "      - 41\n",
            "      - 90\n",
            "      - 46\n",
            "      - 51\n",
            "      - 17\n",
            "      - 102\n",
            "      - 72\n",
            "      - 28\n",
            "      - 20\n",
            "      - 71\n",
            "      - 60\n",
            "      - 45\n",
            "      - 39\n",
            "      - 44\n",
            "      - 32\n",
            "      - 47\n",
            "      - 34\n",
            "      - 33\n",
            "      - 32\n",
            "      - 33\n",
            "      - 80\n",
            "      - 26\n",
            "      - 48\n",
            "      - 78\n",
            "      - 23\n",
            "      - 21\n",
            "      - 35\n",
            "      - 29\n",
            "      - 22\n",
            "      - 14\n",
            "      - 36\n",
            "      - 16\n",
            "      - 19\n",
            "      - 40\n",
            "      - 19\n",
            "      - 35\n",
            "      - 16\n",
            "      - 28\n",
            "      - 20\n",
            "      - 66\n",
            "      - 49\n",
            "      - 36\n",
            "      - 64\n",
            "      - 64\n",
            "      - 52\n",
            "      - 16\n",
            "      - 50\n",
            "      - 43\n",
            "      - 44\n",
            "      - 50\n",
            "      - 67\n",
            "      - 28\n",
            "      - 17\n",
            "      - 59\n",
            "      - 26\n",
            "      - 26\n",
            "      - 26\n",
            "      - 37\n",
            "      - 72\n",
            "      - 21\n",
            "      - 23\n",
            "      - 16\n",
            "      - 48\n",
            "      - 24\n",
            "      - 95\n",
            "      - 61\n",
            "      - 50\n",
            "      - 59\n",
            "      - 14\n",
            "      - 44\n",
            "      - 40\n",
            "      - 44\n",
            "      - 31\n",
            "      - 41\n",
            "      - 37\n",
            "      - 36\n",
            "      - 31\n",
            "      - 49\n",
            "      - 23\n",
            "      - 79\n",
            "      - 45\n",
            "      - 22\n",
            "      - 46\n",
            "      - 65\n",
            "      - 16\n",
            "      - 76\n",
            "      - 35\n",
            "      - 45\n",
            "      - 123\n",
            "      - 38\n",
            "      - 19\n",
            "      - 60\n",
            "      - 61\n",
            "      - 56\n",
            "      - 200\n",
            "      - 48\n",
            "      - 52\n",
            "      - 35\n",
            "      - 91\n",
            "      - 60\n",
            "      - 34\n",
            "      - 28\n",
            "      - 29\n",
            "      - 143\n",
            "      - 43\n",
            "      - 46\n",
            "      - 18\n",
            "      - 62\n",
            "      - 42\n",
            "      - 34\n",
            "      - 47\n",
            "      - 82\n",
            "      - 120\n",
            "      - 77\n",
            "      - 60\n",
            "      - 63\n",
            "      - 61\n",
            "      - 115\n",
            "      - 73\n",
            "      - 52\n",
            "      - 17\n",
            "      - 60\n",
            "      - 36\n",
            "      episode_reward:\n",
            "      - 58.0\n",
            "      - 21.0\n",
            "      - 53.0\n",
            "      - 70.0\n",
            "      - 28.0\n",
            "      - 62.0\n",
            "      - 37.0\n",
            "      - 15.0\n",
            "      - 28.0\n",
            "      - 26.0\n",
            "      - 23.0\n",
            "      - 73.0\n",
            "      - 11.0\n",
            "      - 37.0\n",
            "      - 75.0\n",
            "      - 35.0\n",
            "      - 22.0\n",
            "      - 68.0\n",
            "      - 18.0\n",
            "      - 42.0\n",
            "      - 54.0\n",
            "      - 73.0\n",
            "      - 40.0\n",
            "      - 89.0\n",
            "      - 27.0\n",
            "      - 66.0\n",
            "      - 54.0\n",
            "      - 106.0\n",
            "      - 51.0\n",
            "      - 20.0\n",
            "      - 29.0\n",
            "      - 69.0\n",
            "      - 62.0\n",
            "      - 33.0\n",
            "      - 60.0\n",
            "      - 49.0\n",
            "      - 31.0\n",
            "      - 51.0\n",
            "      - 43.0\n",
            "      - 66.0\n",
            "      - 26.0\n",
            "      - 46.0\n",
            "      - 38.0\n",
            "      - 20.0\n",
            "      - 41.0\n",
            "      - 61.0\n",
            "      - 103.0\n",
            "      - 135.0\n",
            "      - 36.0\n",
            "      - 17.0\n",
            "      - 94.0\n",
            "      - 47.0\n",
            "      - 45.0\n",
            "      - 21.0\n",
            "      - 25.0\n",
            "      - 25.0\n",
            "      - 73.0\n",
            "      - 39.0\n",
            "      - 26.0\n",
            "      - 20.0\n",
            "      - 76.0\n",
            "      - 32.0\n",
            "      - 50.0\n",
            "      - 41.0\n",
            "      - 26.0\n",
            "      - 45.0\n",
            "      - 46.0\n",
            "      - 32.0\n",
            "      - 61.0\n",
            "      - 63.0\n",
            "      - 59.0\n",
            "      - 41.0\n",
            "      - 90.0\n",
            "      - 46.0\n",
            "      - 51.0\n",
            "      - 17.0\n",
            "      - 102.0\n",
            "      - 72.0\n",
            "      - 28.0\n",
            "      - 20.0\n",
            "      - 71.0\n",
            "      - 60.0\n",
            "      - 45.0\n",
            "      - 39.0\n",
            "      - 44.0\n",
            "      - 32.0\n",
            "      - 47.0\n",
            "      - 34.0\n",
            "      - 33.0\n",
            "      - 32.0\n",
            "      - 33.0\n",
            "      - 80.0\n",
            "      - 26.0\n",
            "      - 48.0\n",
            "      - 78.0\n",
            "      - 23.0\n",
            "      - 21.0\n",
            "      - 35.0\n",
            "      - 29.0\n",
            "      - 22.0\n",
            "      - 14.0\n",
            "      - 36.0\n",
            "      - 16.0\n",
            "      - 19.0\n",
            "      - 40.0\n",
            "      - 19.0\n",
            "      - 35.0\n",
            "      - 16.0\n",
            "      - 28.0\n",
            "      - 20.0\n",
            "      - 66.0\n",
            "      - 49.0\n",
            "      - 36.0\n",
            "      - 64.0\n",
            "      - 64.0\n",
            "      - 52.0\n",
            "      - 16.0\n",
            "      - 50.0\n",
            "      - 43.0\n",
            "      - 44.0\n",
            "      - 50.0\n",
            "      - 67.0\n",
            "      - 28.0\n",
            "      - 17.0\n",
            "      - 59.0\n",
            "      - 26.0\n",
            "      - 26.0\n",
            "      - 26.0\n",
            "      - 37.0\n",
            "      - 72.0\n",
            "      - 21.0\n",
            "      - 23.0\n",
            "      - 16.0\n",
            "      - 48.0\n",
            "      - 24.0\n",
            "      - 95.0\n",
            "      - 61.0\n",
            "      - 50.0\n",
            "      - 59.0\n",
            "      - 14.0\n",
            "      - 44.0\n",
            "      - 40.0\n",
            "      - 44.0\n",
            "      - 31.0\n",
            "      - 41.0\n",
            "      - 37.0\n",
            "      - 36.0\n",
            "      - 31.0\n",
            "      - 49.0\n",
            "      - 23.0\n",
            "      - 79.0\n",
            "      - 45.0\n",
            "      - 22.0\n",
            "      - 46.0\n",
            "      - 65.0\n",
            "      - 16.0\n",
            "      - 76.0\n",
            "      - 35.0\n",
            "      - 45.0\n",
            "      - 123.0\n",
            "      - 38.0\n",
            "      - 19.0\n",
            "      - 60.0\n",
            "      - 61.0\n",
            "      - 56.0\n",
            "      - 200.0\n",
            "      - 48.0\n",
            "      - 52.0\n",
            "      - 35.0\n",
            "      - 91.0\n",
            "      - 60.0\n",
            "      - 34.0\n",
            "      - 28.0\n",
            "      - 29.0\n",
            "      - 143.0\n",
            "      - 43.0\n",
            "      - 46.0\n",
            "      - 18.0\n",
            "      - 62.0\n",
            "      - 42.0\n",
            "      - 34.0\n",
            "      - 47.0\n",
            "      - 82.0\n",
            "      - 120.0\n",
            "      - 77.0\n",
            "      - 60.0\n",
            "      - 63.0\n",
            "      - 61.0\n",
            "      - 115.0\n",
            "      - 73.0\n",
            "      - 52.0\n",
            "      - 17.0\n",
            "      - 60.0\n",
            "      - 36.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1432230783169548\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11377799432271733\n",
            "      mean_inference_ms: 1.374330505516538\n",
            "      mean_raw_obs_processing_ms: 0.28622027227461005\n",
            "  time_since_restore: 30.175222635269165\n",
            "  time_this_iter_s: 10.096268892288208\n",
            "  time_total_s: 30.175222635269165\n",
            "  timers:\n",
            "    learn_throughput: 29857.692\n",
            "    learn_time_ms: 6.698\n",
            "    load_throughput: 857292.591\n",
            "    load_time_ms: 0.233\n",
            "    training_iteration_time_ms: 219.82\n",
            "    update_time_ms: 4.031\n",
            "  timestamp: 1656953899\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 19000\n",
            "  training_iteration: 3\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:24 (running for 00:00:54.42)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         30.1752</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\"> 47.3144</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">           47.3144</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:30 (running for 00:00:59.50)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         30.1752</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\"> 47.3144</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">           47.3144</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 20400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 20400\n",
            "    num_agent_steps_trained: 20400\n",
            "    num_env_steps_sampled: 20400\n",
            "    num_env_steps_trained: 20400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-58-31\n",
            "  done: false\n",
            "  episode_len_mean: 53.39\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 53.39\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 580\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 61.05\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 168.0\n",
            "    episode_reward_mean: 61.05\n",
            "    episode_reward_min: 23.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 79\n",
            "      - 68\n",
            "      - 23\n",
            "      - 38\n",
            "      - 47\n",
            "      - 42\n",
            "      - 38\n",
            "      - 54\n",
            "      - 59\n",
            "      - 88\n",
            "      - 56\n",
            "      - 43\n",
            "      - 60\n",
            "      - 31\n",
            "      - 134\n",
            "      - 46\n",
            "      - 168\n",
            "      - 46\n",
            "      - 47\n",
            "      - 54\n",
            "      episode_reward:\n",
            "      - 79.0\n",
            "      - 68.0\n",
            "      - 23.0\n",
            "      - 38.0\n",
            "      - 47.0\n",
            "      - 42.0\n",
            "      - 38.0\n",
            "      - 54.0\n",
            "      - 59.0\n",
            "      - 88.0\n",
            "      - 56.0\n",
            "      - 43.0\n",
            "      - 60.0\n",
            "      - 31.0\n",
            "      - 134.0\n",
            "      - 46.0\n",
            "      - 168.0\n",
            "      - 46.0\n",
            "      - 47.0\n",
            "      - 54.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10217878474347522\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0781893284354713\n",
            "      mean_inference_ms: 1.0103594659421549\n",
            "      mean_raw_obs_processing_ms: 0.11639516569378665\n",
            "    timesteps_this_iter: 1221\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 126.7438735961914\n",
            "          policy_loss: 1114.10693359375\n",
            "          var_gnorm: 22.71419334411621\n",
            "          vf_explained_var: 0.0028585195541381836\n",
            "          vf_loss: 11360.482421875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 20400\n",
            "    num_agent_steps_trained: 20400\n",
            "    num_env_steps_sampled: 20400\n",
            "    num_env_steps_trained: 20400\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 20400\n",
            "  num_agent_steps_trained: 20400\n",
            "  num_env_steps_sampled: 20400\n",
            "  num_env_steps_sampled_this_iter: 1400\n",
            "  num_env_steps_trained: 20400\n",
            "  num_env_steps_trained_this_iter: 1400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 70.0875\n",
            "    ram_util_percent: 21.41875\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14287756963393775\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11332858361349916\n",
            "    mean_inference_ms: 1.3700337413146046\n",
            "    mean_raw_obs_processing_ms: 0.2952132281440452\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 53.39\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 53.39\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 16\n",
            "      - 50\n",
            "      - 43\n",
            "      - 44\n",
            "      - 50\n",
            "      - 67\n",
            "      - 28\n",
            "      - 17\n",
            "      - 59\n",
            "      - 26\n",
            "      - 26\n",
            "      - 26\n",
            "      - 37\n",
            "      - 72\n",
            "      - 21\n",
            "      - 23\n",
            "      - 16\n",
            "      - 48\n",
            "      - 24\n",
            "      - 95\n",
            "      - 61\n",
            "      - 50\n",
            "      - 59\n",
            "      - 14\n",
            "      - 44\n",
            "      - 40\n",
            "      - 44\n",
            "      - 31\n",
            "      - 41\n",
            "      - 37\n",
            "      - 36\n",
            "      - 31\n",
            "      - 49\n",
            "      - 23\n",
            "      - 79\n",
            "      - 45\n",
            "      - 22\n",
            "      - 46\n",
            "      - 65\n",
            "      - 16\n",
            "      - 76\n",
            "      - 35\n",
            "      - 45\n",
            "      - 123\n",
            "      - 38\n",
            "      - 19\n",
            "      - 60\n",
            "      - 61\n",
            "      - 56\n",
            "      - 200\n",
            "      - 48\n",
            "      - 52\n",
            "      - 35\n",
            "      - 91\n",
            "      - 60\n",
            "      - 34\n",
            "      - 28\n",
            "      - 29\n",
            "      - 143\n",
            "      - 43\n",
            "      - 46\n",
            "      - 18\n",
            "      - 62\n",
            "      - 42\n",
            "      - 34\n",
            "      - 47\n",
            "      - 82\n",
            "      - 120\n",
            "      - 77\n",
            "      - 60\n",
            "      - 63\n",
            "      - 61\n",
            "      - 115\n",
            "      - 73\n",
            "      - 52\n",
            "      - 17\n",
            "      - 60\n",
            "      - 36\n",
            "      - 102\n",
            "      - 61\n",
            "      - 61\n",
            "      - 53\n",
            "      - 141\n",
            "      - 35\n",
            "      - 77\n",
            "      - 50\n",
            "      - 86\n",
            "      - 64\n",
            "      - 41\n",
            "      - 55\n",
            "      - 71\n",
            "      - 17\n",
            "      - 59\n",
            "      - 61\n",
            "      - 36\n",
            "      - 55\n",
            "      - 38\n",
            "      - 81\n",
            "      - 54\n",
            "      - 79\n",
            "      episode_reward:\n",
            "      - 16.0\n",
            "      - 50.0\n",
            "      - 43.0\n",
            "      - 44.0\n",
            "      - 50.0\n",
            "      - 67.0\n",
            "      - 28.0\n",
            "      - 17.0\n",
            "      - 59.0\n",
            "      - 26.0\n",
            "      - 26.0\n",
            "      - 26.0\n",
            "      - 37.0\n",
            "      - 72.0\n",
            "      - 21.0\n",
            "      - 23.0\n",
            "      - 16.0\n",
            "      - 48.0\n",
            "      - 24.0\n",
            "      - 95.0\n",
            "      - 61.0\n",
            "      - 50.0\n",
            "      - 59.0\n",
            "      - 14.0\n",
            "      - 44.0\n",
            "      - 40.0\n",
            "      - 44.0\n",
            "      - 31.0\n",
            "      - 41.0\n",
            "      - 37.0\n",
            "      - 36.0\n",
            "      - 31.0\n",
            "      - 49.0\n",
            "      - 23.0\n",
            "      - 79.0\n",
            "      - 45.0\n",
            "      - 22.0\n",
            "      - 46.0\n",
            "      - 65.0\n",
            "      - 16.0\n",
            "      - 76.0\n",
            "      - 35.0\n",
            "      - 45.0\n",
            "      - 123.0\n",
            "      - 38.0\n",
            "      - 19.0\n",
            "      - 60.0\n",
            "      - 61.0\n",
            "      - 56.0\n",
            "      - 200.0\n",
            "      - 48.0\n",
            "      - 52.0\n",
            "      - 35.0\n",
            "      - 91.0\n",
            "      - 60.0\n",
            "      - 34.0\n",
            "      - 28.0\n",
            "      - 29.0\n",
            "      - 143.0\n",
            "      - 43.0\n",
            "      - 46.0\n",
            "      - 18.0\n",
            "      - 62.0\n",
            "      - 42.0\n",
            "      - 34.0\n",
            "      - 47.0\n",
            "      - 82.0\n",
            "      - 120.0\n",
            "      - 77.0\n",
            "      - 60.0\n",
            "      - 63.0\n",
            "      - 61.0\n",
            "      - 115.0\n",
            "      - 73.0\n",
            "      - 52.0\n",
            "      - 17.0\n",
            "      - 60.0\n",
            "      - 36.0\n",
            "      - 102.0\n",
            "      - 61.0\n",
            "      - 61.0\n",
            "      - 53.0\n",
            "      - 141.0\n",
            "      - 35.0\n",
            "      - 77.0\n",
            "      - 50.0\n",
            "      - 86.0\n",
            "      - 64.0\n",
            "      - 41.0\n",
            "      - 55.0\n",
            "      - 71.0\n",
            "      - 17.0\n",
            "      - 59.0\n",
            "      - 61.0\n",
            "      - 36.0\n",
            "      - 55.0\n",
            "      - 38.0\n",
            "      - 81.0\n",
            "      - 54.0\n",
            "      - 79.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14287756963393775\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11332858361349916\n",
            "      mean_inference_ms: 1.3700337413146046\n",
            "      mean_raw_obs_processing_ms: 0.2952132281440452\n",
            "  time_since_restore: 41.766637563705444\n",
            "  time_this_iter_s: 11.59141492843628\n",
            "  time_total_s: 41.766637563705444\n",
            "  timers:\n",
            "    learn_throughput: 26928.597\n",
            "    learn_time_ms: 7.427\n",
            "    load_throughput: 920914.261\n",
            "    load_time_ms: 0.217\n",
            "    training_iteration_time_ms: 253.556\n",
            "    update_time_ms: 3.967\n",
            "  timestamp: 1656953911\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 20400\n",
            "  training_iteration: 4\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:36 (running for 00:01:06.05)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.7666</td><td style=\"text-align: right;\">20400</td><td style=\"text-align: right;\">   53.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             53.39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:41 (running for 00:01:11.16)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         41.7666</td><td style=\"text-align: right;\">20400</td><td style=\"text-align: right;\">   53.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             53.39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 29800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 29800\n",
            "    num_agent_steps_trained: 29800\n",
            "    num_env_steps_sampled: 29800\n",
            "    num_env_steps_trained: 29800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-58-41\n",
            "  done: false\n",
            "  episode_len_mean: 65.96503496503496\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 65.96503496503496\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 143\n",
            "  episodes_total: 723\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.98179626464844\n",
            "          policy_loss: 1019.970947265625\n",
            "          var_gnorm: 22.744802474975586\n",
            "          vf_explained_var: 0.00992363691329956\n",
            "          vf_loss: 9900.123046875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 29800\n",
            "    num_agent_steps_trained: 29800\n",
            "    num_env_steps_sampled: 29800\n",
            "    num_env_steps_trained: 29800\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 29800\n",
            "  num_agent_steps_trained: 29800\n",
            "  num_env_steps_sampled: 29800\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 29800\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.92666666666669\n",
            "    ram_util_percent: 21.493333333333332\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1430555475085352\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1126019134598679\n",
            "    mean_inference_ms: 1.3663072760781456\n",
            "    mean_raw_obs_processing_ms: 0.27655436148291507\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 65.96503496503496\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 65.96503496503496\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 143\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 47\n",
            "      - 142\n",
            "      - 50\n",
            "      - 68\n",
            "      - 49\n",
            "      - 55\n",
            "      - 75\n",
            "      - 35\n",
            "      - 61\n",
            "      - 58\n",
            "      - 52\n",
            "      - 62\n",
            "      - 68\n",
            "      - 103\n",
            "      - 50\n",
            "      - 17\n",
            "      - 34\n",
            "      - 58\n",
            "      - 86\n",
            "      - 66\n",
            "      - 65\n",
            "      - 42\n",
            "      - 34\n",
            "      - 40\n",
            "      - 59\n",
            "      - 73\n",
            "      - 136\n",
            "      - 118\n",
            "      - 88\n",
            "      - 55\n",
            "      - 52\n",
            "      - 44\n",
            "      - 79\n",
            "      - 51\n",
            "      - 69\n",
            "      - 56\n",
            "      - 30\n",
            "      - 61\n",
            "      - 132\n",
            "      - 200\n",
            "      - 41\n",
            "      - 84\n",
            "      - 65\n",
            "      - 81\n",
            "      - 64\n",
            "      - 104\n",
            "      - 79\n",
            "      - 63\n",
            "      - 49\n",
            "      - 55\n",
            "      - 42\n",
            "      - 157\n",
            "      - 200\n",
            "      - 43\n",
            "      - 52\n",
            "      - 59\n",
            "      - 117\n",
            "      - 67\n",
            "      - 33\n",
            "      - 76\n",
            "      - 26\n",
            "      - 54\n",
            "      - 81\n",
            "      - 200\n",
            "      - 27\n",
            "      - 70\n",
            "      - 97\n",
            "      - 33\n",
            "      - 62\n",
            "      - 43\n",
            "      - 44\n",
            "      - 74\n",
            "      - 19\n",
            "      - 92\n",
            "      - 57\n",
            "      - 83\n",
            "      - 42\n",
            "      - 60\n",
            "      - 127\n",
            "      - 26\n",
            "      - 54\n",
            "      - 59\n",
            "      - 14\n",
            "      - 88\n",
            "      - 91\n",
            "      - 57\n",
            "      - 37\n",
            "      - 27\n",
            "      - 34\n",
            "      - 90\n",
            "      - 42\n",
            "      - 97\n",
            "      - 31\n",
            "      - 43\n",
            "      - 25\n",
            "      - 42\n",
            "      - 25\n",
            "      - 29\n",
            "      - 42\n",
            "      - 37\n",
            "      - 64\n",
            "      - 67\n",
            "      - 64\n",
            "      - 56\n",
            "      - 63\n",
            "      - 39\n",
            "      - 89\n",
            "      - 40\n",
            "      - 52\n",
            "      - 41\n",
            "      - 26\n",
            "      - 85\n",
            "      - 30\n",
            "      - 85\n",
            "      - 50\n",
            "      - 74\n",
            "      - 71\n",
            "      - 17\n",
            "      - 83\n",
            "      - 36\n",
            "      - 36\n",
            "      - 128\n",
            "      - 65\n",
            "      - 118\n",
            "      - 38\n",
            "      - 58\n",
            "      - 104\n",
            "      - 66\n",
            "      - 118\n",
            "      - 36\n",
            "      - 43\n",
            "      - 97\n",
            "      - 62\n",
            "      - 124\n",
            "      - 36\n",
            "      - 125\n",
            "      - 127\n",
            "      - 106\n",
            "      - 52\n",
            "      - 71\n",
            "      - 74\n",
            "      - 41\n",
            "      - 44\n",
            "      episode_reward:\n",
            "      - 47.0\n",
            "      - 142.0\n",
            "      - 50.0\n",
            "      - 68.0\n",
            "      - 49.0\n",
            "      - 55.0\n",
            "      - 75.0\n",
            "      - 35.0\n",
            "      - 61.0\n",
            "      - 58.0\n",
            "      - 52.0\n",
            "      - 62.0\n",
            "      - 68.0\n",
            "      - 103.0\n",
            "      - 50.0\n",
            "      - 17.0\n",
            "      - 34.0\n",
            "      - 58.0\n",
            "      - 86.0\n",
            "      - 66.0\n",
            "      - 65.0\n",
            "      - 42.0\n",
            "      - 34.0\n",
            "      - 40.0\n",
            "      - 59.0\n",
            "      - 73.0\n",
            "      - 136.0\n",
            "      - 118.0\n",
            "      - 88.0\n",
            "      - 55.0\n",
            "      - 52.0\n",
            "      - 44.0\n",
            "      - 79.0\n",
            "      - 51.0\n",
            "      - 69.0\n",
            "      - 56.0\n",
            "      - 30.0\n",
            "      - 61.0\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 41.0\n",
            "      - 84.0\n",
            "      - 65.0\n",
            "      - 81.0\n",
            "      - 64.0\n",
            "      - 104.0\n",
            "      - 79.0\n",
            "      - 63.0\n",
            "      - 49.0\n",
            "      - 55.0\n",
            "      - 42.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 52.0\n",
            "      - 59.0\n",
            "      - 117.0\n",
            "      - 67.0\n",
            "      - 33.0\n",
            "      - 76.0\n",
            "      - 26.0\n",
            "      - 54.0\n",
            "      - 81.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 70.0\n",
            "      - 97.0\n",
            "      - 33.0\n",
            "      - 62.0\n",
            "      - 43.0\n",
            "      - 44.0\n",
            "      - 74.0\n",
            "      - 19.0\n",
            "      - 92.0\n",
            "      - 57.0\n",
            "      - 83.0\n",
            "      - 42.0\n",
            "      - 60.0\n",
            "      - 127.0\n",
            "      - 26.0\n",
            "      - 54.0\n",
            "      - 59.0\n",
            "      - 14.0\n",
            "      - 88.0\n",
            "      - 91.0\n",
            "      - 57.0\n",
            "      - 37.0\n",
            "      - 27.0\n",
            "      - 34.0\n",
            "      - 90.0\n",
            "      - 42.0\n",
            "      - 97.0\n",
            "      - 31.0\n",
            "      - 43.0\n",
            "      - 25.0\n",
            "      - 42.0\n",
            "      - 25.0\n",
            "      - 29.0\n",
            "      - 42.0\n",
            "      - 37.0\n",
            "      - 64.0\n",
            "      - 67.0\n",
            "      - 64.0\n",
            "      - 56.0\n",
            "      - 63.0\n",
            "      - 39.0\n",
            "      - 89.0\n",
            "      - 40.0\n",
            "      - 52.0\n",
            "      - 41.0\n",
            "      - 26.0\n",
            "      - 85.0\n",
            "      - 30.0\n",
            "      - 85.0\n",
            "      - 50.0\n",
            "      - 74.0\n",
            "      - 71.0\n",
            "      - 17.0\n",
            "      - 83.0\n",
            "      - 36.0\n",
            "      - 36.0\n",
            "      - 128.0\n",
            "      - 65.0\n",
            "      - 118.0\n",
            "      - 38.0\n",
            "      - 58.0\n",
            "      - 104.0\n",
            "      - 66.0\n",
            "      - 118.0\n",
            "      - 36.0\n",
            "      - 43.0\n",
            "      - 97.0\n",
            "      - 62.0\n",
            "      - 124.0\n",
            "      - 36.0\n",
            "      - 125.0\n",
            "      - 127.0\n",
            "      - 106.0\n",
            "      - 52.0\n",
            "      - 71.0\n",
            "      - 74.0\n",
            "      - 41.0\n",
            "      - 44.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1430555475085352\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1126019134598679\n",
            "      mean_inference_ms: 1.3663072760781456\n",
            "      mean_raw_obs_processing_ms: 0.27655436148291507\n",
            "  time_since_restore: 51.98202562332153\n",
            "  time_this_iter_s: 10.215388059616089\n",
            "  time_total_s: 51.98202562332153\n",
            "  timers:\n",
            "    learn_throughput: 32770.56\n",
            "    learn_time_ms: 6.103\n",
            "    load_throughput: 1047528.472\n",
            "    load_time_ms: 0.191\n",
            "    training_iteration_time_ms: 211.765\n",
            "    update_time_ms: 3.548\n",
            "  timestamp: 1656953921\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 29800\n",
            "  training_iteration: 5\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:46 (running for 00:01:16.30)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          51.982</td><td style=\"text-align: right;\">29800</td><td style=\"text-align: right;\">  65.965</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            65.965</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:51 (running for 00:01:21.39)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          51.982</td><td style=\"text-align: right;\">29800</td><td style=\"text-align: right;\">  65.965</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            65.965</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 30800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 30800\n",
            "    num_agent_steps_trained: 30800\n",
            "    num_env_steps_sampled: 30800\n",
            "    num_env_steps_trained: 30800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-58-53\n",
            "  done: false\n",
            "  episode_len_mean: 67.3\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 67.3\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 9\n",
            "  episodes_total: 732\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 95.45\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 95.45\n",
            "    episode_reward_min: 26.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 186\n",
            "      - 26\n",
            "      - 80\n",
            "      - 57\n",
            "      - 80\n",
            "      - 63\n",
            "      - 129\n",
            "      - 81\n",
            "      - 74\n",
            "      - 35\n",
            "      - 200\n",
            "      - 60\n",
            "      - 39\n",
            "      - 132\n",
            "      - 73\n",
            "      - 132\n",
            "      - 152\n",
            "      - 32\n",
            "      - 145\n",
            "      - 133\n",
            "      episode_reward:\n",
            "      - 186.0\n",
            "      - 26.0\n",
            "      - 80.0\n",
            "      - 57.0\n",
            "      - 80.0\n",
            "      - 63.0\n",
            "      - 129.0\n",
            "      - 81.0\n",
            "      - 74.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 60.0\n",
            "      - 39.0\n",
            "      - 132.0\n",
            "      - 73.0\n",
            "      - 132.0\n",
            "      - 152.0\n",
            "      - 32.0\n",
            "      - 145.0\n",
            "      - 133.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10063315194750588\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0770688002882631\n",
            "      mean_inference_ms: 0.9858508553364379\n",
            "      mean_raw_obs_processing_ms: 0.11204997428150136\n",
            "    timesteps_this_iter: 1909\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.117919921875\n",
            "          policy_loss: 1180.5009765625\n",
            "          var_gnorm: 22.748939514160156\n",
            "          vf_explained_var: 0.0001386404037475586\n",
            "          vf_loss: 12204.4326171875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 30800\n",
            "    num_agent_steps_trained: 30800\n",
            "    num_env_steps_sampled: 30800\n",
            "    num_env_steps_trained: 30800\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 30800\n",
            "  num_agent_steps_trained: 30800\n",
            "  num_env_steps_sampled: 30800\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 30800\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.32941176470588\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14219911998097376\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11261600554104775\n",
            "    mean_inference_ms: 1.3623217924798905\n",
            "    mean_raw_obs_processing_ms: 0.280620858153344\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 67.3\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 67.3\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 9\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 43\n",
            "      - 52\n",
            "      - 59\n",
            "      - 117\n",
            "      - 67\n",
            "      - 33\n",
            "      - 76\n",
            "      - 26\n",
            "      - 54\n",
            "      - 81\n",
            "      - 200\n",
            "      - 27\n",
            "      - 70\n",
            "      - 97\n",
            "      - 33\n",
            "      - 62\n",
            "      - 43\n",
            "      - 44\n",
            "      - 74\n",
            "      - 19\n",
            "      - 92\n",
            "      - 57\n",
            "      - 83\n",
            "      - 42\n",
            "      - 60\n",
            "      - 127\n",
            "      - 26\n",
            "      - 54\n",
            "      - 59\n",
            "      - 14\n",
            "      - 88\n",
            "      - 91\n",
            "      - 57\n",
            "      - 37\n",
            "      - 27\n",
            "      - 34\n",
            "      - 90\n",
            "      - 42\n",
            "      - 97\n",
            "      - 31\n",
            "      - 43\n",
            "      - 25\n",
            "      - 42\n",
            "      - 25\n",
            "      - 29\n",
            "      - 42\n",
            "      - 37\n",
            "      - 64\n",
            "      - 67\n",
            "      - 64\n",
            "      - 56\n",
            "      - 63\n",
            "      - 39\n",
            "      - 89\n",
            "      - 40\n",
            "      - 52\n",
            "      - 41\n",
            "      - 26\n",
            "      - 85\n",
            "      - 30\n",
            "      - 85\n",
            "      - 50\n",
            "      - 74\n",
            "      - 71\n",
            "      - 17\n",
            "      - 83\n",
            "      - 36\n",
            "      - 36\n",
            "      - 128\n",
            "      - 65\n",
            "      - 118\n",
            "      - 38\n",
            "      - 58\n",
            "      - 104\n",
            "      - 66\n",
            "      - 118\n",
            "      - 36\n",
            "      - 43\n",
            "      - 97\n",
            "      - 62\n",
            "      - 124\n",
            "      - 36\n",
            "      - 125\n",
            "      - 127\n",
            "      - 106\n",
            "      - 52\n",
            "      - 71\n",
            "      - 74\n",
            "      - 41\n",
            "      - 44\n",
            "      - 107\n",
            "      - 56\n",
            "      - 51\n",
            "      - 106\n",
            "      - 95\n",
            "      - 105\n",
            "      - 92\n",
            "      - 91\n",
            "      - 198\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 52.0\n",
            "      - 59.0\n",
            "      - 117.0\n",
            "      - 67.0\n",
            "      - 33.0\n",
            "      - 76.0\n",
            "      - 26.0\n",
            "      - 54.0\n",
            "      - 81.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 70.0\n",
            "      - 97.0\n",
            "      - 33.0\n",
            "      - 62.0\n",
            "      - 43.0\n",
            "      - 44.0\n",
            "      - 74.0\n",
            "      - 19.0\n",
            "      - 92.0\n",
            "      - 57.0\n",
            "      - 83.0\n",
            "      - 42.0\n",
            "      - 60.0\n",
            "      - 127.0\n",
            "      - 26.0\n",
            "      - 54.0\n",
            "      - 59.0\n",
            "      - 14.0\n",
            "      - 88.0\n",
            "      - 91.0\n",
            "      - 57.0\n",
            "      - 37.0\n",
            "      - 27.0\n",
            "      - 34.0\n",
            "      - 90.0\n",
            "      - 42.0\n",
            "      - 97.0\n",
            "      - 31.0\n",
            "      - 43.0\n",
            "      - 25.0\n",
            "      - 42.0\n",
            "      - 25.0\n",
            "      - 29.0\n",
            "      - 42.0\n",
            "      - 37.0\n",
            "      - 64.0\n",
            "      - 67.0\n",
            "      - 64.0\n",
            "      - 56.0\n",
            "      - 63.0\n",
            "      - 39.0\n",
            "      - 89.0\n",
            "      - 40.0\n",
            "      - 52.0\n",
            "      - 41.0\n",
            "      - 26.0\n",
            "      - 85.0\n",
            "      - 30.0\n",
            "      - 85.0\n",
            "      - 50.0\n",
            "      - 74.0\n",
            "      - 71.0\n",
            "      - 17.0\n",
            "      - 83.0\n",
            "      - 36.0\n",
            "      - 36.0\n",
            "      - 128.0\n",
            "      - 65.0\n",
            "      - 118.0\n",
            "      - 38.0\n",
            "      - 58.0\n",
            "      - 104.0\n",
            "      - 66.0\n",
            "      - 118.0\n",
            "      - 36.0\n",
            "      - 43.0\n",
            "      - 97.0\n",
            "      - 62.0\n",
            "      - 124.0\n",
            "      - 36.0\n",
            "      - 125.0\n",
            "      - 127.0\n",
            "      - 106.0\n",
            "      - 52.0\n",
            "      - 71.0\n",
            "      - 74.0\n",
            "      - 41.0\n",
            "      - 44.0\n",
            "      - 107.0\n",
            "      - 56.0\n",
            "      - 51.0\n",
            "      - 106.0\n",
            "      - 95.0\n",
            "      - 105.0\n",
            "      - 92.0\n",
            "      - 91.0\n",
            "      - 198.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14219911998097376\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11261600554104775\n",
            "      mean_inference_ms: 1.3623217924798905\n",
            "      mean_raw_obs_processing_ms: 0.280620858153344\n",
            "  time_since_restore: 63.77640414237976\n",
            "  time_this_iter_s: 11.794378519058228\n",
            "  time_total_s: 63.77640414237976\n",
            "  timers:\n",
            "    learn_throughput: 33877.214\n",
            "    learn_time_ms: 5.904\n",
            "    load_throughput: 1058766.629\n",
            "    load_time_ms: 0.189\n",
            "    training_iteration_time_ms: 212.4\n",
            "    update_time_ms: 3.569\n",
            "  timestamp: 1656953933\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 30800\n",
            "  training_iteration: 6\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:58:58 (running for 00:01:28.13)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         63.7764</td><td style=\"text-align: right;\">30800</td><td style=\"text-align: right;\">    67.3</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">              67.3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:03 (running for 00:01:33.37)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         63.7764</td><td style=\"text-align: right;\">30800</td><td style=\"text-align: right;\">    67.3</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">              67.3</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 40200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 40200\n",
            "    num_agent_steps_trained: 40200\n",
            "    num_env_steps_sampled: 40200\n",
            "    num_env_steps_trained: 40200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-59-03\n",
            "  done: false\n",
            "  episode_len_mean: 104.69\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 104.69\n",
            "  episode_reward_min: 24.0\n",
            "  episodes_this_iter: 87\n",
            "  episodes_total: 819\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 125.50918579101562\n",
            "          policy_loss: 1115.4632568359375\n",
            "          var_gnorm: 22.785213470458984\n",
            "          vf_explained_var: 0.004967153072357178\n",
            "          vf_loss: 11655.9375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 40200\n",
            "    num_agent_steps_trained: 40200\n",
            "    num_env_steps_sampled: 40200\n",
            "    num_env_steps_trained: 40200\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 40200\n",
            "  num_agent_steps_trained: 40200\n",
            "  num_env_steps_sampled: 40200\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 40200\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.38000000000001\n",
            "    ram_util_percent: 21.513333333333332\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14204916779782586\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11148331344577808\n",
            "    mean_inference_ms: 1.357835577529396\n",
            "    mean_raw_obs_processing_ms: 0.26889614774198045\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 104.69\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 104.69\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 87\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 71\n",
            "      - 74\n",
            "      - 41\n",
            "      - 44\n",
            "      - 107\n",
            "      - 56\n",
            "      - 51\n",
            "      - 106\n",
            "      - 95\n",
            "      - 105\n",
            "      - 92\n",
            "      - 91\n",
            "      - 198\n",
            "      - 136\n",
            "      - 43\n",
            "      - 44\n",
            "      - 124\n",
            "      - 100\n",
            "      - 146\n",
            "      - 193\n",
            "      - 42\n",
            "      - 49\n",
            "      - 78\n",
            "      - 66\n",
            "      - 46\n",
            "      - 101\n",
            "      - 122\n",
            "      - 65\n",
            "      - 86\n",
            "      - 119\n",
            "      - 53\n",
            "      - 200\n",
            "      - 138\n",
            "      - 63\n",
            "      - 104\n",
            "      - 55\n",
            "      - 43\n",
            "      - 30\n",
            "      - 104\n",
            "      - 28\n",
            "      - 173\n",
            "      - 83\n",
            "      - 159\n",
            "      - 200\n",
            "      - 101\n",
            "      - 90\n",
            "      - 175\n",
            "      - 152\n",
            "      - 200\n",
            "      - 72\n",
            "      - 152\n",
            "      - 92\n",
            "      - 104\n",
            "      - 152\n",
            "      - 99\n",
            "      - 151\n",
            "      - 125\n",
            "      - 72\n",
            "      - 69\n",
            "      - 50\n",
            "      - 200\n",
            "      - 65\n",
            "      - 124\n",
            "      - 75\n",
            "      - 126\n",
            "      - 112\n",
            "      - 122\n",
            "      - 82\n",
            "      - 81\n",
            "      - 25\n",
            "      - 97\n",
            "      - 178\n",
            "      - 83\n",
            "      - 99\n",
            "      - 96\n",
            "      - 131\n",
            "      - 188\n",
            "      - 149\n",
            "      - 84\n",
            "      - 200\n",
            "      - 82\n",
            "      - 101\n",
            "      - 61\n",
            "      - 35\n",
            "      - 184\n",
            "      - 128\n",
            "      - 122\n",
            "      - 92\n",
            "      - 37\n",
            "      - 144\n",
            "      - 49\n",
            "      - 24\n",
            "      - 162\n",
            "      - 176\n",
            "      - 155\n",
            "      - 109\n",
            "      - 176\n",
            "      - 163\n",
            "      - 99\n",
            "      - 73\n",
            "      episode_reward:\n",
            "      - 71.0\n",
            "      - 74.0\n",
            "      - 41.0\n",
            "      - 44.0\n",
            "      - 107.0\n",
            "      - 56.0\n",
            "      - 51.0\n",
            "      - 106.0\n",
            "      - 95.0\n",
            "      - 105.0\n",
            "      - 92.0\n",
            "      - 91.0\n",
            "      - 198.0\n",
            "      - 136.0\n",
            "      - 43.0\n",
            "      - 44.0\n",
            "      - 124.0\n",
            "      - 100.0\n",
            "      - 146.0\n",
            "      - 193.0\n",
            "      - 42.0\n",
            "      - 49.0\n",
            "      - 78.0\n",
            "      - 66.0\n",
            "      - 46.0\n",
            "      - 101.0\n",
            "      - 122.0\n",
            "      - 65.0\n",
            "      - 86.0\n",
            "      - 119.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 63.0\n",
            "      - 104.0\n",
            "      - 55.0\n",
            "      - 43.0\n",
            "      - 30.0\n",
            "      - 104.0\n",
            "      - 28.0\n",
            "      - 173.0\n",
            "      - 83.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 90.0\n",
            "      - 175.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 72.0\n",
            "      - 152.0\n",
            "      - 92.0\n",
            "      - 104.0\n",
            "      - 152.0\n",
            "      - 99.0\n",
            "      - 151.0\n",
            "      - 125.0\n",
            "      - 72.0\n",
            "      - 69.0\n",
            "      - 50.0\n",
            "      - 200.0\n",
            "      - 65.0\n",
            "      - 124.0\n",
            "      - 75.0\n",
            "      - 126.0\n",
            "      - 112.0\n",
            "      - 122.0\n",
            "      - 82.0\n",
            "      - 81.0\n",
            "      - 25.0\n",
            "      - 97.0\n",
            "      - 178.0\n",
            "      - 83.0\n",
            "      - 99.0\n",
            "      - 96.0\n",
            "      - 131.0\n",
            "      - 188.0\n",
            "      - 149.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 82.0\n",
            "      - 101.0\n",
            "      - 61.0\n",
            "      - 35.0\n",
            "      - 184.0\n",
            "      - 128.0\n",
            "      - 122.0\n",
            "      - 92.0\n",
            "      - 37.0\n",
            "      - 144.0\n",
            "      - 49.0\n",
            "      - 24.0\n",
            "      - 162.0\n",
            "      - 176.0\n",
            "      - 155.0\n",
            "      - 109.0\n",
            "      - 176.0\n",
            "      - 163.0\n",
            "      - 99.0\n",
            "      - 73.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14204916779782586\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11148331344577808\n",
            "      mean_inference_ms: 1.357835577529396\n",
            "      mean_raw_obs_processing_ms: 0.26889614774198045\n",
            "  time_since_restore: 74.07889008522034\n",
            "  time_this_iter_s: 10.302485942840576\n",
            "  time_total_s: 74.07889008522034\n",
            "  timers:\n",
            "    learn_throughput: 27743.042\n",
            "    learn_time_ms: 7.209\n",
            "    load_throughput: 961004.468\n",
            "    load_time_ms: 0.208\n",
            "    training_iteration_time_ms: 251.928\n",
            "    update_time_ms: 3.445\n",
            "  timestamp: 1656953943\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 40200\n",
            "  training_iteration: 7\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:09 (running for 00:01:38.51)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         74.0789</td><td style=\"text-align: right;\">40200</td><td style=\"text-align: right;\">  104.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            104.69</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:14 (running for 00:01:43.61)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         74.0789</td><td style=\"text-align: right;\">40200</td><td style=\"text-align: right;\">  104.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            104.69</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 41000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 41000\n",
            "    num_agent_steps_trained: 41000\n",
            "    num_env_steps_sampled: 41000\n",
            "    num_env_steps_trained: 41000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-59-16\n",
            "  done: false\n",
            "  episode_len_mean: 106.0\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 106.0\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 10\n",
            "  episodes_total: 829\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 102.15\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 102.15\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 103\n",
            "      - 122\n",
            "      - 130\n",
            "      - 156\n",
            "      - 79\n",
            "      - 109\n",
            "      - 116\n",
            "      - 111\n",
            "      - 136\n",
            "      - 141\n",
            "      - 86\n",
            "      - 57\n",
            "      - 120\n",
            "      - 143\n",
            "      - 34\n",
            "      - 200\n",
            "      - 28\n",
            "      - 19\n",
            "      - 67\n",
            "      - 86\n",
            "      episode_reward:\n",
            "      - 103.0\n",
            "      - 122.0\n",
            "      - 130.0\n",
            "      - 156.0\n",
            "      - 79.0\n",
            "      - 109.0\n",
            "      - 116.0\n",
            "      - 111.0\n",
            "      - 136.0\n",
            "      - 141.0\n",
            "      - 86.0\n",
            "      - 57.0\n",
            "      - 120.0\n",
            "      - 143.0\n",
            "      - 34.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 19.0\n",
            "      - 67.0\n",
            "      - 86.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1014853594824672\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07790536619722843\n",
            "      mean_inference_ms: 0.9961749892681839\n",
            "      mean_raw_obs_processing_ms: 0.11163974025597176\n",
            "    timesteps_this_iter: 2043\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 124.38644409179688\n",
            "          policy_loss: 1094.271728515625\n",
            "          var_gnorm: 22.787017822265625\n",
            "          vf_explained_var: 0.012893736362457275\n",
            "          vf_loss: 11021.794921875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 41000\n",
            "    num_agent_steps_trained: 41000\n",
            "    num_env_steps_sampled: 41000\n",
            "    num_env_steps_trained: 41000\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 41000\n",
            "  num_agent_steps_trained: 41000\n",
            "  num_env_steps_sampled: 41000\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 41000\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 71.92222222222223\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14196897510435827\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11136025204682012\n",
            "    mean_inference_ms: 1.3577461787400824\n",
            "    mean_raw_obs_processing_ms: 0.2681927636811134\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 106.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 106.0\n",
            "    episode_reward_min: 22.0\n",
            "    episodes_this_iter: 10\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 92\n",
            "      - 91\n",
            "      - 198\n",
            "      - 136\n",
            "      - 43\n",
            "      - 44\n",
            "      - 124\n",
            "      - 100\n",
            "      - 146\n",
            "      - 193\n",
            "      - 42\n",
            "      - 49\n",
            "      - 78\n",
            "      - 66\n",
            "      - 46\n",
            "      - 101\n",
            "      - 122\n",
            "      - 65\n",
            "      - 86\n",
            "      - 119\n",
            "      - 53\n",
            "      - 200\n",
            "      - 138\n",
            "      - 63\n",
            "      - 104\n",
            "      - 55\n",
            "      - 43\n",
            "      - 30\n",
            "      - 104\n",
            "      - 28\n",
            "      - 173\n",
            "      - 83\n",
            "      - 159\n",
            "      - 200\n",
            "      - 101\n",
            "      - 90\n",
            "      - 175\n",
            "      - 152\n",
            "      - 200\n",
            "      - 72\n",
            "      - 152\n",
            "      - 92\n",
            "      - 104\n",
            "      - 152\n",
            "      - 99\n",
            "      - 151\n",
            "      - 125\n",
            "      - 72\n",
            "      - 69\n",
            "      - 50\n",
            "      - 200\n",
            "      - 65\n",
            "      - 124\n",
            "      - 75\n",
            "      - 126\n",
            "      - 112\n",
            "      - 122\n",
            "      - 82\n",
            "      - 81\n",
            "      - 25\n",
            "      - 97\n",
            "      - 178\n",
            "      - 83\n",
            "      - 99\n",
            "      - 96\n",
            "      - 131\n",
            "      - 188\n",
            "      - 149\n",
            "      - 84\n",
            "      - 200\n",
            "      - 82\n",
            "      - 101\n",
            "      - 61\n",
            "      - 35\n",
            "      - 184\n",
            "      - 128\n",
            "      - 122\n",
            "      - 92\n",
            "      - 37\n",
            "      - 144\n",
            "      - 49\n",
            "      - 24\n",
            "      - 162\n",
            "      - 176\n",
            "      - 155\n",
            "      - 109\n",
            "      - 176\n",
            "      - 163\n",
            "      - 99\n",
            "      - 73\n",
            "      - 140\n",
            "      - 36\n",
            "      - 60\n",
            "      - 65\n",
            "      - 101\n",
            "      - 141\n",
            "      - 117\n",
            "      - 22\n",
            "      - 76\n",
            "      - 123\n",
            "      episode_reward:\n",
            "      - 92.0\n",
            "      - 91.0\n",
            "      - 198.0\n",
            "      - 136.0\n",
            "      - 43.0\n",
            "      - 44.0\n",
            "      - 124.0\n",
            "      - 100.0\n",
            "      - 146.0\n",
            "      - 193.0\n",
            "      - 42.0\n",
            "      - 49.0\n",
            "      - 78.0\n",
            "      - 66.0\n",
            "      - 46.0\n",
            "      - 101.0\n",
            "      - 122.0\n",
            "      - 65.0\n",
            "      - 86.0\n",
            "      - 119.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 63.0\n",
            "      - 104.0\n",
            "      - 55.0\n",
            "      - 43.0\n",
            "      - 30.0\n",
            "      - 104.0\n",
            "      - 28.0\n",
            "      - 173.0\n",
            "      - 83.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 90.0\n",
            "      - 175.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 72.0\n",
            "      - 152.0\n",
            "      - 92.0\n",
            "      - 104.0\n",
            "      - 152.0\n",
            "      - 99.0\n",
            "      - 151.0\n",
            "      - 125.0\n",
            "      - 72.0\n",
            "      - 69.0\n",
            "      - 50.0\n",
            "      - 200.0\n",
            "      - 65.0\n",
            "      - 124.0\n",
            "      - 75.0\n",
            "      - 126.0\n",
            "      - 112.0\n",
            "      - 122.0\n",
            "      - 82.0\n",
            "      - 81.0\n",
            "      - 25.0\n",
            "      - 97.0\n",
            "      - 178.0\n",
            "      - 83.0\n",
            "      - 99.0\n",
            "      - 96.0\n",
            "      - 131.0\n",
            "      - 188.0\n",
            "      - 149.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 82.0\n",
            "      - 101.0\n",
            "      - 61.0\n",
            "      - 35.0\n",
            "      - 184.0\n",
            "      - 128.0\n",
            "      - 122.0\n",
            "      - 92.0\n",
            "      - 37.0\n",
            "      - 144.0\n",
            "      - 49.0\n",
            "      - 24.0\n",
            "      - 162.0\n",
            "      - 176.0\n",
            "      - 155.0\n",
            "      - 109.0\n",
            "      - 176.0\n",
            "      - 163.0\n",
            "      - 99.0\n",
            "      - 73.0\n",
            "      - 140.0\n",
            "      - 36.0\n",
            "      - 60.0\n",
            "      - 65.0\n",
            "      - 101.0\n",
            "      - 141.0\n",
            "      - 117.0\n",
            "      - 22.0\n",
            "      - 76.0\n",
            "      - 123.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14196897510435827\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11136025204682012\n",
            "      mean_inference_ms: 1.3577461787400824\n",
            "      mean_raw_obs_processing_ms: 0.2681927636811134\n",
            "  time_since_restore: 86.69735860824585\n",
            "  time_this_iter_s: 12.618468523025513\n",
            "  time_total_s: 86.69735860824585\n",
            "  timers:\n",
            "    learn_throughput: 24292.626\n",
            "    learn_time_ms: 8.233\n",
            "    load_throughput: 940954.347\n",
            "    load_time_ms: 0.213\n",
            "    training_iteration_time_ms: 282.353\n",
            "    update_time_ms: 3.47\n",
            "  timestamp: 1656953956\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 41000\n",
            "  training_iteration: 8\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:21 (running for 00:01:51.14)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         86.6974</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">     106</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">               106</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:26 (running for 00:01:56.20)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         86.6974</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">     106</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">               106</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 50600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 50600\n",
            "    num_agent_steps_trained: 50600\n",
            "    num_env_steps_sampled: 50600\n",
            "    num_env_steps_trained: 50600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-59-26\n",
            "  done: false\n",
            "  episode_len_mean: 117.36\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 117.36\n",
            "  episode_reward_min: 17.0\n",
            "  episodes_this_iter: 79\n",
            "  episodes_total: 908\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 117.91451263427734\n",
            "          policy_loss: 1178.751708984375\n",
            "          var_gnorm: 22.830032348632812\n",
            "          vf_explained_var: -0.0002713203430175781\n",
            "          vf_loss: 12033.556640625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 50600\n",
            "    num_agent_steps_trained: 50600\n",
            "    num_env_steps_sampled: 50600\n",
            "    num_env_steps_trained: 50600\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 50600\n",
            "  num_agent_steps_trained: 50600\n",
            "  num_env_steps_sampled: 50600\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 50600\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.49285714285715\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14064310823184425\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1104294495772191\n",
            "    mean_inference_ms: 1.3482695373688223\n",
            "    mean_raw_obs_processing_ms: 0.264356773272089\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 117.36\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 117.36\n",
            "    episode_reward_min: 17.0\n",
            "    episodes_this_iter: 79\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 144\n",
            "      - 49\n",
            "      - 24\n",
            "      - 162\n",
            "      - 176\n",
            "      - 155\n",
            "      - 109\n",
            "      - 176\n",
            "      - 163\n",
            "      - 99\n",
            "      - 73\n",
            "      - 140\n",
            "      - 36\n",
            "      - 60\n",
            "      - 65\n",
            "      - 101\n",
            "      - 141\n",
            "      - 117\n",
            "      - 22\n",
            "      - 76\n",
            "      - 123\n",
            "      - 100\n",
            "      - 123\n",
            "      - 89\n",
            "      - 200\n",
            "      - 85\n",
            "      - 200\n",
            "      - 90\n",
            "      - 108\n",
            "      - 70\n",
            "      - 90\n",
            "      - 200\n",
            "      - 194\n",
            "      - 135\n",
            "      - 21\n",
            "      - 136\n",
            "      - 59\n",
            "      - 49\n",
            "      - 60\n",
            "      - 51\n",
            "      - 95\n",
            "      - 136\n",
            "      - 163\n",
            "      - 171\n",
            "      - 200\n",
            "      - 109\n",
            "      - 147\n",
            "      - 200\n",
            "      - 200\n",
            "      - 97\n",
            "      - 157\n",
            "      - 200\n",
            "      - 111\n",
            "      - 79\n",
            "      - 99\n",
            "      - 140\n",
            "      - 94\n",
            "      - 112\n",
            "      - 194\n",
            "      - 64\n",
            "      - 127\n",
            "      - 76\n",
            "      - 105\n",
            "      - 59\n",
            "      - 126\n",
            "      - 97\n",
            "      - 200\n",
            "      - 174\n",
            "      - 101\n",
            "      - 190\n",
            "      - 70\n",
            "      - 95\n",
            "      - 137\n",
            "      - 76\n",
            "      - 43\n",
            "      - 105\n",
            "      - 200\n",
            "      - 115\n",
            "      - 78\n",
            "      - 81\n",
            "      - 105\n",
            "      - 157\n",
            "      - 90\n",
            "      - 135\n",
            "      - 190\n",
            "      - 45\n",
            "      - 179\n",
            "      - 116\n",
            "      - 135\n",
            "      - 85\n",
            "      - 94\n",
            "      - 118\n",
            "      - 145\n",
            "      - 200\n",
            "      - 94\n",
            "      - 17\n",
            "      - 148\n",
            "      - 158\n",
            "      - 141\n",
            "      - 90\n",
            "      episode_reward:\n",
            "      - 144.0\n",
            "      - 49.0\n",
            "      - 24.0\n",
            "      - 162.0\n",
            "      - 176.0\n",
            "      - 155.0\n",
            "      - 109.0\n",
            "      - 176.0\n",
            "      - 163.0\n",
            "      - 99.0\n",
            "      - 73.0\n",
            "      - 140.0\n",
            "      - 36.0\n",
            "      - 60.0\n",
            "      - 65.0\n",
            "      - 101.0\n",
            "      - 141.0\n",
            "      - 117.0\n",
            "      - 22.0\n",
            "      - 76.0\n",
            "      - 123.0\n",
            "      - 100.0\n",
            "      - 123.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 85.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 108.0\n",
            "      - 70.0\n",
            "      - 90.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 135.0\n",
            "      - 21.0\n",
            "      - 136.0\n",
            "      - 59.0\n",
            "      - 49.0\n",
            "      - 60.0\n",
            "      - 51.0\n",
            "      - 95.0\n",
            "      - 136.0\n",
            "      - 163.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 97.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 111.0\n",
            "      - 79.0\n",
            "      - 99.0\n",
            "      - 140.0\n",
            "      - 94.0\n",
            "      - 112.0\n",
            "      - 194.0\n",
            "      - 64.0\n",
            "      - 127.0\n",
            "      - 76.0\n",
            "      - 105.0\n",
            "      - 59.0\n",
            "      - 126.0\n",
            "      - 97.0\n",
            "      - 200.0\n",
            "      - 174.0\n",
            "      - 101.0\n",
            "      - 190.0\n",
            "      - 70.0\n",
            "      - 95.0\n",
            "      - 137.0\n",
            "      - 76.0\n",
            "      - 43.0\n",
            "      - 105.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 78.0\n",
            "      - 81.0\n",
            "      - 105.0\n",
            "      - 157.0\n",
            "      - 90.0\n",
            "      - 135.0\n",
            "      - 190.0\n",
            "      - 45.0\n",
            "      - 179.0\n",
            "      - 116.0\n",
            "      - 135.0\n",
            "      - 85.0\n",
            "      - 94.0\n",
            "      - 118.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 17.0\n",
            "      - 148.0\n",
            "      - 158.0\n",
            "      - 141.0\n",
            "      - 90.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14064310823184425\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1104294495772191\n",
            "      mean_inference_ms: 1.3482695373688223\n",
            "      mean_raw_obs_processing_ms: 0.264356773272089\n",
            "  time_since_restore: 96.72449827194214\n",
            "  time_this_iter_s: 10.027139663696289\n",
            "  time_total_s: 96.72449827194214\n",
            "  timers:\n",
            "    learn_throughput: 30672.0\n",
            "    learn_time_ms: 6.521\n",
            "    load_throughput: 1082540.715\n",
            "    load_time_ms: 0.185\n",
            "    training_iteration_time_ms: 205.68\n",
            "    update_time_ms: 4.063\n",
            "  timestamp: 1656953966\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 50600\n",
            "  training_iteration: 9\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:31 (running for 00:02:01.24)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         96.7245</td><td style=\"text-align: right;\">50600</td><td style=\"text-align: right;\">  117.36</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            117.36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:36 (running for 00:02:06.31)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         96.7245</td><td style=\"text-align: right;\">50600</td><td style=\"text-align: right;\">  117.36</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            117.36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 51200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 51200\n",
            "    num_agent_steps_trained: 51200\n",
            "    num_env_steps_sampled: 51200\n",
            "    num_env_steps_trained: 51200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-59-36\n",
            "  done: false\n",
            "  episode_len_mean: 115.98\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 115.98\n",
            "  episode_reward_min: 17.0\n",
            "  episodes_this_iter: 6\n",
            "  episodes_total: 914\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 130.4\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 130.4\n",
            "    episode_reward_min: 42.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 174\n",
            "      - 178\n",
            "      - 158\n",
            "      - 106\n",
            "      - 82\n",
            "      - 42\n",
            "      - 89\n",
            "      - 144\n",
            "      - 104\n",
            "      - 71\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 66\n",
            "      - 62\n",
            "      - 154\n",
            "      - 148\n",
            "      - 138\n",
            "      - 200\n",
            "      - 92\n",
            "      episode_reward:\n",
            "      - 174.0\n",
            "      - 178.0\n",
            "      - 158.0\n",
            "      - 106.0\n",
            "      - 82.0\n",
            "      - 42.0\n",
            "      - 89.0\n",
            "      - 144.0\n",
            "      - 104.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 62.0\n",
            "      - 154.0\n",
            "      - 148.0\n",
            "      - 138.0\n",
            "      - 200.0\n",
            "      - 92.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10088394685473827\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07729325096097533\n",
            "      mean_inference_ms: 0.9862348497559166\n",
            "      mean_raw_obs_processing_ms: 0.1099603154660204\n",
            "    timesteps_this_iter: 2608\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 122.25386810302734\n",
            "          policy_loss: 1179.5830078125\n",
            "          var_gnorm: 22.832456588745117\n",
            "          vf_explained_var: -0.0036377906799316406\n",
            "          vf_loss: 12125.69921875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 51200\n",
            "    num_agent_steps_trained: 51200\n",
            "    num_env_steps_sampled: 51200\n",
            "    num_env_steps_trained: 51200\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 51200\n",
            "  num_agent_steps_trained: 51200\n",
            "  num_env_steps_sampled: 51200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 51200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.3142857142857\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14061658171572883\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11036852723028975\n",
            "    mean_inference_ms: 1.3478703122750721\n",
            "    mean_raw_obs_processing_ms: 0.2637644669545857\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 115.98\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 115.98\n",
            "    episode_reward_min: 17.0\n",
            "    episodes_this_iter: 6\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 109\n",
            "      - 176\n",
            "      - 163\n",
            "      - 99\n",
            "      - 73\n",
            "      - 140\n",
            "      - 36\n",
            "      - 60\n",
            "      - 65\n",
            "      - 101\n",
            "      - 141\n",
            "      - 117\n",
            "      - 22\n",
            "      - 76\n",
            "      - 123\n",
            "      - 100\n",
            "      - 123\n",
            "      - 89\n",
            "      - 200\n",
            "      - 85\n",
            "      - 200\n",
            "      - 90\n",
            "      - 108\n",
            "      - 70\n",
            "      - 90\n",
            "      - 200\n",
            "      - 194\n",
            "      - 135\n",
            "      - 21\n",
            "      - 136\n",
            "      - 59\n",
            "      - 49\n",
            "      - 60\n",
            "      - 51\n",
            "      - 95\n",
            "      - 136\n",
            "      - 163\n",
            "      - 171\n",
            "      - 200\n",
            "      - 109\n",
            "      - 147\n",
            "      - 200\n",
            "      - 200\n",
            "      - 97\n",
            "      - 157\n",
            "      - 200\n",
            "      - 111\n",
            "      - 79\n",
            "      - 99\n",
            "      - 140\n",
            "      - 94\n",
            "      - 112\n",
            "      - 194\n",
            "      - 64\n",
            "      - 127\n",
            "      - 76\n",
            "      - 105\n",
            "      - 59\n",
            "      - 126\n",
            "      - 97\n",
            "      - 200\n",
            "      - 174\n",
            "      - 101\n",
            "      - 190\n",
            "      - 70\n",
            "      - 95\n",
            "      - 137\n",
            "      - 76\n",
            "      - 43\n",
            "      - 105\n",
            "      - 200\n",
            "      - 115\n",
            "      - 78\n",
            "      - 81\n",
            "      - 105\n",
            "      - 157\n",
            "      - 90\n",
            "      - 135\n",
            "      - 190\n",
            "      - 45\n",
            "      - 179\n",
            "      - 116\n",
            "      - 135\n",
            "      - 85\n",
            "      - 94\n",
            "      - 118\n",
            "      - 145\n",
            "      - 200\n",
            "      - 94\n",
            "      - 17\n",
            "      - 148\n",
            "      - 158\n",
            "      - 141\n",
            "      - 90\n",
            "      - 88\n",
            "      - 88\n",
            "      - 176\n",
            "      - 29\n",
            "      - 103\n",
            "      - 88\n",
            "      episode_reward:\n",
            "      - 109.0\n",
            "      - 176.0\n",
            "      - 163.0\n",
            "      - 99.0\n",
            "      - 73.0\n",
            "      - 140.0\n",
            "      - 36.0\n",
            "      - 60.0\n",
            "      - 65.0\n",
            "      - 101.0\n",
            "      - 141.0\n",
            "      - 117.0\n",
            "      - 22.0\n",
            "      - 76.0\n",
            "      - 123.0\n",
            "      - 100.0\n",
            "      - 123.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 85.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 108.0\n",
            "      - 70.0\n",
            "      - 90.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 135.0\n",
            "      - 21.0\n",
            "      - 136.0\n",
            "      - 59.0\n",
            "      - 49.0\n",
            "      - 60.0\n",
            "      - 51.0\n",
            "      - 95.0\n",
            "      - 136.0\n",
            "      - 163.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 97.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 111.0\n",
            "      - 79.0\n",
            "      - 99.0\n",
            "      - 140.0\n",
            "      - 94.0\n",
            "      - 112.0\n",
            "      - 194.0\n",
            "      - 64.0\n",
            "      - 127.0\n",
            "      - 76.0\n",
            "      - 105.0\n",
            "      - 59.0\n",
            "      - 126.0\n",
            "      - 97.0\n",
            "      - 200.0\n",
            "      - 174.0\n",
            "      - 101.0\n",
            "      - 190.0\n",
            "      - 70.0\n",
            "      - 95.0\n",
            "      - 137.0\n",
            "      - 76.0\n",
            "      - 43.0\n",
            "      - 105.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 78.0\n",
            "      - 81.0\n",
            "      - 105.0\n",
            "      - 157.0\n",
            "      - 90.0\n",
            "      - 135.0\n",
            "      - 190.0\n",
            "      - 45.0\n",
            "      - 179.0\n",
            "      - 116.0\n",
            "      - 135.0\n",
            "      - 85.0\n",
            "      - 94.0\n",
            "      - 118.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 17.0\n",
            "      - 148.0\n",
            "      - 158.0\n",
            "      - 141.0\n",
            "      - 90.0\n",
            "      - 88.0\n",
            "      - 88.0\n",
            "      - 176.0\n",
            "      - 29.0\n",
            "      - 103.0\n",
            "      - 88.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14061658171572883\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11036852723028975\n",
            "      mean_inference_ms: 1.3478703122750721\n",
            "      mean_raw_obs_processing_ms: 0.2637644669545857\n",
            "  time_since_restore: 106.76713180541992\n",
            "  time_this_iter_s: 10.042633533477783\n",
            "  time_total_s: 106.76713180541992\n",
            "  timers:\n",
            "    learn_throughput: 31934.947\n",
            "    learn_time_ms: 6.263\n",
            "    load_throughput: 1039094.265\n",
            "    load_time_ms: 0.192\n",
            "    training_iteration_time_ms: 208.91\n",
            "    update_time_ms: 4.014\n",
            "  timestamp: 1656953976\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 51200\n",
            "  training_iteration: 10\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:41 (running for 00:02:11.35)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         106.767</td><td style=\"text-align: right;\">51200</td><td style=\"text-align: right;\">  115.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            115.98</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:46 (running for 00:02:16.40)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         106.767</td><td style=\"text-align: right;\">51200</td><td style=\"text-align: right;\">  115.98</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            115.98</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 60800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 60800\n",
            "    num_agent_steps_trained: 60800\n",
            "    num_env_steps_sampled: 60800\n",
            "    num_env_steps_trained: 60800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_16-59-46\n",
            "  done: false\n",
            "  episode_len_mean: 131.36\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 131.36\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 70\n",
            "  episodes_total: 984\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 120.94695281982422\n",
            "          policy_loss: 1107.5439453125\n",
            "          var_gnorm: 22.8756160736084\n",
            "          vf_explained_var: 0.014187157154083252\n",
            "          vf_loss: 11556.54296875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 60800\n",
            "    num_agent_steps_trained: 60800\n",
            "    num_env_steps_sampled: 60800\n",
            "    num_env_steps_trained: 60800\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 60800\n",
            "  num_agent_steps_trained: 60800\n",
            "  num_env_steps_sampled: 60800\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 60800\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.90000000000002\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13949222356296986\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1098303990753391\n",
            "    mean_inference_ms: 1.3347237792074786\n",
            "    mean_raw_obs_processing_ms: 0.26138817042167445\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 131.36\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 131.36\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 70\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 115\n",
            "      - 78\n",
            "      - 81\n",
            "      - 105\n",
            "      - 157\n",
            "      - 90\n",
            "      - 135\n",
            "      - 190\n",
            "      - 45\n",
            "      - 179\n",
            "      - 116\n",
            "      - 135\n",
            "      - 85\n",
            "      - 94\n",
            "      - 118\n",
            "      - 145\n",
            "      - 200\n",
            "      - 94\n",
            "      - 17\n",
            "      - 148\n",
            "      - 158\n",
            "      - 141\n",
            "      - 90\n",
            "      - 88\n",
            "      - 88\n",
            "      - 176\n",
            "      - 29\n",
            "      - 103\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 112\n",
            "      - 71\n",
            "      - 103\n",
            "      - 51\n",
            "      - 196\n",
            "      - 182\n",
            "      - 184\n",
            "      - 111\n",
            "      - 104\n",
            "      - 166\n",
            "      - 159\n",
            "      - 93\n",
            "      - 110\n",
            "      - 178\n",
            "      - 164\n",
            "      - 108\n",
            "      - 161\n",
            "      - 200\n",
            "      - 157\n",
            "      - 147\n",
            "      - 169\n",
            "      - 200\n",
            "      - 194\n",
            "      - 153\n",
            "      - 200\n",
            "      - 66\n",
            "      - 167\n",
            "      - 200\n",
            "      - 152\n",
            "      - 16\n",
            "      - 178\n",
            "      - 112\n",
            "      - 125\n",
            "      - 148\n",
            "      - 95\n",
            "      - 155\n",
            "      - 116\n",
            "      - 75\n",
            "      - 177\n",
            "      - 200\n",
            "      - 46\n",
            "      - 97\n",
            "      - 123\n",
            "      - 200\n",
            "      - 94\n",
            "      - 200\n",
            "      - 131\n",
            "      - 49\n",
            "      - 152\n",
            "      - 115\n",
            "      - 118\n",
            "      - 104\n",
            "      - 49\n",
            "      - 200\n",
            "      - 158\n",
            "      - 175\n",
            "      - 113\n",
            "      - 104\n",
            "      - 194\n",
            "      - 98\n",
            "      - 149\n",
            "      - 85\n",
            "      - 25\n",
            "      - 200\n",
            "      - 181\n",
            "      - 200\n",
            "      - 98\n",
            "      - 135\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 78.0\n",
            "      - 81.0\n",
            "      - 105.0\n",
            "      - 157.0\n",
            "      - 90.0\n",
            "      - 135.0\n",
            "      - 190.0\n",
            "      - 45.0\n",
            "      - 179.0\n",
            "      - 116.0\n",
            "      - 135.0\n",
            "      - 85.0\n",
            "      - 94.0\n",
            "      - 118.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 17.0\n",
            "      - 148.0\n",
            "      - 158.0\n",
            "      - 141.0\n",
            "      - 90.0\n",
            "      - 88.0\n",
            "      - 88.0\n",
            "      - 176.0\n",
            "      - 29.0\n",
            "      - 103.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 112.0\n",
            "      - 71.0\n",
            "      - 103.0\n",
            "      - 51.0\n",
            "      - 196.0\n",
            "      - 182.0\n",
            "      - 184.0\n",
            "      - 111.0\n",
            "      - 104.0\n",
            "      - 166.0\n",
            "      - 159.0\n",
            "      - 93.0\n",
            "      - 110.0\n",
            "      - 178.0\n",
            "      - 164.0\n",
            "      - 108.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 147.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 16.0\n",
            "      - 178.0\n",
            "      - 112.0\n",
            "      - 125.0\n",
            "      - 148.0\n",
            "      - 95.0\n",
            "      - 155.0\n",
            "      - 116.0\n",
            "      - 75.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 97.0\n",
            "      - 123.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 49.0\n",
            "      - 152.0\n",
            "      - 115.0\n",
            "      - 118.0\n",
            "      - 104.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 175.0\n",
            "      - 113.0\n",
            "      - 104.0\n",
            "      - 194.0\n",
            "      - 98.0\n",
            "      - 149.0\n",
            "      - 85.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13949222356296986\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1098303990753391\n",
            "      mean_inference_ms: 1.3347237792074786\n",
            "      mean_raw_obs_processing_ms: 0.26138817042167445\n",
            "  time_since_restore: 116.77867698669434\n",
            "  time_this_iter_s: 10.011545181274414\n",
            "  time_total_s: 116.77867698669434\n",
            "  timers:\n",
            "    learn_throughput: 33140.834\n",
            "    learn_time_ms: 6.035\n",
            "    load_throughput: 785597.303\n",
            "    load_time_ms: 0.255\n",
            "    training_iteration_time_ms: 202.933\n",
            "    update_time_ms: 3.601\n",
            "  timestamp: 1656953986\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 60800\n",
            "  training_iteration: 11\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:51 (running for 00:02:21.44)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         116.779</td><td style=\"text-align: right;\">60800</td><td style=\"text-align: right;\">  131.36</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            131.36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 16:59:57 (running for 00:02:26.53)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         116.779</td><td style=\"text-align: right;\">60800</td><td style=\"text-align: right;\">  131.36</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            131.36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 61400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 61400\n",
            "    num_agent_steps_trained: 61400\n",
            "    num_env_steps_sampled: 61400\n",
            "    num_env_steps_trained: 61400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-00-00\n",
            "  done: false\n",
            "  episode_len_mean: 132.8\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 132.8\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 988\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 158.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 158.0\n",
            "    episode_reward_min: 32.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 153\n",
            "      - 199\n",
            "      - 136\n",
            "      - 200\n",
            "      - 152\n",
            "      - 132\n",
            "      - 200\n",
            "      - 200\n",
            "      - 195\n",
            "      - 161\n",
            "      - 134\n",
            "      - 200\n",
            "      - 143\n",
            "      - 200\n",
            "      - 32\n",
            "      - 141\n",
            "      - 154\n",
            "      - 200\n",
            "      - 106\n",
            "      - 122\n",
            "      episode_reward:\n",
            "      - 153.0\n",
            "      - 199.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 195.0\n",
            "      - 161.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 32.0\n",
            "      - 141.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 106.0\n",
            "      - 122.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1004163719272519\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07695664525071474\n",
            "      mean_inference_ms: 0.9814919500291014\n",
            "      mean_raw_obs_processing_ms: 0.10849010005834833\n",
            "    timesteps_this_iter: 3160\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.28561401367188\n",
            "          policy_loss: 1136.353271484375\n",
            "          var_gnorm: 22.8781795501709\n",
            "          vf_explained_var: -0.010217666625976562\n",
            "          vf_loss: 11518.80078125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 61400\n",
            "    num_agent_steps_trained: 61400\n",
            "    num_env_steps_sampled: 61400\n",
            "    num_env_steps_trained: 61400\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 61400\n",
            "  num_agent_steps_trained: 61400\n",
            "  num_env_steps_sampled: 61400\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 61400\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 64.4684210526316\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13951662102774035\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10980330341125764\n",
            "    mean_inference_ms: 1.3343652712502425\n",
            "    mean_raw_obs_processing_ms: 0.26102345242817293\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 132.8\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 132.8\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 105\n",
            "      - 157\n",
            "      - 90\n",
            "      - 135\n",
            "      - 190\n",
            "      - 45\n",
            "      - 179\n",
            "      - 116\n",
            "      - 135\n",
            "      - 85\n",
            "      - 94\n",
            "      - 118\n",
            "      - 145\n",
            "      - 200\n",
            "      - 94\n",
            "      - 17\n",
            "      - 148\n",
            "      - 158\n",
            "      - 141\n",
            "      - 90\n",
            "      - 88\n",
            "      - 88\n",
            "      - 176\n",
            "      - 29\n",
            "      - 103\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 112\n",
            "      - 71\n",
            "      - 103\n",
            "      - 51\n",
            "      - 196\n",
            "      - 182\n",
            "      - 184\n",
            "      - 111\n",
            "      - 104\n",
            "      - 166\n",
            "      - 159\n",
            "      - 93\n",
            "      - 110\n",
            "      - 178\n",
            "      - 164\n",
            "      - 108\n",
            "      - 161\n",
            "      - 200\n",
            "      - 157\n",
            "      - 147\n",
            "      - 169\n",
            "      - 200\n",
            "      - 194\n",
            "      - 153\n",
            "      - 200\n",
            "      - 66\n",
            "      - 167\n",
            "      - 200\n",
            "      - 152\n",
            "      - 16\n",
            "      - 178\n",
            "      - 112\n",
            "      - 125\n",
            "      - 148\n",
            "      - 95\n",
            "      - 155\n",
            "      - 116\n",
            "      - 75\n",
            "      - 177\n",
            "      - 200\n",
            "      - 46\n",
            "      - 97\n",
            "      - 123\n",
            "      - 200\n",
            "      - 94\n",
            "      - 200\n",
            "      - 131\n",
            "      - 49\n",
            "      - 152\n",
            "      - 115\n",
            "      - 118\n",
            "      - 104\n",
            "      - 49\n",
            "      - 200\n",
            "      - 158\n",
            "      - 175\n",
            "      - 113\n",
            "      - 104\n",
            "      - 194\n",
            "      - 98\n",
            "      - 149\n",
            "      - 85\n",
            "      - 25\n",
            "      - 200\n",
            "      - 181\n",
            "      - 200\n",
            "      - 98\n",
            "      - 135\n",
            "      - 185\n",
            "      - 118\n",
            "      - 200\n",
            "      - 115\n",
            "      episode_reward:\n",
            "      - 105.0\n",
            "      - 157.0\n",
            "      - 90.0\n",
            "      - 135.0\n",
            "      - 190.0\n",
            "      - 45.0\n",
            "      - 179.0\n",
            "      - 116.0\n",
            "      - 135.0\n",
            "      - 85.0\n",
            "      - 94.0\n",
            "      - 118.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 17.0\n",
            "      - 148.0\n",
            "      - 158.0\n",
            "      - 141.0\n",
            "      - 90.0\n",
            "      - 88.0\n",
            "      - 88.0\n",
            "      - 176.0\n",
            "      - 29.0\n",
            "      - 103.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 112.0\n",
            "      - 71.0\n",
            "      - 103.0\n",
            "      - 51.0\n",
            "      - 196.0\n",
            "      - 182.0\n",
            "      - 184.0\n",
            "      - 111.0\n",
            "      - 104.0\n",
            "      - 166.0\n",
            "      - 159.0\n",
            "      - 93.0\n",
            "      - 110.0\n",
            "      - 178.0\n",
            "      - 164.0\n",
            "      - 108.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 147.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 194.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 16.0\n",
            "      - 178.0\n",
            "      - 112.0\n",
            "      - 125.0\n",
            "      - 148.0\n",
            "      - 95.0\n",
            "      - 155.0\n",
            "      - 116.0\n",
            "      - 75.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 97.0\n",
            "      - 123.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 49.0\n",
            "      - 152.0\n",
            "      - 115.0\n",
            "      - 118.0\n",
            "      - 104.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 175.0\n",
            "      - 113.0\n",
            "      - 104.0\n",
            "      - 194.0\n",
            "      - 98.0\n",
            "      - 149.0\n",
            "      - 85.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 185.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13951662102774035\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10980330341125764\n",
            "      mean_inference_ms: 1.3343652712502425\n",
            "      mean_raw_obs_processing_ms: 0.26102345242817293\n",
            "  time_since_restore: 129.94452810287476\n",
            "  time_this_iter_s: 13.16585111618042\n",
            "  time_total_s: 129.94452810287476\n",
            "  timers:\n",
            "    learn_throughput: 33042.016\n",
            "    learn_time_ms: 6.053\n",
            "    load_throughput: 778091.828\n",
            "    load_time_ms: 0.257\n",
            "    training_iteration_time_ms: 204.043\n",
            "    update_time_ms: 3.372\n",
            "  timestamp: 1656954000\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 61400\n",
            "  training_iteration: 12\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:05 (running for 00:02:34.65)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         129.945</td><td style=\"text-align: right;\">61400</td><td style=\"text-align: right;\">   132.8</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">             132.8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:10 (running for 00:02:39.75)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         129.945</td><td style=\"text-align: right;\">61400</td><td style=\"text-align: right;\">   132.8</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">             132.8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 70200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 70200\n",
            "    num_agent_steps_trained: 70200\n",
            "    num_env_steps_sampled: 70200\n",
            "    num_env_steps_trained: 70200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-00-10\n",
            "  done: false\n",
            "  episode_len_mean: 144.03\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 144.03\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 57\n",
            "  episodes_total: 1045\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 122.61073303222656\n",
            "          policy_loss: 1144.385986328125\n",
            "          var_gnorm: 22.93048667907715\n",
            "          vf_explained_var: -0.014585375785827637\n",
            "          vf_loss: 12094.109375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 70200\n",
            "    num_agent_steps_trained: 70200\n",
            "    num_env_steps_sampled: 70200\n",
            "    num_env_steps_trained: 70200\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 70200\n",
            "  num_agent_steps_trained: 70200\n",
            "  num_env_steps_sampled: 70200\n",
            "  num_env_steps_sampled_this_iter: 8800\n",
            "  num_env_steps_trained: 70200\n",
            "  num_env_steps_trained_this_iter: 8800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.13571428571429\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1400283334925435\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11015966059313576\n",
            "    mean_inference_ms: 1.3351741583651515\n",
            "    mean_raw_obs_processing_ms: 0.25983908418345414\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 144.03\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 144.03\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 57\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 16\n",
            "      - 178\n",
            "      - 112\n",
            "      - 125\n",
            "      - 148\n",
            "      - 95\n",
            "      - 155\n",
            "      - 116\n",
            "      - 75\n",
            "      - 177\n",
            "      - 200\n",
            "      - 46\n",
            "      - 97\n",
            "      - 123\n",
            "      - 200\n",
            "      - 94\n",
            "      - 200\n",
            "      - 131\n",
            "      - 49\n",
            "      - 152\n",
            "      - 115\n",
            "      - 118\n",
            "      - 104\n",
            "      - 49\n",
            "      - 200\n",
            "      - 158\n",
            "      - 175\n",
            "      - 113\n",
            "      - 104\n",
            "      - 194\n",
            "      - 98\n",
            "      - 149\n",
            "      - 85\n",
            "      - 25\n",
            "      - 200\n",
            "      - 181\n",
            "      - 200\n",
            "      - 98\n",
            "      - 135\n",
            "      - 185\n",
            "      - 118\n",
            "      - 200\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 35\n",
            "      - 142\n",
            "      - 153\n",
            "      - 93\n",
            "      - 105\n",
            "      - 101\n",
            "      - 144\n",
            "      - 191\n",
            "      - 137\n",
            "      - 170\n",
            "      - 180\n",
            "      - 200\n",
            "      - 151\n",
            "      - 75\n",
            "      - 200\n",
            "      - 72\n",
            "      - 121\n",
            "      - 164\n",
            "      - 200\n",
            "      - 132\n",
            "      - 200\n",
            "      - 200\n",
            "      - 129\n",
            "      - 200\n",
            "      - 70\n",
            "      - 200\n",
            "      - 140\n",
            "      - 159\n",
            "      - 126\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 136\n",
            "      - 90\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 109\n",
            "      - 200\n",
            "      - 181\n",
            "      - 153\n",
            "      - 200\n",
            "      - 107\n",
            "      - 65\n",
            "      - 139\n",
            "      - 108\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 16.0\n",
            "      - 178.0\n",
            "      - 112.0\n",
            "      - 125.0\n",
            "      - 148.0\n",
            "      - 95.0\n",
            "      - 155.0\n",
            "      - 116.0\n",
            "      - 75.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 97.0\n",
            "      - 123.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 49.0\n",
            "      - 152.0\n",
            "      - 115.0\n",
            "      - 118.0\n",
            "      - 104.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 175.0\n",
            "      - 113.0\n",
            "      - 104.0\n",
            "      - 194.0\n",
            "      - 98.0\n",
            "      - 149.0\n",
            "      - 85.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 185.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 35.0\n",
            "      - 142.0\n",
            "      - 153.0\n",
            "      - 93.0\n",
            "      - 105.0\n",
            "      - 101.0\n",
            "      - 144.0\n",
            "      - 191.0\n",
            "      - 137.0\n",
            "      - 170.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 151.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 72.0\n",
            "      - 121.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 129.0\n",
            "      - 200.0\n",
            "      - 70.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 90.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 65.0\n",
            "      - 139.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1400283334925435\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11015966059313576\n",
            "      mean_inference_ms: 1.3351741583651515\n",
            "      mean_raw_obs_processing_ms: 0.25983908418345414\n",
            "  time_since_restore: 140.01788330078125\n",
            "  time_this_iter_s: 10.073355197906494\n",
            "  time_total_s: 140.01788330078125\n",
            "  timers:\n",
            "    learn_throughput: 31216.561\n",
            "    learn_time_ms: 6.407\n",
            "    load_throughput: 1005466.619\n",
            "    load_time_ms: 0.199\n",
            "    training_iteration_time_ms: 214.263\n",
            "    update_time_ms: 3.572\n",
            "  timestamp: 1656954010\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 70200\n",
            "  training_iteration: 13\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:15 (running for 00:02:44.79)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         140.018</td><td style=\"text-align: right;\">70200</td><td style=\"text-align: right;\">  144.03</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            144.03</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:20 (running for 00:02:49.88)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         140.018</td><td style=\"text-align: right;\">70200</td><td style=\"text-align: right;\">  144.03</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            144.03</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 70800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 70800\n",
            "    num_agent_steps_trained: 70800\n",
            "    num_env_steps_sampled: 70800\n",
            "    num_env_steps_trained: 70800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-00-23\n",
            "  done: false\n",
            "  episode_len_mean: 144.44\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 144.44\n",
            "  episode_reward_min: 25.0\n",
            "  episodes_this_iter: 5\n",
            "  episodes_total: 1050\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 162.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 162.0\n",
            "    episode_reward_min: 56.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 133\n",
            "      - 200\n",
            "      - 173\n",
            "      - 200\n",
            "      - 108\n",
            "      - 200\n",
            "      - 134\n",
            "      - 200\n",
            "      - 98\n",
            "      - 200\n",
            "      - 87\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 151\n",
            "      - 200\n",
            "      - 159\n",
            "      - 200\n",
            "      - 56\n",
            "      episode_reward:\n",
            "      - 133.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 87.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10076803062706594\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07712645750703313\n",
            "      mean_inference_ms: 0.9822263317197146\n",
            "      mean_raw_obs_processing_ms: 0.10797893401561716\n",
            "    timesteps_this_iter: 3240\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 114.21595764160156\n",
            "          policy_loss: 1085.494873046875\n",
            "          var_gnorm: 22.93459701538086\n",
            "          vf_explained_var: -0.011240601539611816\n",
            "          vf_loss: 10592.666015625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 70800\n",
            "    num_agent_steps_trained: 70800\n",
            "    num_env_steps_sampled: 70800\n",
            "    num_env_steps_trained: 70800\n",
            "  iterations_since_restore: 14\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 70800\n",
            "  num_agent_steps_trained: 70800\n",
            "  num_env_steps_sampled: 70800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 70800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.10555555555555\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14012273672997305\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1101967665317575\n",
            "    mean_inference_ms: 1.3356276408636905\n",
            "    mean_raw_obs_processing_ms: 0.2597254663802536\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 144.44\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 144.44\n",
            "    episode_reward_min: 25.0\n",
            "    episodes_this_iter: 5\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 95\n",
            "      - 155\n",
            "      - 116\n",
            "      - 75\n",
            "      - 177\n",
            "      - 200\n",
            "      - 46\n",
            "      - 97\n",
            "      - 123\n",
            "      - 200\n",
            "      - 94\n",
            "      - 200\n",
            "      - 131\n",
            "      - 49\n",
            "      - 152\n",
            "      - 115\n",
            "      - 118\n",
            "      - 104\n",
            "      - 49\n",
            "      - 200\n",
            "      - 158\n",
            "      - 175\n",
            "      - 113\n",
            "      - 104\n",
            "      - 194\n",
            "      - 98\n",
            "      - 149\n",
            "      - 85\n",
            "      - 25\n",
            "      - 200\n",
            "      - 181\n",
            "      - 200\n",
            "      - 98\n",
            "      - 135\n",
            "      - 185\n",
            "      - 118\n",
            "      - 200\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 35\n",
            "      - 142\n",
            "      - 153\n",
            "      - 93\n",
            "      - 105\n",
            "      - 101\n",
            "      - 144\n",
            "      - 191\n",
            "      - 137\n",
            "      - 170\n",
            "      - 180\n",
            "      - 200\n",
            "      - 151\n",
            "      - 75\n",
            "      - 200\n",
            "      - 72\n",
            "      - 121\n",
            "      - 164\n",
            "      - 200\n",
            "      - 132\n",
            "      - 200\n",
            "      - 200\n",
            "      - 129\n",
            "      - 200\n",
            "      - 70\n",
            "      - 200\n",
            "      - 140\n",
            "      - 159\n",
            "      - 126\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 136\n",
            "      - 90\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 109\n",
            "      - 200\n",
            "      - 181\n",
            "      - 153\n",
            "      - 200\n",
            "      - 107\n",
            "      - 65\n",
            "      - 139\n",
            "      - 108\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 200\n",
            "      - 98\n",
            "      - 111\n",
            "      - 45\n",
            "      - 200\n",
            "      - 166\n",
            "      episode_reward:\n",
            "      - 95.0\n",
            "      - 155.0\n",
            "      - 116.0\n",
            "      - 75.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 97.0\n",
            "      - 123.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 49.0\n",
            "      - 152.0\n",
            "      - 115.0\n",
            "      - 118.0\n",
            "      - 104.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 175.0\n",
            "      - 113.0\n",
            "      - 104.0\n",
            "      - 194.0\n",
            "      - 98.0\n",
            "      - 149.0\n",
            "      - 85.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 185.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 35.0\n",
            "      - 142.0\n",
            "      - 153.0\n",
            "      - 93.0\n",
            "      - 105.0\n",
            "      - 101.0\n",
            "      - 144.0\n",
            "      - 191.0\n",
            "      - 137.0\n",
            "      - 170.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 151.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 72.0\n",
            "      - 121.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 129.0\n",
            "      - 200.0\n",
            "      - 70.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 90.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 65.0\n",
            "      - 139.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 111.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14012273672997305\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1101967665317575\n",
            "      mean_inference_ms: 1.3356276408636905\n",
            "      mean_raw_obs_processing_ms: 0.2597254663802536\n",
            "  time_since_restore: 152.82323598861694\n",
            "  time_this_iter_s: 12.805352687835693\n",
            "  time_total_s: 152.82323598861694\n",
            "  timers:\n",
            "    learn_throughput: 31750.736\n",
            "    learn_time_ms: 6.299\n",
            "    load_throughput: 1051204.01\n",
            "    load_time_ms: 0.19\n",
            "    training_iteration_time_ms: 213.604\n",
            "    update_time_ms: 3.674\n",
            "  timestamp: 1656954023\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 70800\n",
            "  training_iteration: 14\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:28 (running for 00:02:57.63)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         152.823</td><td style=\"text-align: right;\">70800</td><td style=\"text-align: right;\">  144.44</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">            144.44</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:33 (running for 00:03:02.74)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         152.823</td><td style=\"text-align: right;\">70800</td><td style=\"text-align: right;\">  144.44</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">            144.44</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 80200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 80200\n",
            "    num_agent_steps_trained: 80200\n",
            "    num_env_steps_sampled: 80200\n",
            "    num_env_steps_trained: 80200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-00-33\n",
            "  done: false\n",
            "  episode_len_mean: 155.69\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 155.69\n",
            "  episode_reward_min: 32.0\n",
            "  episodes_this_iter: 60\n",
            "  episodes_total: 1110\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 119.29917907714844\n",
            "          policy_loss: 1138.446044921875\n",
            "          var_gnorm: 23.0104923248291\n",
            "          vf_explained_var: -0.033014774322509766\n",
            "          vf_loss: 11381.6953125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 80200\n",
            "    num_agent_steps_trained: 80200\n",
            "    num_env_steps_sampled: 80200\n",
            "    num_env_steps_trained: 80200\n",
            "  iterations_since_restore: 15\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 80200\n",
            "  num_agent_steps_trained: 80200\n",
            "  num_env_steps_sampled: 80200\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 80200\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.63333333333334\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14076008945546634\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11038964979476658\n",
            "    mean_inference_ms: 1.3361575752385928\n",
            "    mean_raw_obs_processing_ms: 0.25763609425969397\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 155.69\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 155.69\n",
            "    episode_reward_min: 32.0\n",
            "    episodes_this_iter: 60\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 132\n",
            "      - 200\n",
            "      - 200\n",
            "      - 129\n",
            "      - 200\n",
            "      - 70\n",
            "      - 200\n",
            "      - 140\n",
            "      - 159\n",
            "      - 126\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 136\n",
            "      - 90\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 109\n",
            "      - 200\n",
            "      - 181\n",
            "      - 153\n",
            "      - 200\n",
            "      - 107\n",
            "      - 65\n",
            "      - 139\n",
            "      - 108\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 200\n",
            "      - 98\n",
            "      - 111\n",
            "      - 45\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 99\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 177\n",
            "      - 32\n",
            "      - 200\n",
            "      - 200\n",
            "      - 101\n",
            "      - 187\n",
            "      - 116\n",
            "      - 83\n",
            "      - 155\n",
            "      - 172\n",
            "      - 161\n",
            "      - 104\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 137\n",
            "      - 156\n",
            "      - 163\n",
            "      - 159\n",
            "      - 200\n",
            "      - 115\n",
            "      - 200\n",
            "      - 118\n",
            "      - 121\n",
            "      - 119\n",
            "      - 156\n",
            "      - 180\n",
            "      - 194\n",
            "      - 159\n",
            "      - 200\n",
            "      - 168\n",
            "      - 200\n",
            "      - 188\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 120\n",
            "      - 187\n",
            "      - 147\n",
            "      - 141\n",
            "      - 145\n",
            "      - 98\n",
            "      - 135\n",
            "      - 173\n",
            "      - 165\n",
            "      - 105\n",
            "      - 137\n",
            "      - 200\n",
            "      - 200\n",
            "      - 197\n",
            "      - 135\n",
            "      - 139\n",
            "      - 91\n",
            "      episode_reward:\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 129.0\n",
            "      - 200.0\n",
            "      - 70.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 90.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 65.0\n",
            "      - 139.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 111.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "      - 32.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 187.0\n",
            "      - 116.0\n",
            "      - 83.0\n",
            "      - 155.0\n",
            "      - 172.0\n",
            "      - 161.0\n",
            "      - 104.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 156.0\n",
            "      - 163.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 118.0\n",
            "      - 121.0\n",
            "      - 119.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 194.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 120.0\n",
            "      - 187.0\n",
            "      - 147.0\n",
            "      - 141.0\n",
            "      - 145.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 173.0\n",
            "      - 165.0\n",
            "      - 105.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 197.0\n",
            "      - 135.0\n",
            "      - 139.0\n",
            "      - 91.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14076008945546634\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11038964979476658\n",
            "      mean_inference_ms: 1.3361575752385928\n",
            "      mean_raw_obs_processing_ms: 0.25763609425969397\n",
            "  time_since_restore: 162.90375113487244\n",
            "  time_this_iter_s: 10.080515146255493\n",
            "  time_total_s: 162.90375113487244\n",
            "  timers:\n",
            "    learn_throughput: 32973.57\n",
            "    learn_time_ms: 6.065\n",
            "    load_throughput: 1060909.068\n",
            "    load_time_ms: 0.189\n",
            "    training_iteration_time_ms: 211.366\n",
            "    update_time_ms: 3.466\n",
            "  timestamp: 1656954033\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 80200\n",
            "  training_iteration: 15\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:38 (running for 00:03:07.79)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         162.904</td><td style=\"text-align: right;\">80200</td><td style=\"text-align: right;\">  155.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">            155.69</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:43 (running for 00:03:12.89)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         162.904</td><td style=\"text-align: right;\">80200</td><td style=\"text-align: right;\">  155.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">            155.69</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 80800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 80800\n",
            "    num_agent_steps_trained: 80800\n",
            "    num_env_steps_sampled: 80800\n",
            "    num_env_steps_trained: 80800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-00-45\n",
            "  done: false\n",
            "  episode_len_mean: 155.86\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 155.86\n",
            "  episode_reward_min: 32.0\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 1113\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 139.75\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 139.75\n",
            "    episode_reward_min: 41.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 103\n",
            "      - 91\n",
            "      - 109\n",
            "      - 72\n",
            "      - 162\n",
            "      - 178\n",
            "      - 200\n",
            "      - 200\n",
            "      - 144\n",
            "      - 113\n",
            "      - 200\n",
            "      - 196\n",
            "      - 184\n",
            "      - 41\n",
            "      - 136\n",
            "      - 123\n",
            "      - 200\n",
            "      - 124\n",
            "      - 43\n",
            "      - 176\n",
            "      episode_reward:\n",
            "      - 103.0\n",
            "      - 91.0\n",
            "      - 109.0\n",
            "      - 72.0\n",
            "      - 162.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 144.0\n",
            "      - 113.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 184.0\n",
            "      - 41.0\n",
            "      - 136.0\n",
            "      - 123.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 43.0\n",
            "      - 176.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10071410745605958\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.076883309025841\n",
            "      mean_inference_ms: 0.9796290054490088\n",
            "      mean_raw_obs_processing_ms: 0.10747017730535884\n",
            "    timesteps_this_iter: 2795\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 125.20777130126953\n",
            "          policy_loss: 1086.2490234375\n",
            "          var_gnorm: 23.0156307220459\n",
            "          vf_explained_var: 0.04883110523223877\n",
            "          vf_loss: 11280.2109375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 80800\n",
            "    num_agent_steps_trained: 80800\n",
            "    num_env_steps_sampled: 80800\n",
            "    num_env_steps_trained: 80800\n",
            "  iterations_since_restore: 16\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 80800\n",
            "  num_agent_steps_trained: 80800\n",
            "  num_env_steps_sampled: 80800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 80800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.18235294117646\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1407298552626872\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11039015645787195\n",
            "    mean_inference_ms: 1.3359342514144164\n",
            "    mean_raw_obs_processing_ms: 0.2576498505608365\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 155.86\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 155.86\n",
            "    episode_reward_min: 32.0\n",
            "    episodes_this_iter: 3\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 129\n",
            "      - 200\n",
            "      - 70\n",
            "      - 200\n",
            "      - 140\n",
            "      - 159\n",
            "      - 126\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 136\n",
            "      - 90\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 109\n",
            "      - 200\n",
            "      - 181\n",
            "      - 153\n",
            "      - 200\n",
            "      - 107\n",
            "      - 65\n",
            "      - 139\n",
            "      - 108\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 200\n",
            "      - 98\n",
            "      - 111\n",
            "      - 45\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 99\n",
            "      - 88\n",
            "      - 200\n",
            "      - 200\n",
            "      - 177\n",
            "      - 32\n",
            "      - 200\n",
            "      - 200\n",
            "      - 101\n",
            "      - 187\n",
            "      - 116\n",
            "      - 83\n",
            "      - 155\n",
            "      - 172\n",
            "      - 161\n",
            "      - 104\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 137\n",
            "      - 156\n",
            "      - 163\n",
            "      - 159\n",
            "      - 200\n",
            "      - 115\n",
            "      - 200\n",
            "      - 118\n",
            "      - 121\n",
            "      - 119\n",
            "      - 156\n",
            "      - 180\n",
            "      - 194\n",
            "      - 159\n",
            "      - 200\n",
            "      - 168\n",
            "      - 200\n",
            "      - 188\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 120\n",
            "      - 187\n",
            "      - 147\n",
            "      - 141\n",
            "      - 145\n",
            "      - 98\n",
            "      - 135\n",
            "      - 173\n",
            "      - 165\n",
            "      - 105\n",
            "      - 137\n",
            "      - 200\n",
            "      - 200\n",
            "      - 197\n",
            "      - 135\n",
            "      - 139\n",
            "      - 91\n",
            "      - 191\n",
            "      - 175\n",
            "      - 183\n",
            "      episode_reward:\n",
            "      - 129.0\n",
            "      - 200.0\n",
            "      - 70.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 90.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 65.0\n",
            "      - 139.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 111.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 88.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "      - 32.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 187.0\n",
            "      - 116.0\n",
            "      - 83.0\n",
            "      - 155.0\n",
            "      - 172.0\n",
            "      - 161.0\n",
            "      - 104.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 156.0\n",
            "      - 163.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 118.0\n",
            "      - 121.0\n",
            "      - 119.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 194.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 120.0\n",
            "      - 187.0\n",
            "      - 147.0\n",
            "      - 141.0\n",
            "      - 145.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 173.0\n",
            "      - 165.0\n",
            "      - 105.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 197.0\n",
            "      - 135.0\n",
            "      - 139.0\n",
            "      - 91.0\n",
            "      - 191.0\n",
            "      - 175.0\n",
            "      - 183.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1407298552626872\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11039015645787195\n",
            "      mean_inference_ms: 1.3359342514144164\n",
            "      mean_raw_obs_processing_ms: 0.2576498505608365\n",
            "  time_since_restore: 174.84628820419312\n",
            "  time_this_iter_s: 11.942537069320679\n",
            "  time_total_s: 174.84628820419312\n",
            "  timers:\n",
            "    learn_throughput: 32418.112\n",
            "    learn_time_ms: 6.169\n",
            "    load_throughput: 1023500.244\n",
            "    load_time_ms: 0.195\n",
            "    training_iteration_time_ms: 212.238\n",
            "    update_time_ms: 3.741\n",
            "  timestamp: 1656954045\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 80800\n",
            "  training_iteration: 16\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:50 (running for 00:03:19.77)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         174.846</td><td style=\"text-align: right;\">80800</td><td style=\"text-align: right;\">  155.86</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">            155.86</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:00:55 (running for 00:03:24.90)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         174.846</td><td style=\"text-align: right;\">80800</td><td style=\"text-align: right;\">  155.86</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">            155.86</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 88600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 88600\n",
            "    num_agent_steps_trained: 88600\n",
            "    num_env_steps_sampled: 88600\n",
            "    num_env_steps_trained: 88600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-00-55\n",
            "  done: false\n",
            "  episode_len_mean: 162.27\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 162.27\n",
            "  episode_reward_min: 53.0\n",
            "  episodes_this_iter: 47\n",
            "  episodes_total: 1160\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 120.58938598632812\n",
            "          policy_loss: 1015.89892578125\n",
            "          var_gnorm: 23.0939998626709\n",
            "          vf_explained_var: 0.03381258249282837\n",
            "          vf_loss: 10799.1337890625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 88600\n",
            "    num_agent_steps_trained: 88600\n",
            "    num_env_steps_sampled: 88600\n",
            "    num_env_steps_trained: 88600\n",
            "  iterations_since_restore: 17\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 88600\n",
            "  num_agent_steps_trained: 88600\n",
            "  num_env_steps_sampled: 88600\n",
            "  num_env_steps_sampled_this_iter: 7800\n",
            "  num_env_steps_trained: 88600\n",
            "  num_env_steps_trained_this_iter: 7800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.86428571428573\n",
            "    ram_util_percent: 21.535714285714285\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14110231637332352\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1105768436778872\n",
            "    mean_inference_ms: 1.3460386405781883\n",
            "    mean_raw_obs_processing_ms: 0.25695056892935575\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 162.27\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 162.27\n",
            "    episode_reward_min: 53.0\n",
            "    episodes_this_iter: 47\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 101\n",
            "      - 187\n",
            "      - 116\n",
            "      - 83\n",
            "      - 155\n",
            "      - 172\n",
            "      - 161\n",
            "      - 104\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 137\n",
            "      - 156\n",
            "      - 163\n",
            "      - 159\n",
            "      - 200\n",
            "      - 115\n",
            "      - 200\n",
            "      - 118\n",
            "      - 121\n",
            "      - 119\n",
            "      - 156\n",
            "      - 180\n",
            "      - 194\n",
            "      - 159\n",
            "      - 200\n",
            "      - 168\n",
            "      - 200\n",
            "      - 188\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 120\n",
            "      - 187\n",
            "      - 147\n",
            "      - 141\n",
            "      - 145\n",
            "      - 98\n",
            "      - 135\n",
            "      - 173\n",
            "      - 165\n",
            "      - 105\n",
            "      - 137\n",
            "      - 200\n",
            "      - 200\n",
            "      - 197\n",
            "      - 135\n",
            "      - 139\n",
            "      - 91\n",
            "      - 191\n",
            "      - 175\n",
            "      - 183\n",
            "      - 200\n",
            "      - 200\n",
            "      - 151\n",
            "      - 198\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 147\n",
            "      - 200\n",
            "      - 183\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 149\n",
            "      - 93\n",
            "      - 200\n",
            "      - 147\n",
            "      - 184\n",
            "      - 181\n",
            "      - 140\n",
            "      - 191\n",
            "      - 149\n",
            "      - 200\n",
            "      - 199\n",
            "      - 200\n",
            "      - 137\n",
            "      - 197\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 182\n",
            "      - 57\n",
            "      - 175\n",
            "      - 147\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 138\n",
            "      - 165\n",
            "      - 199\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 125\n",
            "      - 200\n",
            "      - 153\n",
            "      - 151\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 187.0\n",
            "      - 116.0\n",
            "      - 83.0\n",
            "      - 155.0\n",
            "      - 172.0\n",
            "      - 161.0\n",
            "      - 104.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 156.0\n",
            "      - 163.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 118.0\n",
            "      - 121.0\n",
            "      - 119.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 194.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 120.0\n",
            "      - 187.0\n",
            "      - 147.0\n",
            "      - 141.0\n",
            "      - 145.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 173.0\n",
            "      - 165.0\n",
            "      - 105.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 197.0\n",
            "      - 135.0\n",
            "      - 139.0\n",
            "      - 91.0\n",
            "      - 191.0\n",
            "      - 175.0\n",
            "      - 183.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 151.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 183.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 149.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 184.0\n",
            "      - 181.0\n",
            "      - 140.0\n",
            "      - 191.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 57.0\n",
            "      - 175.0\n",
            "      - 147.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 165.0\n",
            "      - 199.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 151.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14110231637332352\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1105768436778872\n",
            "      mean_inference_ms: 1.3460386405781883\n",
            "      mean_raw_obs_processing_ms: 0.25695056892935575\n",
            "  time_since_restore: 185.00793266296387\n",
            "  time_this_iter_s: 10.161644458770752\n",
            "  time_total_s: 185.00793266296387\n",
            "  timers:\n",
            "    learn_throughput: 24289.109\n",
            "    learn_time_ms: 8.234\n",
            "    load_throughput: 967544.175\n",
            "    load_time_ms: 0.207\n",
            "    training_iteration_time_ms: 220.351\n",
            "    update_time_ms: 4.512\n",
            "  timestamp: 1656954055\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 88600\n",
            "  training_iteration: 17\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:00 (running for 00:03:29.96)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         185.008</td><td style=\"text-align: right;\">88600</td><td style=\"text-align: right;\">  162.27</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  53</td><td style=\"text-align: right;\">            162.27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:05 (running for 00:03:35.04)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         185.008</td><td style=\"text-align: right;\">88600</td><td style=\"text-align: right;\">  162.27</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  53</td><td style=\"text-align: right;\">            162.27</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 89200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 89200\n",
            "    num_agent_steps_trained: 89200\n",
            "    num_env_steps_sampled: 89200\n",
            "    num_env_steps_trained: 89200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-01-07\n",
            "  done: false\n",
            "  episode_len_mean: 162.63\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 162.63\n",
            "  episode_reward_min: 53.0\n",
            "  episodes_this_iter: 5\n",
            "  episodes_total: 1165\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 150.15\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 150.15\n",
            "    episode_reward_min: 56.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 97\n",
            "      - 75\n",
            "      - 109\n",
            "      - 200\n",
            "      - 200\n",
            "      - 97\n",
            "      - 196\n",
            "      - 140\n",
            "      - 200\n",
            "      - 90\n",
            "      - 56\n",
            "      - 200\n",
            "      - 137\n",
            "      - 177\n",
            "      - 129\n",
            "      - 200\n",
            "      - 188\n",
            "      - 200\n",
            "      - 128\n",
            "      - 184\n",
            "      episode_reward:\n",
            "      - 97.0\n",
            "      - 75.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 97.0\n",
            "      - 196.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 56.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 177.0\n",
            "      - 129.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 184.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10054720381952413\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07684237861976095\n",
            "      mean_inference_ms: 0.978514686020475\n",
            "      mean_raw_obs_processing_ms: 0.10714174665448431\n",
            "    timesteps_this_iter: 3003\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.51974487304688\n",
            "          policy_loss: 1098.14208984375\n",
            "          var_gnorm: 23.100629806518555\n",
            "          vf_explained_var: 0.05627870559692383\n",
            "          vf_loss: 11500.3017578125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 89200\n",
            "    num_agent_steps_trained: 89200\n",
            "    num_env_steps_sampled: 89200\n",
            "    num_env_steps_trained: 89200\n",
            "  iterations_since_restore: 18\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 89200\n",
            "  num_agent_steps_trained: 89200\n",
            "  num_env_steps_sampled: 89200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 89200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.99411764705883\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14108182923531556\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.110621807116202\n",
            "    mean_inference_ms: 1.3469774604482259\n",
            "    mean_raw_obs_processing_ms: 0.257196588265311\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 162.63\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 162.63\n",
            "    episode_reward_min: 53.0\n",
            "    episodes_this_iter: 5\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 155\n",
            "      - 172\n",
            "      - 161\n",
            "      - 104\n",
            "      - 122\n",
            "      - 200\n",
            "      - 200\n",
            "      - 137\n",
            "      - 156\n",
            "      - 163\n",
            "      - 159\n",
            "      - 200\n",
            "      - 115\n",
            "      - 200\n",
            "      - 118\n",
            "      - 121\n",
            "      - 119\n",
            "      - 156\n",
            "      - 180\n",
            "      - 194\n",
            "      - 159\n",
            "      - 200\n",
            "      - 168\n",
            "      - 200\n",
            "      - 188\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 120\n",
            "      - 187\n",
            "      - 147\n",
            "      - 141\n",
            "      - 145\n",
            "      - 98\n",
            "      - 135\n",
            "      - 173\n",
            "      - 165\n",
            "      - 105\n",
            "      - 137\n",
            "      - 200\n",
            "      - 200\n",
            "      - 197\n",
            "      - 135\n",
            "      - 139\n",
            "      - 91\n",
            "      - 191\n",
            "      - 175\n",
            "      - 183\n",
            "      - 200\n",
            "      - 200\n",
            "      - 151\n",
            "      - 198\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 147\n",
            "      - 200\n",
            "      - 183\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 149\n",
            "      - 93\n",
            "      - 200\n",
            "      - 147\n",
            "      - 184\n",
            "      - 181\n",
            "      - 140\n",
            "      - 191\n",
            "      - 149\n",
            "      - 200\n",
            "      - 199\n",
            "      - 200\n",
            "      - 137\n",
            "      - 197\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 182\n",
            "      - 57\n",
            "      - 175\n",
            "      - 147\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 138\n",
            "      - 165\n",
            "      - 199\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 125\n",
            "      - 200\n",
            "      - 153\n",
            "      - 151\n",
            "      - 178\n",
            "      - 138\n",
            "      - 129\n",
            "      - 78\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 155.0\n",
            "      - 172.0\n",
            "      - 161.0\n",
            "      - 104.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 156.0\n",
            "      - 163.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 118.0\n",
            "      - 121.0\n",
            "      - 119.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 194.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 120.0\n",
            "      - 187.0\n",
            "      - 147.0\n",
            "      - 141.0\n",
            "      - 145.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 173.0\n",
            "      - 165.0\n",
            "      - 105.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 197.0\n",
            "      - 135.0\n",
            "      - 139.0\n",
            "      - 91.0\n",
            "      - 191.0\n",
            "      - 175.0\n",
            "      - 183.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 151.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 183.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 149.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 184.0\n",
            "      - 181.0\n",
            "      - 140.0\n",
            "      - 191.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 57.0\n",
            "      - 175.0\n",
            "      - 147.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 165.0\n",
            "      - 199.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 151.0\n",
            "      - 178.0\n",
            "      - 138.0\n",
            "      - 129.0\n",
            "      - 78.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14108182923531556\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.110621807116202\n",
            "      mean_inference_ms: 1.3469774604482259\n",
            "      mean_raw_obs_processing_ms: 0.257196588265311\n",
            "  time_since_restore: 196.7097098827362\n",
            "  time_this_iter_s: 11.701777219772339\n",
            "  time_total_s: 196.7097098827362\n",
            "  timers:\n",
            "    learn_throughput: 23208.597\n",
            "    learn_time_ms: 8.617\n",
            "    load_throughput: 977465.393\n",
            "    load_time_ms: 0.205\n",
            "    training_iteration_time_ms: 221.694\n",
            "    update_time_ms: 4.894\n",
            "  timestamp: 1656954067\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 89200\n",
            "  training_iteration: 18\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:12 (running for 00:03:41.71)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">          196.71</td><td style=\"text-align: right;\">89200</td><td style=\"text-align: right;\">  162.63</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  53</td><td style=\"text-align: right;\">            162.63</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:17 (running for 00:03:46.80)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">          196.71</td><td style=\"text-align: right;\">89200</td><td style=\"text-align: right;\">  162.63</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  53</td><td style=\"text-align: right;\">            162.63</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 99000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 99000\n",
            "    num_agent_steps_trained: 99000\n",
            "    num_env_steps_sampled: 99000\n",
            "    num_env_steps_trained: 99000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-01-17\n",
            "  done: false\n",
            "  episode_len_mean: 153.78\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 153.78\n",
            "  episode_reward_min: 18.0\n",
            "  episodes_this_iter: 65\n",
            "  episodes_total: 1230\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 124.9400634765625\n",
            "          policy_loss: 1074.3148193359375\n",
            "          var_gnorm: 23.206981658935547\n",
            "          vf_explained_var: 0.04783523082733154\n",
            "          vf_loss: 10959.9140625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 99000\n",
            "    num_agent_steps_trained: 99000\n",
            "    num_env_steps_sampled: 99000\n",
            "    num_env_steps_trained: 99000\n",
            "  iterations_since_restore: 19\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 99000\n",
            "  num_agent_steps_trained: 99000\n",
            "  num_env_steps_sampled: 99000\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 99000\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.82000000000001\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14092763216769266\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11051316796276449\n",
            "    mean_inference_ms: 1.3521333843234464\n",
            "    mean_raw_obs_processing_ms: 0.25642145186264437\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 153.78\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 153.78\n",
            "    episode_reward_min: 18.0\n",
            "    episodes_this_iter: 65\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 184\n",
            "      - 181\n",
            "      - 140\n",
            "      - 191\n",
            "      - 149\n",
            "      - 200\n",
            "      - 199\n",
            "      - 200\n",
            "      - 137\n",
            "      - 197\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 182\n",
            "      - 57\n",
            "      - 175\n",
            "      - 147\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 138\n",
            "      - 165\n",
            "      - 199\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 125\n",
            "      - 200\n",
            "      - 153\n",
            "      - 151\n",
            "      - 178\n",
            "      - 138\n",
            "      - 129\n",
            "      - 78\n",
            "      - 200\n",
            "      - 155\n",
            "      - 154\n",
            "      - 200\n",
            "      - 108\n",
            "      - 200\n",
            "      - 146\n",
            "      - 175\n",
            "      - 114\n",
            "      - 67\n",
            "      - 126\n",
            "      - 200\n",
            "      - 188\n",
            "      - 200\n",
            "      - 159\n",
            "      - 108\n",
            "      - 118\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 79\n",
            "      - 53\n",
            "      - 173\n",
            "      - 99\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 134\n",
            "      - 157\n",
            "      - 200\n",
            "      - 131\n",
            "      - 200\n",
            "      - 158\n",
            "      - 200\n",
            "      - 189\n",
            "      - 77\n",
            "      - 186\n",
            "      - 113\n",
            "      - 139\n",
            "      - 184\n",
            "      - 124\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 76\n",
            "      - 200\n",
            "      - 162\n",
            "      - 163\n",
            "      - 200\n",
            "      - 69\n",
            "      - 195\n",
            "      - 200\n",
            "      - 85\n",
            "      - 157\n",
            "      - 200\n",
            "      - 109\n",
            "      - 200\n",
            "      - 29\n",
            "      - 200\n",
            "      - 126\n",
            "      - 146\n",
            "      - 96\n",
            "      - 18\n",
            "      - 184\n",
            "      - 76\n",
            "      episode_reward:\n",
            "      - 184.0\n",
            "      - 181.0\n",
            "      - 140.0\n",
            "      - 191.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 57.0\n",
            "      - 175.0\n",
            "      - 147.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 165.0\n",
            "      - 199.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 151.0\n",
            "      - 178.0\n",
            "      - 138.0\n",
            "      - 129.0\n",
            "      - 78.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 146.0\n",
            "      - 175.0\n",
            "      - 114.0\n",
            "      - 67.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 159.0\n",
            "      - 108.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 79.0\n",
            "      - 53.0\n",
            "      - 173.0\n",
            "      - 99.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 134.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 77.0\n",
            "      - 186.0\n",
            "      - 113.0\n",
            "      - 139.0\n",
            "      - 184.0\n",
            "      - 124.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 85.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 29.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 146.0\n",
            "      - 96.0\n",
            "      - 18.0\n",
            "      - 184.0\n",
            "      - 76.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14092763216769266\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11051316796276449\n",
            "      mean_inference_ms: 1.3521333843234464\n",
            "      mean_raw_obs_processing_ms: 0.25642145186264437\n",
            "  time_since_restore: 206.92716932296753\n",
            "  time_this_iter_s: 10.217459440231323\n",
            "  time_total_s: 206.92716932296753\n",
            "  timers:\n",
            "    learn_throughput: 33389.754\n",
            "    learn_time_ms: 5.99\n",
            "    load_throughput: 1052522.961\n",
            "    load_time_ms: 0.19\n",
            "    training_iteration_time_ms: 209.291\n",
            "    update_time_ms: 3.538\n",
            "  timestamp: 1656954077\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 99000\n",
            "  training_iteration: 19\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:22 (running for 00:03:51.96)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         206.927</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">  153.78</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            153.78</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:27 (running for 00:03:57.05)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         206.927</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">  153.78</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            153.78</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 99600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 99600\n",
            "    num_agent_steps_trained: 99600\n",
            "    num_env_steps_sampled: 99600\n",
            "    num_env_steps_trained: 99600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-01-28\n",
            "  done: false\n",
            "  episode_len_mean: 154.23\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 154.23\n",
            "  episode_reward_min: 18.0\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 1233\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 161.85\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 161.85\n",
            "    episode_reward_min: 72.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 198\n",
            "      - 200\n",
            "      - 72\n",
            "      - 181\n",
            "      - 190\n",
            "      - 135\n",
            "      - 194\n",
            "      - 200\n",
            "      - 153\n",
            "      - 139\n",
            "      - 141\n",
            "      - 162\n",
            "      - 152\n",
            "      - 133\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 182\n",
            "      - 103\n",
            "      - 176\n",
            "      episode_reward:\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 72.0\n",
            "      - 181.0\n",
            "      - 190.0\n",
            "      - 135.0\n",
            "      - 194.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 139.0\n",
            "      - 141.0\n",
            "      - 162.0\n",
            "      - 152.0\n",
            "      - 133.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 103.0\n",
            "      - 176.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1004618927971737\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07677154379354806\n",
            "      mean_inference_ms: 0.9774866355560627\n",
            "      mean_raw_obs_processing_ms: 0.10689673782239772\n",
            "    timesteps_this_iter: 3237\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 122.75624084472656\n",
            "          policy_loss: 1049.6102294921875\n",
            "          var_gnorm: 23.214157104492188\n",
            "          vf_explained_var: 0.020319461822509766\n",
            "          vf_loss: 11013.669921875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 99600\n",
            "    num_agent_steps_trained: 99600\n",
            "    num_env_steps_sampled: 99600\n",
            "    num_env_steps_trained: 99600\n",
            "  iterations_since_restore: 20\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 99600\n",
            "  num_agent_steps_trained: 99600\n",
            "  num_env_steps_sampled: 99600\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 99600\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.08125000000001\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14086293077456222\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.11050019578768476\n",
            "    mean_inference_ms: 1.3516497559585126\n",
            "    mean_raw_obs_processing_ms: 0.2564806386570222\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 154.23\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 154.23\n",
            "    episode_reward_min: 18.0\n",
            "    episodes_this_iter: 3\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 191\n",
            "      - 149\n",
            "      - 200\n",
            "      - 199\n",
            "      - 200\n",
            "      - 137\n",
            "      - 197\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 182\n",
            "      - 57\n",
            "      - 175\n",
            "      - 147\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 138\n",
            "      - 165\n",
            "      - 199\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 125\n",
            "      - 200\n",
            "      - 153\n",
            "      - 151\n",
            "      - 178\n",
            "      - 138\n",
            "      - 129\n",
            "      - 78\n",
            "      - 200\n",
            "      - 155\n",
            "      - 154\n",
            "      - 200\n",
            "      - 108\n",
            "      - 200\n",
            "      - 146\n",
            "      - 175\n",
            "      - 114\n",
            "      - 67\n",
            "      - 126\n",
            "      - 200\n",
            "      - 188\n",
            "      - 200\n",
            "      - 159\n",
            "      - 108\n",
            "      - 118\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 79\n",
            "      - 53\n",
            "      - 173\n",
            "      - 99\n",
            "      - 121\n",
            "      - 200\n",
            "      - 200\n",
            "      - 134\n",
            "      - 157\n",
            "      - 200\n",
            "      - 131\n",
            "      - 200\n",
            "      - 158\n",
            "      - 200\n",
            "      - 189\n",
            "      - 77\n",
            "      - 186\n",
            "      - 113\n",
            "      - 139\n",
            "      - 184\n",
            "      - 124\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 76\n",
            "      - 200\n",
            "      - 162\n",
            "      - 163\n",
            "      - 200\n",
            "      - 69\n",
            "      - 195\n",
            "      - 200\n",
            "      - 85\n",
            "      - 157\n",
            "      - 200\n",
            "      - 109\n",
            "      - 200\n",
            "      - 29\n",
            "      - 200\n",
            "      - 126\n",
            "      - 146\n",
            "      - 96\n",
            "      - 18\n",
            "      - 184\n",
            "      - 76\n",
            "      - 200\n",
            "      - 163\n",
            "      - 187\n",
            "      episode_reward:\n",
            "      - 191.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 57.0\n",
            "      - 175.0\n",
            "      - 147.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 165.0\n",
            "      - 199.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 151.0\n",
            "      - 178.0\n",
            "      - 138.0\n",
            "      - 129.0\n",
            "      - 78.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 146.0\n",
            "      - 175.0\n",
            "      - 114.0\n",
            "      - 67.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 159.0\n",
            "      - 108.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 79.0\n",
            "      - 53.0\n",
            "      - 173.0\n",
            "      - 99.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 134.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 77.0\n",
            "      - 186.0\n",
            "      - 113.0\n",
            "      - 139.0\n",
            "      - 184.0\n",
            "      - 124.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 85.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 29.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 146.0\n",
            "      - 96.0\n",
            "      - 18.0\n",
            "      - 184.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 187.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14086293077456222\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.11050019578768476\n",
            "      mean_inference_ms: 1.3516497559585126\n",
            "      mean_raw_obs_processing_ms: 0.2564806386570222\n",
            "  time_since_restore: 218.3197226524353\n",
            "  time_this_iter_s: 11.392553329467773\n",
            "  time_total_s: 218.3197226524353\n",
            "  timers:\n",
            "    learn_throughput: 32666.301\n",
            "    learn_time_ms: 6.123\n",
            "    load_throughput: 1048313.922\n",
            "    load_time_ms: 0.191\n",
            "    training_iteration_time_ms: 208.688\n",
            "    update_time_ms: 3.265\n",
            "  timestamp: 1656954088\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 99600\n",
            "  training_iteration: 20\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:33 (running for 00:04:03.38)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">          218.32</td><td style=\"text-align: right;\">99600</td><td style=\"text-align: right;\">  154.23</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            154.23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:39 (running for 00:04:08.49)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">          218.32</td><td style=\"text-align: right;\">99600</td><td style=\"text-align: right;\">  154.23</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            154.23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 109400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 109400\n",
            "    num_agent_steps_trained: 109400\n",
            "    num_env_steps_sampled: 109400\n",
            "    num_env_steps_trained: 109400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-01-39\n",
            "  done: false\n",
            "  episode_len_mean: 144.02\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 144.02\n",
            "  episode_reward_min: 18.0\n",
            "  episodes_this_iter: 69\n",
            "  episodes_total: 1302\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.64170837402344\n",
            "          policy_loss: 1070.994140625\n",
            "          var_gnorm: 23.348081588745117\n",
            "          vf_explained_var: 0.04074901342391968\n",
            "          vf_loss: 11139.701171875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 109400\n",
            "    num_agent_steps_trained: 109400\n",
            "    num_env_steps_sampled: 109400\n",
            "    num_env_steps_trained: 109400\n",
            "  iterations_since_restore: 21\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 109400\n",
            "  num_agent_steps_trained: 109400\n",
            "  num_env_steps_sampled: 109400\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 109400\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.98000000000002\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1403817538030674\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10993089196893421\n",
            "    mean_inference_ms: 1.342669212761362\n",
            "    mean_raw_obs_processing_ms: 0.25460004807404035\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 144.02\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 144.02\n",
            "    episode_reward_min: 18.0\n",
            "    episodes_this_iter: 69\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 139\n",
            "      - 184\n",
            "      - 124\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 76\n",
            "      - 200\n",
            "      - 162\n",
            "      - 163\n",
            "      - 200\n",
            "      - 69\n",
            "      - 195\n",
            "      - 200\n",
            "      - 85\n",
            "      - 157\n",
            "      - 200\n",
            "      - 109\n",
            "      - 200\n",
            "      - 29\n",
            "      - 200\n",
            "      - 126\n",
            "      - 146\n",
            "      - 96\n",
            "      - 18\n",
            "      - 184\n",
            "      - 76\n",
            "      - 200\n",
            "      - 163\n",
            "      - 187\n",
            "      - 154\n",
            "      - 200\n",
            "      - 99\n",
            "      - 31\n",
            "      - 138\n",
            "      - 174\n",
            "      - 130\n",
            "      - 155\n",
            "      - 184\n",
            "      - 81\n",
            "      - 139\n",
            "      - 177\n",
            "      - 151\n",
            "      - 145\n",
            "      - 166\n",
            "      - 148\n",
            "      - 200\n",
            "      - 115\n",
            "      - 163\n",
            "      - 140\n",
            "      - 172\n",
            "      - 200\n",
            "      - 189\n",
            "      - 146\n",
            "      - 40\n",
            "      - 138\n",
            "      - 200\n",
            "      - 98\n",
            "      - 135\n",
            "      - 91\n",
            "      - 163\n",
            "      - 200\n",
            "      - 200\n",
            "      - 113\n",
            "      - 139\n",
            "      - 182\n",
            "      - 200\n",
            "      - 157\n",
            "      - 160\n",
            "      - 200\n",
            "      - 132\n",
            "      - 189\n",
            "      - 174\n",
            "      - 200\n",
            "      - 176\n",
            "      - 108\n",
            "      - 104\n",
            "      - 119\n",
            "      - 200\n",
            "      - 49\n",
            "      - 22\n",
            "      - 26\n",
            "      - 153\n",
            "      - 165\n",
            "      - 127\n",
            "      - 154\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 101\n",
            "      - 160\n",
            "      - 142\n",
            "      - 74\n",
            "      - 152\n",
            "      - 89\n",
            "      - 183\n",
            "      - 136\n",
            "      - 174\n",
            "      - 52\n",
            "      episode_reward:\n",
            "      - 139.0\n",
            "      - 184.0\n",
            "      - 124.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 85.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 29.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 146.0\n",
            "      - 96.0\n",
            "      - 18.0\n",
            "      - 184.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 187.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 31.0\n",
            "      - 138.0\n",
            "      - 174.0\n",
            "      - 130.0\n",
            "      - 155.0\n",
            "      - 184.0\n",
            "      - 81.0\n",
            "      - 139.0\n",
            "      - 177.0\n",
            "      - 151.0\n",
            "      - 145.0\n",
            "      - 166.0\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 163.0\n",
            "      - 140.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 146.0\n",
            "      - 40.0\n",
            "      - 138.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 91.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 139.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 160.0\n",
            "      - 200.0\n",
            "      - 132.0\n",
            "      - 189.0\n",
            "      - 174.0\n",
            "      - 200.0\n",
            "      - 176.0\n",
            "      - 108.0\n",
            "      - 104.0\n",
            "      - 119.0\n",
            "      - 200.0\n",
            "      - 49.0\n",
            "      - 22.0\n",
            "      - 26.0\n",
            "      - 153.0\n",
            "      - 165.0\n",
            "      - 127.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 160.0\n",
            "      - 142.0\n",
            "      - 74.0\n",
            "      - 152.0\n",
            "      - 89.0\n",
            "      - 183.0\n",
            "      - 136.0\n",
            "      - 174.0\n",
            "      - 52.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1403817538030674\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10993089196893421\n",
            "      mean_inference_ms: 1.342669212761362\n",
            "      mean_raw_obs_processing_ms: 0.25460004807404035\n",
            "  time_since_restore: 228.57372117042542\n",
            "  time_this_iter_s: 10.253998517990112\n",
            "  time_total_s: 228.57372117042542\n",
            "  timers:\n",
            "    learn_throughput: 32720.964\n",
            "    learn_time_ms: 6.112\n",
            "    load_throughput: 1041286.991\n",
            "    load_time_ms: 0.192\n",
            "    training_iteration_time_ms: 213.241\n",
            "    update_time_ms: 3.636\n",
            "  timestamp: 1656954099\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 109400\n",
            "  training_iteration: 21\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:44 (running for 00:04:13.67)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         228.574</td><td style=\"text-align: right;\">109400</td><td style=\"text-align: right;\">  144.02</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            144.02</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:49 (running for 00:04:18.77)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         228.574</td><td style=\"text-align: right;\">109400</td><td style=\"text-align: right;\">  144.02</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            144.02</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 110000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 110000\n",
            "    num_agent_steps_trained: 110000\n",
            "    num_env_steps_sampled: 110000\n",
            "    num_env_steps_trained: 110000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-01-49\n",
            "  done: false\n",
            "  episode_len_mean: 143.79\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 143.79\n",
            "  episode_reward_min: 18.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 1306\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 118.4\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 118.4\n",
            "    episode_reward_min: 42.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 149\n",
            "      - 109\n",
            "      - 127\n",
            "      - 58\n",
            "      - 113\n",
            "      - 120\n",
            "      - 163\n",
            "      - 42\n",
            "      - 192\n",
            "      - 53\n",
            "      - 146\n",
            "      - 153\n",
            "      - 104\n",
            "      - 66\n",
            "      - 152\n",
            "      - 51\n",
            "      - 92\n",
            "      - 106\n",
            "      - 172\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 109.0\n",
            "      - 127.0\n",
            "      - 58.0\n",
            "      - 113.0\n",
            "      - 120.0\n",
            "      - 163.0\n",
            "      - 42.0\n",
            "      - 192.0\n",
            "      - 53.0\n",
            "      - 146.0\n",
            "      - 153.0\n",
            "      - 104.0\n",
            "      - 66.0\n",
            "      - 152.0\n",
            "      - 51.0\n",
            "      - 92.0\n",
            "      - 106.0\n",
            "      - 172.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10026208875663396\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07660332253965668\n",
            "      mean_inference_ms: 0.9748573832559814\n",
            "      mean_raw_obs_processing_ms: 0.10659971628764069\n",
            "    timesteps_this_iter: 2368\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 125.60945892333984\n",
            "          policy_loss: 1007.672119140625\n",
            "          var_gnorm: 23.35710334777832\n",
            "          vf_explained_var: -0.026424765586853027\n",
            "          vf_loss: 10712.9619140625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 110000\n",
            "    num_agent_steps_trained: 110000\n",
            "    num_env_steps_sampled: 110000\n",
            "    num_env_steps_trained: 110000\n",
            "  iterations_since_restore: 22\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 110000\n",
            "  num_agent_steps_trained: 110000\n",
            "  num_env_steps_sampled: 110000\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 110000\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.19333333333334\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14038522448823873\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10990827730956251\n",
            "    mean_inference_ms: 1.3424288226274117\n",
            "    mean_raw_obs_processing_ms: 0.25446381462167555\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 143.79\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 143.79\n",
            "    episode_reward_min: 18.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 107\n",
            "      - 76\n",
            "      - 200\n",
            "      - 162\n",
            "      - 163\n",
            "      - 200\n",
            "      - 69\n",
            "      - 195\n",
            "      - 200\n",
            "      - 85\n",
            "      - 157\n",
            "      - 200\n",
            "      - 109\n",
            "      - 200\n",
            "      - 29\n",
            "      - 200\n",
            "      - 126\n",
            "      - 146\n",
            "      - 96\n",
            "      - 18\n",
            "      - 184\n",
            "      - 76\n",
            "      - 200\n",
            "      - 163\n",
            "      - 187\n",
            "      - 154\n",
            "      - 200\n",
            "      - 99\n",
            "      - 31\n",
            "      - 138\n",
            "      - 174\n",
            "      - 130\n",
            "      - 155\n",
            "      - 184\n",
            "      - 81\n",
            "      - 139\n",
            "      - 177\n",
            "      - 151\n",
            "      - 145\n",
            "      - 166\n",
            "      - 148\n",
            "      - 200\n",
            "      - 115\n",
            "      - 163\n",
            "      - 140\n",
            "      - 172\n",
            "      - 200\n",
            "      - 189\n",
            "      - 146\n",
            "      - 40\n",
            "      - 138\n",
            "      - 200\n",
            "      - 98\n",
            "      - 135\n",
            "      - 91\n",
            "      - 163\n",
            "      - 200\n",
            "      - 200\n",
            "      - 113\n",
            "      - 139\n",
            "      - 182\n",
            "      - 200\n",
            "      - 157\n",
            "      - 160\n",
            "      - 200\n",
            "      - 132\n",
            "      - 189\n",
            "      - 174\n",
            "      - 200\n",
            "      - 176\n",
            "      - 108\n",
            "      - 104\n",
            "      - 119\n",
            "      - 200\n",
            "      - 49\n",
            "      - 22\n",
            "      - 26\n",
            "      - 153\n",
            "      - 165\n",
            "      - 127\n",
            "      - 154\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 101\n",
            "      - 160\n",
            "      - 142\n",
            "      - 74\n",
            "      - 152\n",
            "      - 89\n",
            "      - 183\n",
            "      - 136\n",
            "      - 174\n",
            "      - 52\n",
            "      - 200\n",
            "      - 200\n",
            "      - 50\n",
            "      - 131\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 85.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 29.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 146.0\n",
            "      - 96.0\n",
            "      - 18.0\n",
            "      - 184.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 187.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 31.0\n",
            "      - 138.0\n",
            "      - 174.0\n",
            "      - 130.0\n",
            "      - 155.0\n",
            "      - 184.0\n",
            "      - 81.0\n",
            "      - 139.0\n",
            "      - 177.0\n",
            "      - 151.0\n",
            "      - 145.0\n",
            "      - 166.0\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 163.0\n",
            "      - 140.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 146.0\n",
            "      - 40.0\n",
            "      - 138.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 135.0\n",
            "      - 91.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 139.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 160.0\n",
            "      - 200.0\n",
            "      - 132.0\n",
            "      - 189.0\n",
            "      - 174.0\n",
            "      - 200.0\n",
            "      - 176.0\n",
            "      - 108.0\n",
            "      - 104.0\n",
            "      - 119.0\n",
            "      - 200.0\n",
            "      - 49.0\n",
            "      - 22.0\n",
            "      - 26.0\n",
            "      - 153.0\n",
            "      - 165.0\n",
            "      - 127.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 160.0\n",
            "      - 142.0\n",
            "      - 74.0\n",
            "      - 152.0\n",
            "      - 89.0\n",
            "      - 183.0\n",
            "      - 136.0\n",
            "      - 174.0\n",
            "      - 52.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 131.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14038522448823873\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10990827730956251\n",
            "      mean_inference_ms: 1.3424288226274117\n",
            "      mean_raw_obs_processing_ms: 0.25446381462167555\n",
            "  time_since_restore: 239.19769525527954\n",
            "  time_this_iter_s: 10.623974084854126\n",
            "  time_total_s: 239.19769525527954\n",
            "  timers:\n",
            "    learn_throughput: 33467.951\n",
            "    learn_time_ms: 5.976\n",
            "    load_throughput: 1078088.678\n",
            "    load_time_ms: 0.186\n",
            "    training_iteration_time_ms: 210.273\n",
            "    update_time_ms: 3.643\n",
            "  timestamp: 1656954109\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 110000\n",
            "  training_iteration: 22\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:54 (running for 00:04:24.33)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         239.198</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">  143.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            143.79</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:01:59 (running for 00:04:29.46)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         239.198</td><td style=\"text-align: right;\">110000</td><td style=\"text-align: right;\">  143.79</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">            143.79</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 119800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 119800\n",
            "    num_agent_steps_trained: 119800\n",
            "    num_env_steps_sampled: 119800\n",
            "    num_env_steps_trained: 119800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-02-00\n",
            "  done: false\n",
            "  episode_len_mean: 127.57\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 127.57\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 79\n",
            "  episodes_total: 1385\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 124.82044219970703\n",
            "          policy_loss: 1006.0278930664062\n",
            "          var_gnorm: 23.507156372070312\n",
            "          vf_explained_var: 0.09455519914627075\n",
            "          vf_loss: 10122.7216796875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 119800\n",
            "    num_agent_steps_trained: 119800\n",
            "    num_env_steps_sampled: 119800\n",
            "    num_env_steps_trained: 119800\n",
            "  iterations_since_restore: 23\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 119800\n",
            "  num_agent_steps_trained: 119800\n",
            "  num_env_steps_sampled: 119800\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 119800\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.4\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13996929069537753\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10943547961256847\n",
            "    mean_inference_ms: 1.335886727525098\n",
            "    mean_raw_obs_processing_ms: 0.2524046678360358\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 127.57\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 127.57\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 79\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 153\n",
            "      - 165\n",
            "      - 127\n",
            "      - 154\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 101\n",
            "      - 160\n",
            "      - 142\n",
            "      - 74\n",
            "      - 152\n",
            "      - 89\n",
            "      - 183\n",
            "      - 136\n",
            "      - 174\n",
            "      - 52\n",
            "      - 200\n",
            "      - 200\n",
            "      - 50\n",
            "      - 131\n",
            "      - 200\n",
            "      - 162\n",
            "      - 200\n",
            "      - 139\n",
            "      - 48\n",
            "      - 173\n",
            "      - 72\n",
            "      - 89\n",
            "      - 56\n",
            "      - 64\n",
            "      - 89\n",
            "      - 46\n",
            "      - 118\n",
            "      - 200\n",
            "      - 106\n",
            "      - 98\n",
            "      - 36\n",
            "      - 113\n",
            "      - 106\n",
            "      - 178\n",
            "      - 151\n",
            "      - 173\n",
            "      - 85\n",
            "      - 151\n",
            "      - 200\n",
            "      - 90\n",
            "      - 109\n",
            "      - 93\n",
            "      - 171\n",
            "      - 68\n",
            "      - 160\n",
            "      - 49\n",
            "      - 21\n",
            "      - 176\n",
            "      - 169\n",
            "      - 184\n",
            "      - 138\n",
            "      - 160\n",
            "      - 68\n",
            "      - 15\n",
            "      - 115\n",
            "      - 95\n",
            "      - 152\n",
            "      - 144\n",
            "      - 54\n",
            "      - 180\n",
            "      - 194\n",
            "      - 139\n",
            "      - 160\n",
            "      - 184\n",
            "      - 200\n",
            "      - 100\n",
            "      - 131\n",
            "      - 36\n",
            "      - 49\n",
            "      - 149\n",
            "      - 62\n",
            "      - 101\n",
            "      - 89\n",
            "      - 200\n",
            "      - 200\n",
            "      - 15\n",
            "      - 168\n",
            "      - 75\n",
            "      - 58\n",
            "      - 133\n",
            "      - 192\n",
            "      - 93\n",
            "      - 200\n",
            "      - 191\n",
            "      - 200\n",
            "      - 124\n",
            "      - 138\n",
            "      - 190\n",
            "      - 98\n",
            "      - 200\n",
            "      - 92\n",
            "      - 122\n",
            "      - 91\n",
            "      episode_reward:\n",
            "      - 153.0\n",
            "      - 165.0\n",
            "      - 127.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 160.0\n",
            "      - 142.0\n",
            "      - 74.0\n",
            "      - 152.0\n",
            "      - 89.0\n",
            "      - 183.0\n",
            "      - 136.0\n",
            "      - 174.0\n",
            "      - 52.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 131.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 48.0\n",
            "      - 173.0\n",
            "      - 72.0\n",
            "      - 89.0\n",
            "      - 56.0\n",
            "      - 64.0\n",
            "      - 89.0\n",
            "      - 46.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 106.0\n",
            "      - 98.0\n",
            "      - 36.0\n",
            "      - 113.0\n",
            "      - 106.0\n",
            "      - 178.0\n",
            "      - 151.0\n",
            "      - 173.0\n",
            "      - 85.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 109.0\n",
            "      - 93.0\n",
            "      - 171.0\n",
            "      - 68.0\n",
            "      - 160.0\n",
            "      - 49.0\n",
            "      - 21.0\n",
            "      - 176.0\n",
            "      - 169.0\n",
            "      - 184.0\n",
            "      - 138.0\n",
            "      - 160.0\n",
            "      - 68.0\n",
            "      - 15.0\n",
            "      - 115.0\n",
            "      - 95.0\n",
            "      - 152.0\n",
            "      - 144.0\n",
            "      - 54.0\n",
            "      - 180.0\n",
            "      - 194.0\n",
            "      - 139.0\n",
            "      - 160.0\n",
            "      - 184.0\n",
            "      - 200.0\n",
            "      - 100.0\n",
            "      - 131.0\n",
            "      - 36.0\n",
            "      - 49.0\n",
            "      - 149.0\n",
            "      - 62.0\n",
            "      - 101.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 168.0\n",
            "      - 75.0\n",
            "      - 58.0\n",
            "      - 133.0\n",
            "      - 192.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 191.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 138.0\n",
            "      - 190.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 92.0\n",
            "      - 122.0\n",
            "      - 91.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13996929069537753\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10943547961256847\n",
            "      mean_inference_ms: 1.335886727525098\n",
            "      mean_raw_obs_processing_ms: 0.2524046678360358\n",
            "  time_since_restore: 249.430073261261\n",
            "  time_this_iter_s: 10.232378005981445\n",
            "  time_total_s: 249.430073261261\n",
            "  timers:\n",
            "    learn_throughput: 33214.054\n",
            "    learn_time_ms: 6.022\n",
            "    load_throughput: 998524.938\n",
            "    load_time_ms: 0.2\n",
            "    training_iteration_time_ms: 217.645\n",
            "    update_time_ms: 3.956\n",
            "  timestamp: 1656954120\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 119800\n",
            "  training_iteration: 23\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:05 (running for 00:04:34.60)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          249.43</td><td style=\"text-align: right;\">119800</td><td style=\"text-align: right;\">  127.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            127.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:10 (running for 00:04:39.70)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">          249.43</td><td style=\"text-align: right;\">119800</td><td style=\"text-align: right;\">  127.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            127.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 120400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 120400\n",
            "    num_agent_steps_trained: 120400\n",
            "    num_env_steps_sampled: 120400\n",
            "    num_env_steps_trained: 120400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-02-10\n",
            "  done: false\n",
            "  episode_len_mean: 124.85\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 124.85\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 5\n",
            "  episodes_total: 1390\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 124.8\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 124.8\n",
            "    episode_reward_min: 34.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 86\n",
            "      - 160\n",
            "      - 48\n",
            "      - 113\n",
            "      - 81\n",
            "      - 34\n",
            "      - 83\n",
            "      - 69\n",
            "      - 197\n",
            "      - 139\n",
            "      - 164\n",
            "      - 101\n",
            "      - 135\n",
            "      - 177\n",
            "      - 86\n",
            "      - 200\n",
            "      - 79\n",
            "      - 186\n",
            "      - 158\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 86.0\n",
            "      - 160.0\n",
            "      - 48.0\n",
            "      - 113.0\n",
            "      - 81.0\n",
            "      - 34.0\n",
            "      - 83.0\n",
            "      - 69.0\n",
            "      - 197.0\n",
            "      - 139.0\n",
            "      - 164.0\n",
            "      - 101.0\n",
            "      - 135.0\n",
            "      - 177.0\n",
            "      - 86.0\n",
            "      - 200.0\n",
            "      - 79.0\n",
            "      - 186.0\n",
            "      - 158.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1001185045708503\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07649829353520522\n",
            "      mean_inference_ms: 0.9793631028476664\n",
            "      mean_raw_obs_processing_ms: 0.10643692357662782\n",
            "    timesteps_this_iter: 2496\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 119.96774291992188\n",
            "          policy_loss: 1066.8048095703125\n",
            "          var_gnorm: 23.516939163208008\n",
            "          vf_explained_var: 0.0632811188697815\n",
            "          vf_loss: 9980.52734375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 120400\n",
            "    num_agent_steps_trained: 120400\n",
            "    num_env_steps_sampled: 120400\n",
            "    num_env_steps_trained: 120400\n",
            "  iterations_since_restore: 24\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 120400\n",
            "  num_agent_steps_trained: 120400\n",
            "  num_env_steps_sampled: 120400\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 120400\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.45714285714287\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.14000025373417013\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10940504415287855\n",
            "    mean_inference_ms: 1.3358073712623157\n",
            "    mean_raw_obs_processing_ms: 0.25218817373337815\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 124.85\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 124.85\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 5\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 76\n",
            "      - 200\n",
            "      - 101\n",
            "      - 160\n",
            "      - 142\n",
            "      - 74\n",
            "      - 152\n",
            "      - 89\n",
            "      - 183\n",
            "      - 136\n",
            "      - 174\n",
            "      - 52\n",
            "      - 200\n",
            "      - 200\n",
            "      - 50\n",
            "      - 131\n",
            "      - 200\n",
            "      - 162\n",
            "      - 200\n",
            "      - 139\n",
            "      - 48\n",
            "      - 173\n",
            "      - 72\n",
            "      - 89\n",
            "      - 56\n",
            "      - 64\n",
            "      - 89\n",
            "      - 46\n",
            "      - 118\n",
            "      - 200\n",
            "      - 106\n",
            "      - 98\n",
            "      - 36\n",
            "      - 113\n",
            "      - 106\n",
            "      - 178\n",
            "      - 151\n",
            "      - 173\n",
            "      - 85\n",
            "      - 151\n",
            "      - 200\n",
            "      - 90\n",
            "      - 109\n",
            "      - 93\n",
            "      - 171\n",
            "      - 68\n",
            "      - 160\n",
            "      - 49\n",
            "      - 21\n",
            "      - 176\n",
            "      - 169\n",
            "      - 184\n",
            "      - 138\n",
            "      - 160\n",
            "      - 68\n",
            "      - 15\n",
            "      - 115\n",
            "      - 95\n",
            "      - 152\n",
            "      - 144\n",
            "      - 54\n",
            "      - 180\n",
            "      - 194\n",
            "      - 139\n",
            "      - 160\n",
            "      - 184\n",
            "      - 200\n",
            "      - 100\n",
            "      - 131\n",
            "      - 36\n",
            "      - 49\n",
            "      - 149\n",
            "      - 62\n",
            "      - 101\n",
            "      - 89\n",
            "      - 200\n",
            "      - 200\n",
            "      - 15\n",
            "      - 168\n",
            "      - 75\n",
            "      - 58\n",
            "      - 133\n",
            "      - 192\n",
            "      - 93\n",
            "      - 200\n",
            "      - 191\n",
            "      - 200\n",
            "      - 124\n",
            "      - 138\n",
            "      - 190\n",
            "      - 98\n",
            "      - 200\n",
            "      - 92\n",
            "      - 122\n",
            "      - 91\n",
            "      - 149\n",
            "      - 152\n",
            "      - 41\n",
            "      - 89\n",
            "      - 96\n",
            "      episode_reward:\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 160.0\n",
            "      - 142.0\n",
            "      - 74.0\n",
            "      - 152.0\n",
            "      - 89.0\n",
            "      - 183.0\n",
            "      - 136.0\n",
            "      - 174.0\n",
            "      - 52.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 131.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 48.0\n",
            "      - 173.0\n",
            "      - 72.0\n",
            "      - 89.0\n",
            "      - 56.0\n",
            "      - 64.0\n",
            "      - 89.0\n",
            "      - 46.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 106.0\n",
            "      - 98.0\n",
            "      - 36.0\n",
            "      - 113.0\n",
            "      - 106.0\n",
            "      - 178.0\n",
            "      - 151.0\n",
            "      - 173.0\n",
            "      - 85.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 109.0\n",
            "      - 93.0\n",
            "      - 171.0\n",
            "      - 68.0\n",
            "      - 160.0\n",
            "      - 49.0\n",
            "      - 21.0\n",
            "      - 176.0\n",
            "      - 169.0\n",
            "      - 184.0\n",
            "      - 138.0\n",
            "      - 160.0\n",
            "      - 68.0\n",
            "      - 15.0\n",
            "      - 115.0\n",
            "      - 95.0\n",
            "      - 152.0\n",
            "      - 144.0\n",
            "      - 54.0\n",
            "      - 180.0\n",
            "      - 194.0\n",
            "      - 139.0\n",
            "      - 160.0\n",
            "      - 184.0\n",
            "      - 200.0\n",
            "      - 100.0\n",
            "      - 131.0\n",
            "      - 36.0\n",
            "      - 49.0\n",
            "      - 149.0\n",
            "      - 62.0\n",
            "      - 101.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 168.0\n",
            "      - 75.0\n",
            "      - 58.0\n",
            "      - 133.0\n",
            "      - 192.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 191.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 138.0\n",
            "      - 190.0\n",
            "      - 98.0\n",
            "      - 200.0\n",
            "      - 92.0\n",
            "      - 122.0\n",
            "      - 91.0\n",
            "      - 149.0\n",
            "      - 152.0\n",
            "      - 41.0\n",
            "      - 89.0\n",
            "      - 96.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.14000025373417013\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10940504415287855\n",
            "      mean_inference_ms: 1.3358073712623157\n",
            "      mean_raw_obs_processing_ms: 0.25218817373337815\n",
            "  time_since_restore: 260.1705687046051\n",
            "  time_this_iter_s: 10.740495443344116\n",
            "  time_total_s: 260.1705687046051\n",
            "  timers:\n",
            "    learn_throughput: 31869.068\n",
            "    learn_time_ms: 6.276\n",
            "    load_throughput: 1012872.253\n",
            "    load_time_ms: 0.197\n",
            "    training_iteration_time_ms: 220.664\n",
            "    update_time_ms: 4.194\n",
            "  timestamp: 1656954130\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 120400\n",
            "  training_iteration: 24\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:15 (running for 00:04:45.37)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         260.171</td><td style=\"text-align: right;\">120400</td><td style=\"text-align: right;\">  124.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            124.85</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:20 (running for 00:04:50.45)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         260.171</td><td style=\"text-align: right;\">120400</td><td style=\"text-align: right;\">  124.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            124.85</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 130200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 130200\n",
            "    num_agent_steps_trained: 130200\n",
            "    num_env_steps_sampled: 130200\n",
            "    num_env_steps_trained: 130200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-02-21\n",
            "  done: false\n",
            "  episode_len_mean: 101.41\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 101.41\n",
            "  episode_reward_min: 19.0\n",
            "  episodes_this_iter: 97\n",
            "  episodes_total: 1487\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 129.0355224609375\n",
            "          policy_loss: 1023.576171875\n",
            "          var_gnorm: 23.68214988708496\n",
            "          vf_explained_var: 0.07083702087402344\n",
            "          vf_loss: 9741.2802734375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 130200\n",
            "    num_agent_steps_trained: 130200\n",
            "    num_env_steps_sampled: 130200\n",
            "    num_env_steps_trained: 130200\n",
            "  iterations_since_restore: 25\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 130200\n",
            "  num_agent_steps_trained: 130200\n",
            "  num_env_steps_sampled: 130200\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 130200\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.78666666666668\n",
            "    ram_util_percent: 21.5\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13957180548337472\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10914857576494018\n",
            "    mean_inference_ms: 1.3295799832878006\n",
            "    mean_raw_obs_processing_ms: 0.2508506518561774\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 101.41\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 101.41\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 97\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 41\n",
            "      - 89\n",
            "      - 96\n",
            "      - 88\n",
            "      - 88\n",
            "      - 178\n",
            "      - 102\n",
            "      - 39\n",
            "      - 93\n",
            "      - 98\n",
            "      - 53\n",
            "      - 123\n",
            "      - 185\n",
            "      - 101\n",
            "      - 146\n",
            "      - 200\n",
            "      - 174\n",
            "      - 151\n",
            "      - 33\n",
            "      - 165\n",
            "      - 59\n",
            "      - 48\n",
            "      - 86\n",
            "      - 88\n",
            "      - 123\n",
            "      - 19\n",
            "      - 20\n",
            "      - 50\n",
            "      - 22\n",
            "      - 43\n",
            "      - 146\n",
            "      - 153\n",
            "      - 70\n",
            "      - 34\n",
            "      - 138\n",
            "      - 67\n",
            "      - 66\n",
            "      - 84\n",
            "      - 200\n",
            "      - 200\n",
            "      - 100\n",
            "      - 102\n",
            "      - 79\n",
            "      - 147\n",
            "      - 104\n",
            "      - 112\n",
            "      - 150\n",
            "      - 66\n",
            "      - 97\n",
            "      - 51\n",
            "      - 68\n",
            "      - 166\n",
            "      - 148\n",
            "      - 100\n",
            "      - 116\n",
            "      - 36\n",
            "      - 200\n",
            "      - 46\n",
            "      - 159\n",
            "      - 116\n",
            "      - 79\n",
            "      - 70\n",
            "      - 126\n",
            "      - 155\n",
            "      - 84\n",
            "      - 57\n",
            "      - 29\n",
            "      - 68\n",
            "      - 26\n",
            "      - 71\n",
            "      - 200\n",
            "      - 127\n",
            "      - 88\n",
            "      - 88\n",
            "      - 138\n",
            "      - 89\n",
            "      - 35\n",
            "      - 105\n",
            "      - 73\n",
            "      - 81\n",
            "      - 95\n",
            "      - 200\n",
            "      - 99\n",
            "      - 36\n",
            "      - 31\n",
            "      - 120\n",
            "      - 94\n",
            "      - 119\n",
            "      - 104\n",
            "      - 40\n",
            "      - 152\n",
            "      - 156\n",
            "      - 121\n",
            "      - 69\n",
            "      - 145\n",
            "      - 152\n",
            "      - 20\n",
            "      - 77\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 41.0\n",
            "      - 89.0\n",
            "      - 96.0\n",
            "      - 88.0\n",
            "      - 88.0\n",
            "      - 178.0\n",
            "      - 102.0\n",
            "      - 39.0\n",
            "      - 93.0\n",
            "      - 98.0\n",
            "      - 53.0\n",
            "      - 123.0\n",
            "      - 185.0\n",
            "      - 101.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 174.0\n",
            "      - 151.0\n",
            "      - 33.0\n",
            "      - 165.0\n",
            "      - 59.0\n",
            "      - 48.0\n",
            "      - 86.0\n",
            "      - 88.0\n",
            "      - 123.0\n",
            "      - 19.0\n",
            "      - 20.0\n",
            "      - 50.0\n",
            "      - 22.0\n",
            "      - 43.0\n",
            "      - 146.0\n",
            "      - 153.0\n",
            "      - 70.0\n",
            "      - 34.0\n",
            "      - 138.0\n",
            "      - 67.0\n",
            "      - 66.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 100.0\n",
            "      - 102.0\n",
            "      - 79.0\n",
            "      - 147.0\n",
            "      - 104.0\n",
            "      - 112.0\n",
            "      - 150.0\n",
            "      - 66.0\n",
            "      - 97.0\n",
            "      - 51.0\n",
            "      - 68.0\n",
            "      - 166.0\n",
            "      - 148.0\n",
            "      - 100.0\n",
            "      - 116.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 159.0\n",
            "      - 116.0\n",
            "      - 79.0\n",
            "      - 70.0\n",
            "      - 126.0\n",
            "      - 155.0\n",
            "      - 84.0\n",
            "      - 57.0\n",
            "      - 29.0\n",
            "      - 68.0\n",
            "      - 26.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 127.0\n",
            "      - 88.0\n",
            "      - 88.0\n",
            "      - 138.0\n",
            "      - 89.0\n",
            "      - 35.0\n",
            "      - 105.0\n",
            "      - 73.0\n",
            "      - 81.0\n",
            "      - 95.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 36.0\n",
            "      - 31.0\n",
            "      - 120.0\n",
            "      - 94.0\n",
            "      - 119.0\n",
            "      - 104.0\n",
            "      - 40.0\n",
            "      - 152.0\n",
            "      - 156.0\n",
            "      - 121.0\n",
            "      - 69.0\n",
            "      - 145.0\n",
            "      - 152.0\n",
            "      - 20.0\n",
            "      - 77.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13957180548337472\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10914857576494018\n",
            "      mean_inference_ms: 1.3295799832878006\n",
            "      mean_raw_obs_processing_ms: 0.2508506518561774\n",
            "  time_since_restore: 270.42842841148376\n",
            "  time_this_iter_s: 10.257859706878662\n",
            "  time_total_s: 270.42842841148376\n",
            "  timers:\n",
            "    learn_throughput: 32643.674\n",
            "    learn_time_ms: 6.127\n",
            "    load_throughput: 1020015.564\n",
            "    load_time_ms: 0.196\n",
            "    training_iteration_time_ms: 218.762\n",
            "    update_time_ms: 3.529\n",
            "  timestamp: 1656954141\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 130200\n",
            "  training_iteration: 25\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:26 (running for 00:04:55.67)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         270.428</td><td style=\"text-align: right;\">130200</td><td style=\"text-align: right;\">  101.41</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            101.41</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:31 (running for 00:05:00.88)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         270.428</td><td style=\"text-align: right;\">130200</td><td style=\"text-align: right;\">  101.41</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            101.41</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 131000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 131000\n",
            "    num_agent_steps_trained: 131000\n",
            "    num_env_steps_sampled: 131000\n",
            "    num_env_steps_trained: 131000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-02-31\n",
            "  done: false\n",
            "  episode_len_mean: 102.77\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 102.77\n",
            "  episode_reward_min: 19.0\n",
            "  episodes_this_iter: 8\n",
            "  episodes_total: 1495\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 87.4\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 160.0\n",
            "    episode_reward_mean: 87.4\n",
            "    episode_reward_min: 23.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 78\n",
            "      - 23\n",
            "      - 156\n",
            "      - 27\n",
            "      - 94\n",
            "      - 64\n",
            "      - 123\n",
            "      - 44\n",
            "      - 156\n",
            "      - 33\n",
            "      - 31\n",
            "      - 114\n",
            "      - 77\n",
            "      - 39\n",
            "      - 160\n",
            "      - 131\n",
            "      - 156\n",
            "      - 140\n",
            "      - 53\n",
            "      - 49\n",
            "      episode_reward:\n",
            "      - 78.0\n",
            "      - 23.0\n",
            "      - 156.0\n",
            "      - 27.0\n",
            "      - 94.0\n",
            "      - 64.0\n",
            "      - 123.0\n",
            "      - 44.0\n",
            "      - 156.0\n",
            "      - 33.0\n",
            "      - 31.0\n",
            "      - 114.0\n",
            "      - 77.0\n",
            "      - 39.0\n",
            "      - 160.0\n",
            "      - 131.0\n",
            "      - 156.0\n",
            "      - 140.0\n",
            "      - 53.0\n",
            "      - 49.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10021143759830071\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07659388740931997\n",
            "      mean_inference_ms: 0.9804696973496757\n",
            "      mean_raw_obs_processing_ms: 0.10654467276409325\n",
            "    timesteps_this_iter: 1748\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 125.60884094238281\n",
            "          policy_loss: 881.6668701171875\n",
            "          var_gnorm: 23.69630241394043\n",
            "          vf_explained_var: -0.05362355709075928\n",
            "          vf_loss: 8757.8017578125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 131000\n",
            "    num_agent_steps_trained: 131000\n",
            "    num_env_steps_sampled: 131000\n",
            "    num_env_steps_trained: 131000\n",
            "  iterations_since_restore: 26\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 131000\n",
            "  num_agent_steps_trained: 131000\n",
            "  num_env_steps_sampled: 131000\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 131000\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 67.29333333333334\n",
            "    ram_util_percent: 21.506666666666668\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13952587928824556\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10915034479800308\n",
            "    mean_inference_ms: 1.3292498229353111\n",
            "    mean_raw_obs_processing_ms: 0.2508912924346053\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 102.77\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 102.77\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 8\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 93\n",
            "      - 98\n",
            "      - 53\n",
            "      - 123\n",
            "      - 185\n",
            "      - 101\n",
            "      - 146\n",
            "      - 200\n",
            "      - 174\n",
            "      - 151\n",
            "      - 33\n",
            "      - 165\n",
            "      - 59\n",
            "      - 48\n",
            "      - 86\n",
            "      - 88\n",
            "      - 123\n",
            "      - 19\n",
            "      - 20\n",
            "      - 50\n",
            "      - 22\n",
            "      - 43\n",
            "      - 146\n",
            "      - 153\n",
            "      - 70\n",
            "      - 34\n",
            "      - 138\n",
            "      - 67\n",
            "      - 66\n",
            "      - 84\n",
            "      - 200\n",
            "      - 200\n",
            "      - 100\n",
            "      - 102\n",
            "      - 79\n",
            "      - 147\n",
            "      - 104\n",
            "      - 112\n",
            "      - 150\n",
            "      - 66\n",
            "      - 97\n",
            "      - 51\n",
            "      - 68\n",
            "      - 166\n",
            "      - 148\n",
            "      - 100\n",
            "      - 116\n",
            "      - 36\n",
            "      - 200\n",
            "      - 46\n",
            "      - 159\n",
            "      - 116\n",
            "      - 79\n",
            "      - 70\n",
            "      - 126\n",
            "      - 155\n",
            "      - 84\n",
            "      - 57\n",
            "      - 29\n",
            "      - 68\n",
            "      - 26\n",
            "      - 71\n",
            "      - 200\n",
            "      - 127\n",
            "      - 88\n",
            "      - 88\n",
            "      - 138\n",
            "      - 89\n",
            "      - 35\n",
            "      - 105\n",
            "      - 73\n",
            "      - 81\n",
            "      - 95\n",
            "      - 200\n",
            "      - 99\n",
            "      - 36\n",
            "      - 31\n",
            "      - 120\n",
            "      - 94\n",
            "      - 119\n",
            "      - 104\n",
            "      - 40\n",
            "      - 152\n",
            "      - 156\n",
            "      - 121\n",
            "      - 69\n",
            "      - 145\n",
            "      - 152\n",
            "      - 20\n",
            "      - 77\n",
            "      - 200\n",
            "      - 200\n",
            "      - 177\n",
            "      - 52\n",
            "      - 108\n",
            "      - 33\n",
            "      - 161\n",
            "      - 45\n",
            "      - 200\n",
            "      - 81\n",
            "      episode_reward:\n",
            "      - 93.0\n",
            "      - 98.0\n",
            "      - 53.0\n",
            "      - 123.0\n",
            "      - 185.0\n",
            "      - 101.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 174.0\n",
            "      - 151.0\n",
            "      - 33.0\n",
            "      - 165.0\n",
            "      - 59.0\n",
            "      - 48.0\n",
            "      - 86.0\n",
            "      - 88.0\n",
            "      - 123.0\n",
            "      - 19.0\n",
            "      - 20.0\n",
            "      - 50.0\n",
            "      - 22.0\n",
            "      - 43.0\n",
            "      - 146.0\n",
            "      - 153.0\n",
            "      - 70.0\n",
            "      - 34.0\n",
            "      - 138.0\n",
            "      - 67.0\n",
            "      - 66.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 100.0\n",
            "      - 102.0\n",
            "      - 79.0\n",
            "      - 147.0\n",
            "      - 104.0\n",
            "      - 112.0\n",
            "      - 150.0\n",
            "      - 66.0\n",
            "      - 97.0\n",
            "      - 51.0\n",
            "      - 68.0\n",
            "      - 166.0\n",
            "      - 148.0\n",
            "      - 100.0\n",
            "      - 116.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 159.0\n",
            "      - 116.0\n",
            "      - 79.0\n",
            "      - 70.0\n",
            "      - 126.0\n",
            "      - 155.0\n",
            "      - 84.0\n",
            "      - 57.0\n",
            "      - 29.0\n",
            "      - 68.0\n",
            "      - 26.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 127.0\n",
            "      - 88.0\n",
            "      - 88.0\n",
            "      - 138.0\n",
            "      - 89.0\n",
            "      - 35.0\n",
            "      - 105.0\n",
            "      - 73.0\n",
            "      - 81.0\n",
            "      - 95.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 36.0\n",
            "      - 31.0\n",
            "      - 120.0\n",
            "      - 94.0\n",
            "      - 119.0\n",
            "      - 104.0\n",
            "      - 40.0\n",
            "      - 152.0\n",
            "      - 156.0\n",
            "      - 121.0\n",
            "      - 69.0\n",
            "      - 145.0\n",
            "      - 152.0\n",
            "      - 20.0\n",
            "      - 77.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "      - 52.0\n",
            "      - 108.0\n",
            "      - 33.0\n",
            "      - 161.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13952587928824556\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10915034479800308\n",
            "      mean_inference_ms: 1.3292498229353111\n",
            "      mean_raw_obs_processing_ms: 0.2508912924346053\n",
            "  time_since_restore: 281.1844279766083\n",
            "  time_this_iter_s: 10.755999565124512\n",
            "  time_total_s: 281.1844279766083\n",
            "  timers:\n",
            "    learn_throughput: 33159.044\n",
            "    learn_time_ms: 6.032\n",
            "    load_throughput: 1006794.047\n",
            "    load_time_ms: 0.199\n",
            "    training_iteration_time_ms: 218.826\n",
            "    update_time_ms: 3.647\n",
            "  timestamp: 1656954151\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 131000\n",
            "  training_iteration: 26\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:37 (running for 00:05:06.50)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         281.184</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">  102.77</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            102.77</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:42 (running for 00:05:11.58)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         281.184</td><td style=\"text-align: right;\">131000</td><td style=\"text-align: right;\">  102.77</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            102.77</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 139400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 139400\n",
            "    num_agent_steps_trained: 139400\n",
            "    num_env_steps_sampled: 139400\n",
            "    num_env_steps_trained: 139400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-02-42\n",
            "  done: false\n",
            "  episode_len_mean: 92.85\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 92.85\n",
            "  episode_reward_min: 18.0\n",
            "  episodes_this_iter: 91\n",
            "  episodes_total: 1586\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 129.5225830078125\n",
            "          policy_loss: 986.6500854492188\n",
            "          var_gnorm: 23.857112884521484\n",
            "          vf_explained_var: 0.051902830600738525\n",
            "          vf_loss: 9223.427734375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 139400\n",
            "    num_agent_steps_trained: 139400\n",
            "    num_env_steps_sampled: 139400\n",
            "    num_env_steps_trained: 139400\n",
            "  iterations_since_restore: 27\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 139400\n",
            "  num_agent_steps_trained: 139400\n",
            "  num_env_steps_sampled: 139400\n",
            "  num_env_steps_sampled_this_iter: 8400\n",
            "  num_env_steps_trained: 139400\n",
            "  num_env_steps_trained_this_iter: 8400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 97.00000000000003\n",
            "    ram_util_percent: 21.526666666666664\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13976665193525262\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10926535909913027\n",
            "    mean_inference_ms: 1.3365117088281653\n",
            "    mean_raw_obs_processing_ms: 0.25121857143178117\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 92.85\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 92.85\n",
            "    episode_reward_min: 18.0\n",
            "    episodes_this_iter: 91\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 177\n",
            "      - 52\n",
            "      - 108\n",
            "      - 33\n",
            "      - 161\n",
            "      - 45\n",
            "      - 200\n",
            "      - 81\n",
            "      - 67\n",
            "      - 44\n",
            "      - 122\n",
            "      - 67\n",
            "      - 19\n",
            "      - 56\n",
            "      - 108\n",
            "      - 170\n",
            "      - 82\n",
            "      - 71\n",
            "      - 97\n",
            "      - 134\n",
            "      - 197\n",
            "      - 88\n",
            "      - 103\n",
            "      - 21\n",
            "      - 109\n",
            "      - 123\n",
            "      - 83\n",
            "      - 37\n",
            "      - 131\n",
            "      - 190\n",
            "      - 62\n",
            "      - 86\n",
            "      - 140\n",
            "      - 87\n",
            "      - 114\n",
            "      - 32\n",
            "      - 149\n",
            "      - 72\n",
            "      - 118\n",
            "      - 31\n",
            "      - 75\n",
            "      - 56\n",
            "      - 49\n",
            "      - 95\n",
            "      - 190\n",
            "      - 41\n",
            "      - 24\n",
            "      - 133\n",
            "      - 27\n",
            "      - 143\n",
            "      - 58\n",
            "      - 157\n",
            "      - 196\n",
            "      - 31\n",
            "      - 87\n",
            "      - 22\n",
            "      - 92\n",
            "      - 92\n",
            "      - 73\n",
            "      - 49\n",
            "      - 170\n",
            "      - 90\n",
            "      - 37\n",
            "      - 92\n",
            "      - 78\n",
            "      - 104\n",
            "      - 69\n",
            "      - 18\n",
            "      - 101\n",
            "      - 197\n",
            "      - 154\n",
            "      - 53\n",
            "      - 84\n",
            "      - 37\n",
            "      - 105\n",
            "      - 50\n",
            "      - 66\n",
            "      - 76\n",
            "      - 66\n",
            "      - 78\n",
            "      - 52\n",
            "      - 128\n",
            "      - 153\n",
            "      - 107\n",
            "      - 124\n",
            "      - 82\n",
            "      - 91\n",
            "      - 153\n",
            "      - 60\n",
            "      - 37\n",
            "      - 132\n",
            "      - 121\n",
            "      - 72\n",
            "      - 36\n",
            "      - 23\n",
            "      - 119\n",
            "      - 176\n",
            "      - 54\n",
            "      - 83\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "      - 52.0\n",
            "      - 108.0\n",
            "      - 33.0\n",
            "      - 161.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 67.0\n",
            "      - 44.0\n",
            "      - 122.0\n",
            "      - 67.0\n",
            "      - 19.0\n",
            "      - 56.0\n",
            "      - 108.0\n",
            "      - 170.0\n",
            "      - 82.0\n",
            "      - 71.0\n",
            "      - 97.0\n",
            "      - 134.0\n",
            "      - 197.0\n",
            "      - 88.0\n",
            "      - 103.0\n",
            "      - 21.0\n",
            "      - 109.0\n",
            "      - 123.0\n",
            "      - 83.0\n",
            "      - 37.0\n",
            "      - 131.0\n",
            "      - 190.0\n",
            "      - 62.0\n",
            "      - 86.0\n",
            "      - 140.0\n",
            "      - 87.0\n",
            "      - 114.0\n",
            "      - 32.0\n",
            "      - 149.0\n",
            "      - 72.0\n",
            "      - 118.0\n",
            "      - 31.0\n",
            "      - 75.0\n",
            "      - 56.0\n",
            "      - 49.0\n",
            "      - 95.0\n",
            "      - 190.0\n",
            "      - 41.0\n",
            "      - 24.0\n",
            "      - 133.0\n",
            "      - 27.0\n",
            "      - 143.0\n",
            "      - 58.0\n",
            "      - 157.0\n",
            "      - 196.0\n",
            "      - 31.0\n",
            "      - 87.0\n",
            "      - 22.0\n",
            "      - 92.0\n",
            "      - 92.0\n",
            "      - 73.0\n",
            "      - 49.0\n",
            "      - 170.0\n",
            "      - 90.0\n",
            "      - 37.0\n",
            "      - 92.0\n",
            "      - 78.0\n",
            "      - 104.0\n",
            "      - 69.0\n",
            "      - 18.0\n",
            "      - 101.0\n",
            "      - 197.0\n",
            "      - 154.0\n",
            "      - 53.0\n",
            "      - 84.0\n",
            "      - 37.0\n",
            "      - 105.0\n",
            "      - 50.0\n",
            "      - 66.0\n",
            "      - 76.0\n",
            "      - 66.0\n",
            "      - 78.0\n",
            "      - 52.0\n",
            "      - 128.0\n",
            "      - 153.0\n",
            "      - 107.0\n",
            "      - 124.0\n",
            "      - 82.0\n",
            "      - 91.0\n",
            "      - 153.0\n",
            "      - 60.0\n",
            "      - 37.0\n",
            "      - 132.0\n",
            "      - 121.0\n",
            "      - 72.0\n",
            "      - 36.0\n",
            "      - 23.0\n",
            "      - 119.0\n",
            "      - 176.0\n",
            "      - 54.0\n",
            "      - 83.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13976665193525262\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10926535909913027\n",
            "      mean_inference_ms: 1.3365117088281653\n",
            "      mean_raw_obs_processing_ms: 0.25121857143178117\n",
            "  time_since_restore: 291.2571985721588\n",
            "  time_this_iter_s: 10.072770595550537\n",
            "  time_total_s: 291.2571985721588\n",
            "  timers:\n",
            "    learn_throughput: 33758.74\n",
            "    learn_time_ms: 5.924\n",
            "    load_throughput: 1073673.109\n",
            "    load_time_ms: 0.186\n",
            "    training_iteration_time_ms: 205.417\n",
            "    update_time_ms: 3.644\n",
            "  timestamp: 1656954162\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 139400\n",
            "  training_iteration: 27\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:47 (running for 00:05:16.62)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         291.257</td><td style=\"text-align: right;\">139400</td><td style=\"text-align: right;\">   92.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">             92.85</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:52 (running for 00:05:21.71)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         291.257</td><td style=\"text-align: right;\">139400</td><td style=\"text-align: right;\">   92.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">             92.85</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 140400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 140400\n",
            "    num_agent_steps_trained: 140400\n",
            "    num_env_steps_sampled: 140400\n",
            "    num_env_steps_trained: 140400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-02-53\n",
            "  done: false\n",
            "  episode_len_mean: 94.09\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 94.09\n",
            "  episode_reward_min: 18.0\n",
            "  episodes_this_iter: 9\n",
            "  episodes_total: 1595\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 91.9\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 197.0\n",
            "    episode_reward_mean: 91.9\n",
            "    episode_reward_min: 22.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 154\n",
            "      - 60\n",
            "      - 138\n",
            "      - 120\n",
            "      - 117\n",
            "      - 34\n",
            "      - 22\n",
            "      - 197\n",
            "      - 113\n",
            "      - 78\n",
            "      - 81\n",
            "      - 100\n",
            "      - 65\n",
            "      - 97\n",
            "      - 74\n",
            "      - 115\n",
            "      - 98\n",
            "      - 38\n",
            "      - 78\n",
            "      - 59\n",
            "      episode_reward:\n",
            "      - 154.0\n",
            "      - 60.0\n",
            "      - 138.0\n",
            "      - 120.0\n",
            "      - 117.0\n",
            "      - 34.0\n",
            "      - 22.0\n",
            "      - 197.0\n",
            "      - 113.0\n",
            "      - 78.0\n",
            "      - 81.0\n",
            "      - 100.0\n",
            "      - 65.0\n",
            "      - 97.0\n",
            "      - 74.0\n",
            "      - 115.0\n",
            "      - 98.0\n",
            "      - 38.0\n",
            "      - 78.0\n",
            "      - 59.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10019145299842629\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07646475700928053\n",
            "      mean_inference_ms: 0.9776778758367672\n",
            "      mean_raw_obs_processing_ms: 0.10647068320666812\n",
            "    timesteps_this_iter: 1838\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 124.17769622802734\n",
            "          policy_loss: 966.688232421875\n",
            "          var_gnorm: 23.87712860107422\n",
            "          vf_explained_var: -0.023780226707458496\n",
            "          vf_loss: 9572.017578125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 140400\n",
            "    num_agent_steps_trained: 140400\n",
            "    num_env_steps_sampled: 140400\n",
            "    num_env_steps_trained: 140400\n",
            "  iterations_since_restore: 28\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 140400\n",
            "  num_agent_steps_trained: 140400\n",
            "  num_env_steps_sampled: 140400\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 140400\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.65\n",
            "    ram_util_percent: 21.575000000000003\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13978738055133802\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10927743299684899\n",
            "    mean_inference_ms: 1.3372066423796656\n",
            "    mean_raw_obs_processing_ms: 0.25125060799606846\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 94.09\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 94.09\n",
            "    episode_reward_min: 18.0\n",
            "    episodes_this_iter: 9\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 67\n",
            "      - 44\n",
            "      - 122\n",
            "      - 67\n",
            "      - 19\n",
            "      - 56\n",
            "      - 108\n",
            "      - 170\n",
            "      - 82\n",
            "      - 71\n",
            "      - 97\n",
            "      - 134\n",
            "      - 197\n",
            "      - 88\n",
            "      - 103\n",
            "      - 21\n",
            "      - 109\n",
            "      - 123\n",
            "      - 83\n",
            "      - 37\n",
            "      - 131\n",
            "      - 190\n",
            "      - 62\n",
            "      - 86\n",
            "      - 140\n",
            "      - 87\n",
            "      - 114\n",
            "      - 32\n",
            "      - 149\n",
            "      - 72\n",
            "      - 118\n",
            "      - 31\n",
            "      - 75\n",
            "      - 56\n",
            "      - 49\n",
            "      - 95\n",
            "      - 190\n",
            "      - 41\n",
            "      - 24\n",
            "      - 133\n",
            "      - 27\n",
            "      - 143\n",
            "      - 58\n",
            "      - 157\n",
            "      - 196\n",
            "      - 31\n",
            "      - 87\n",
            "      - 22\n",
            "      - 92\n",
            "      - 92\n",
            "      - 73\n",
            "      - 49\n",
            "      - 170\n",
            "      - 90\n",
            "      - 37\n",
            "      - 92\n",
            "      - 78\n",
            "      - 104\n",
            "      - 69\n",
            "      - 18\n",
            "      - 101\n",
            "      - 197\n",
            "      - 154\n",
            "      - 53\n",
            "      - 84\n",
            "      - 37\n",
            "      - 105\n",
            "      - 50\n",
            "      - 66\n",
            "      - 76\n",
            "      - 66\n",
            "      - 78\n",
            "      - 52\n",
            "      - 128\n",
            "      - 153\n",
            "      - 107\n",
            "      - 124\n",
            "      - 82\n",
            "      - 91\n",
            "      - 153\n",
            "      - 60\n",
            "      - 37\n",
            "      - 132\n",
            "      - 121\n",
            "      - 72\n",
            "      - 36\n",
            "      - 23\n",
            "      - 119\n",
            "      - 176\n",
            "      - 54\n",
            "      - 83\n",
            "      - 182\n",
            "      - 170\n",
            "      - 200\n",
            "      - 94\n",
            "      - 98\n",
            "      - 86\n",
            "      - 170\n",
            "      - 25\n",
            "      - 156\n",
            "      episode_reward:\n",
            "      - 67.0\n",
            "      - 44.0\n",
            "      - 122.0\n",
            "      - 67.0\n",
            "      - 19.0\n",
            "      - 56.0\n",
            "      - 108.0\n",
            "      - 170.0\n",
            "      - 82.0\n",
            "      - 71.0\n",
            "      - 97.0\n",
            "      - 134.0\n",
            "      - 197.0\n",
            "      - 88.0\n",
            "      - 103.0\n",
            "      - 21.0\n",
            "      - 109.0\n",
            "      - 123.0\n",
            "      - 83.0\n",
            "      - 37.0\n",
            "      - 131.0\n",
            "      - 190.0\n",
            "      - 62.0\n",
            "      - 86.0\n",
            "      - 140.0\n",
            "      - 87.0\n",
            "      - 114.0\n",
            "      - 32.0\n",
            "      - 149.0\n",
            "      - 72.0\n",
            "      - 118.0\n",
            "      - 31.0\n",
            "      - 75.0\n",
            "      - 56.0\n",
            "      - 49.0\n",
            "      - 95.0\n",
            "      - 190.0\n",
            "      - 41.0\n",
            "      - 24.0\n",
            "      - 133.0\n",
            "      - 27.0\n",
            "      - 143.0\n",
            "      - 58.0\n",
            "      - 157.0\n",
            "      - 196.0\n",
            "      - 31.0\n",
            "      - 87.0\n",
            "      - 22.0\n",
            "      - 92.0\n",
            "      - 92.0\n",
            "      - 73.0\n",
            "      - 49.0\n",
            "      - 170.0\n",
            "      - 90.0\n",
            "      - 37.0\n",
            "      - 92.0\n",
            "      - 78.0\n",
            "      - 104.0\n",
            "      - 69.0\n",
            "      - 18.0\n",
            "      - 101.0\n",
            "      - 197.0\n",
            "      - 154.0\n",
            "      - 53.0\n",
            "      - 84.0\n",
            "      - 37.0\n",
            "      - 105.0\n",
            "      - 50.0\n",
            "      - 66.0\n",
            "      - 76.0\n",
            "      - 66.0\n",
            "      - 78.0\n",
            "      - 52.0\n",
            "      - 128.0\n",
            "      - 153.0\n",
            "      - 107.0\n",
            "      - 124.0\n",
            "      - 82.0\n",
            "      - 91.0\n",
            "      - 153.0\n",
            "      - 60.0\n",
            "      - 37.0\n",
            "      - 132.0\n",
            "      - 121.0\n",
            "      - 72.0\n",
            "      - 36.0\n",
            "      - 23.0\n",
            "      - 119.0\n",
            "      - 176.0\n",
            "      - 54.0\n",
            "      - 83.0\n",
            "      - 182.0\n",
            "      - 170.0\n",
            "      - 200.0\n",
            "      - 94.0\n",
            "      - 98.0\n",
            "      - 86.0\n",
            "      - 170.0\n",
            "      - 25.0\n",
            "      - 156.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13978738055133802\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10927743299684899\n",
            "      mean_inference_ms: 1.3372066423796656\n",
            "      mean_raw_obs_processing_ms: 0.25125060799606846\n",
            "  time_since_restore: 302.573126077652\n",
            "  time_this_iter_s: 11.315927505493164\n",
            "  time_total_s: 302.573126077652\n",
            "  timers:\n",
            "    learn_throughput: 33072.239\n",
            "    learn_time_ms: 6.047\n",
            "    load_throughput: 1055303.56\n",
            "    load_time_ms: 0.19\n",
            "    training_iteration_time_ms: 206.899\n",
            "    update_time_ms: 3.817\n",
            "  timestamp: 1656954173\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 140400\n",
            "  training_iteration: 28\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:02:58 (running for 00:05:27.98)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         302.573</td><td style=\"text-align: right;\">140400</td><td style=\"text-align: right;\">   94.09</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">             94.09</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:03 (running for 00:05:33.06)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         302.573</td><td style=\"text-align: right;\">140400</td><td style=\"text-align: right;\">   94.09</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  18</td><td style=\"text-align: right;\">             94.09</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 150000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 150000\n",
            "    num_agent_steps_trained: 150000\n",
            "    num_env_steps_sampled: 150000\n",
            "    num_env_steps_trained: 150000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-03-03\n",
            "  done: false\n",
            "  episode_len_mean: 83.83333333333333\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 197.0\n",
            "  episode_reward_mean: 83.83333333333333\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 114\n",
            "  episodes_total: 1709\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.12603759765625\n",
            "          policy_loss: 882.4016723632812\n",
            "          var_gnorm: 24.071563720703125\n",
            "          vf_explained_var: 0.09163790941238403\n",
            "          vf_loss: 8301.0126953125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 150000\n",
            "    num_agent_steps_trained: 150000\n",
            "    num_env_steps_sampled: 150000\n",
            "    num_env_steps_trained: 150000\n",
            "  iterations_since_restore: 29\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 150000\n",
            "  num_agent_steps_trained: 150000\n",
            "  num_env_steps_sampled: 150000\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 150000\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.10714285714288\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13958638070425783\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10894710752334005\n",
            "    mean_inference_ms: 1.3326597571753032\n",
            "    mean_raw_obs_processing_ms: 0.2504685453030312\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 83.83333333333333\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 197.0\n",
            "    episode_reward_mean: 83.83333333333333\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 114\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 86\n",
            "      - 151\n",
            "      - 84\n",
            "      - 197\n",
            "      - 70\n",
            "      - 35\n",
            "      - 103\n",
            "      - 122\n",
            "      - 49\n",
            "      - 71\n",
            "      - 132\n",
            "      - 86\n",
            "      - 83\n",
            "      - 31\n",
            "      - 87\n",
            "      - 88\n",
            "      - 76\n",
            "      - 35\n",
            "      - 62\n",
            "      - 120\n",
            "      - 128\n",
            "      - 26\n",
            "      - 102\n",
            "      - 37\n",
            "      - 51\n",
            "      - 143\n",
            "      - 63\n",
            "      - 24\n",
            "      - 25\n",
            "      - 29\n",
            "      - 71\n",
            "      - 44\n",
            "      - 33\n",
            "      - 118\n",
            "      - 39\n",
            "      - 103\n",
            "      - 81\n",
            "      - 35\n",
            "      - 95\n",
            "      - 104\n",
            "      - 68\n",
            "      - 34\n",
            "      - 110\n",
            "      - 145\n",
            "      - 82\n",
            "      - 25\n",
            "      - 23\n",
            "      - 153\n",
            "      - 38\n",
            "      - 150\n",
            "      - 21\n",
            "      - 93\n",
            "      - 129\n",
            "      - 38\n",
            "      - 58\n",
            "      - 113\n",
            "      - 48\n",
            "      - 171\n",
            "      - 30\n",
            "      - 130\n",
            "      - 40\n",
            "      - 157\n",
            "      - 62\n",
            "      - 41\n",
            "      - 52\n",
            "      - 19\n",
            "      - 57\n",
            "      - 131\n",
            "      - 163\n",
            "      - 88\n",
            "      - 13\n",
            "      - 163\n",
            "      - 51\n",
            "      - 122\n",
            "      - 91\n",
            "      - 72\n",
            "      - 70\n",
            "      - 16\n",
            "      - 40\n",
            "      - 13\n",
            "      - 44\n",
            "      - 124\n",
            "      - 105\n",
            "      - 161\n",
            "      - 62\n",
            "      - 177\n",
            "      - 89\n",
            "      - 126\n",
            "      - 72\n",
            "      - 23\n",
            "      - 55\n",
            "      - 124\n",
            "      - 141\n",
            "      - 85\n",
            "      - 143\n",
            "      - 127\n",
            "      - 129\n",
            "      - 54\n",
            "      - 54\n",
            "      - 69\n",
            "      - 137\n",
            "      - 141\n",
            "      - 26\n",
            "      - 109\n",
            "      - 75\n",
            "      - 37\n",
            "      - 37\n",
            "      - 23\n",
            "      - 122\n",
            "      - 108\n",
            "      - 118\n",
            "      - 175\n",
            "      - 173\n",
            "      - 73\n",
            "      episode_reward:\n",
            "      - 86.0\n",
            "      - 151.0\n",
            "      - 84.0\n",
            "      - 197.0\n",
            "      - 70.0\n",
            "      - 35.0\n",
            "      - 103.0\n",
            "      - 122.0\n",
            "      - 49.0\n",
            "      - 71.0\n",
            "      - 132.0\n",
            "      - 86.0\n",
            "      - 83.0\n",
            "      - 31.0\n",
            "      - 87.0\n",
            "      - 88.0\n",
            "      - 76.0\n",
            "      - 35.0\n",
            "      - 62.0\n",
            "      - 120.0\n",
            "      - 128.0\n",
            "      - 26.0\n",
            "      - 102.0\n",
            "      - 37.0\n",
            "      - 51.0\n",
            "      - 143.0\n",
            "      - 63.0\n",
            "      - 24.0\n",
            "      - 25.0\n",
            "      - 29.0\n",
            "      - 71.0\n",
            "      - 44.0\n",
            "      - 33.0\n",
            "      - 118.0\n",
            "      - 39.0\n",
            "      - 103.0\n",
            "      - 81.0\n",
            "      - 35.0\n",
            "      - 95.0\n",
            "      - 104.0\n",
            "      - 68.0\n",
            "      - 34.0\n",
            "      - 110.0\n",
            "      - 145.0\n",
            "      - 82.0\n",
            "      - 25.0\n",
            "      - 23.0\n",
            "      - 153.0\n",
            "      - 38.0\n",
            "      - 150.0\n",
            "      - 21.0\n",
            "      - 93.0\n",
            "      - 129.0\n",
            "      - 38.0\n",
            "      - 58.0\n",
            "      - 113.0\n",
            "      - 48.0\n",
            "      - 171.0\n",
            "      - 30.0\n",
            "      - 130.0\n",
            "      - 40.0\n",
            "      - 157.0\n",
            "      - 62.0\n",
            "      - 41.0\n",
            "      - 52.0\n",
            "      - 19.0\n",
            "      - 57.0\n",
            "      - 131.0\n",
            "      - 163.0\n",
            "      - 88.0\n",
            "      - 13.0\n",
            "      - 163.0\n",
            "      - 51.0\n",
            "      - 122.0\n",
            "      - 91.0\n",
            "      - 72.0\n",
            "      - 70.0\n",
            "      - 16.0\n",
            "      - 40.0\n",
            "      - 13.0\n",
            "      - 44.0\n",
            "      - 124.0\n",
            "      - 105.0\n",
            "      - 161.0\n",
            "      - 62.0\n",
            "      - 177.0\n",
            "      - 89.0\n",
            "      - 126.0\n",
            "      - 72.0\n",
            "      - 23.0\n",
            "      - 55.0\n",
            "      - 124.0\n",
            "      - 141.0\n",
            "      - 85.0\n",
            "      - 143.0\n",
            "      - 127.0\n",
            "      - 129.0\n",
            "      - 54.0\n",
            "      - 54.0\n",
            "      - 69.0\n",
            "      - 137.0\n",
            "      - 141.0\n",
            "      - 26.0\n",
            "      - 109.0\n",
            "      - 75.0\n",
            "      - 37.0\n",
            "      - 37.0\n",
            "      - 23.0\n",
            "      - 122.0\n",
            "      - 108.0\n",
            "      - 118.0\n",
            "      - 175.0\n",
            "      - 173.0\n",
            "      - 73.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13958638070425783\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10894710752334005\n",
            "      mean_inference_ms: 1.3326597571753032\n",
            "      mean_raw_obs_processing_ms: 0.2504685453030312\n",
            "  time_since_restore: 312.63721108436584\n",
            "  time_this_iter_s: 10.064085006713867\n",
            "  time_total_s: 312.63721108436584\n",
            "  timers:\n",
            "    learn_throughput: 31264.029\n",
            "    learn_time_ms: 6.397\n",
            "    load_throughput: 968549.59\n",
            "    load_time_ms: 0.206\n",
            "    training_iteration_time_ms: 209.901\n",
            "    update_time_ms: 3.669\n",
            "  timestamp: 1656954183\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 150000\n",
            "  training_iteration: 29\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:08 (running for 00:05:38.11)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         312.637</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\"> 83.8333</td><td style=\"text-align: right;\">                 197</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">           83.8333</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:13 (running for 00:05:43.20)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         312.637</td><td style=\"text-align: right;\">150000</td><td style=\"text-align: right;\"> 83.8333</td><td style=\"text-align: right;\">                 197</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">           83.8333</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 151000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 151000\n",
            "    num_agent_steps_trained: 151000\n",
            "    num_env_steps_sampled: 151000\n",
            "    num_env_steps_trained: 151000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-03-14\n",
            "  done: false\n",
            "  episode_len_mean: 82.32\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 177.0\n",
            "  episode_reward_mean: 82.32\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 14\n",
            "  episodes_total: 1723\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 75.75\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 151.0\n",
            "    episode_reward_mean: 75.75\n",
            "    episode_reward_min: 25.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 100\n",
            "      - 95\n",
            "      - 36\n",
            "      - 151\n",
            "      - 25\n",
            "      - 34\n",
            "      - 103\n",
            "      - 47\n",
            "      - 59\n",
            "      - 40\n",
            "      - 41\n",
            "      - 46\n",
            "      - 148\n",
            "      - 135\n",
            "      - 61\n",
            "      - 29\n",
            "      - 113\n",
            "      - 114\n",
            "      - 66\n",
            "      - 72\n",
            "      episode_reward:\n",
            "      - 100.0\n",
            "      - 95.0\n",
            "      - 36.0\n",
            "      - 151.0\n",
            "      - 25.0\n",
            "      - 34.0\n",
            "      - 103.0\n",
            "      - 47.0\n",
            "      - 59.0\n",
            "      - 40.0\n",
            "      - 41.0\n",
            "      - 46.0\n",
            "      - 148.0\n",
            "      - 135.0\n",
            "      - 61.0\n",
            "      - 29.0\n",
            "      - 113.0\n",
            "      - 114.0\n",
            "      - 66.0\n",
            "      - 72.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10003671650503296\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07640234851647135\n",
            "      mean_inference_ms: 0.976350907670193\n",
            "      mean_raw_obs_processing_ms: 0.10643173429429235\n",
            "    timesteps_this_iter: 1515\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 124.48714447021484\n",
            "          policy_loss: 681.9630126953125\n",
            "          var_gnorm: 24.0921573638916\n",
            "          vf_explained_var: 0.06403356790542603\n",
            "          vf_loss: 6681.607421875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 151000\n",
            "    num_agent_steps_trained: 151000\n",
            "    num_env_steps_sampled: 151000\n",
            "    num_env_steps_trained: 151000\n",
            "  iterations_since_restore: 30\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 151000\n",
            "  num_agent_steps_trained: 151000\n",
            "  num_env_steps_sampled: 151000\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 151000\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 67.625\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13937613072171384\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10896632676242762\n",
            "    mean_inference_ms: 1.3313584739563442\n",
            "    mean_raw_obs_processing_ms: 0.2509162660426088\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 82.32\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 177.0\n",
            "    episode_reward_mean: 82.32\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 14\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 25\n",
            "      - 29\n",
            "      - 71\n",
            "      - 44\n",
            "      - 33\n",
            "      - 118\n",
            "      - 39\n",
            "      - 103\n",
            "      - 81\n",
            "      - 35\n",
            "      - 95\n",
            "      - 104\n",
            "      - 68\n",
            "      - 34\n",
            "      - 110\n",
            "      - 145\n",
            "      - 82\n",
            "      - 25\n",
            "      - 23\n",
            "      - 153\n",
            "      - 38\n",
            "      - 150\n",
            "      - 21\n",
            "      - 93\n",
            "      - 129\n",
            "      - 38\n",
            "      - 58\n",
            "      - 113\n",
            "      - 48\n",
            "      - 171\n",
            "      - 30\n",
            "      - 130\n",
            "      - 40\n",
            "      - 157\n",
            "      - 62\n",
            "      - 41\n",
            "      - 52\n",
            "      - 19\n",
            "      - 57\n",
            "      - 131\n",
            "      - 163\n",
            "      - 88\n",
            "      - 13\n",
            "      - 163\n",
            "      - 51\n",
            "      - 122\n",
            "      - 91\n",
            "      - 72\n",
            "      - 70\n",
            "      - 16\n",
            "      - 40\n",
            "      - 13\n",
            "      - 44\n",
            "      - 124\n",
            "      - 105\n",
            "      - 161\n",
            "      - 62\n",
            "      - 177\n",
            "      - 89\n",
            "      - 126\n",
            "      - 72\n",
            "      - 23\n",
            "      - 55\n",
            "      - 124\n",
            "      - 141\n",
            "      - 85\n",
            "      - 143\n",
            "      - 127\n",
            "      - 129\n",
            "      - 54\n",
            "      - 54\n",
            "      - 69\n",
            "      - 137\n",
            "      - 141\n",
            "      - 26\n",
            "      - 109\n",
            "      - 75\n",
            "      - 37\n",
            "      - 37\n",
            "      - 23\n",
            "      - 122\n",
            "      - 108\n",
            "      - 118\n",
            "      - 175\n",
            "      - 173\n",
            "      - 73\n",
            "      - 30\n",
            "      - 19\n",
            "      - 88\n",
            "      - 88\n",
            "      - 91\n",
            "      - 105\n",
            "      - 55\n",
            "      - 19\n",
            "      - 84\n",
            "      - 71\n",
            "      - 45\n",
            "      - 61\n",
            "      - 106\n",
            "      - 155\n",
            "      episode_reward:\n",
            "      - 25.0\n",
            "      - 29.0\n",
            "      - 71.0\n",
            "      - 44.0\n",
            "      - 33.0\n",
            "      - 118.0\n",
            "      - 39.0\n",
            "      - 103.0\n",
            "      - 81.0\n",
            "      - 35.0\n",
            "      - 95.0\n",
            "      - 104.0\n",
            "      - 68.0\n",
            "      - 34.0\n",
            "      - 110.0\n",
            "      - 145.0\n",
            "      - 82.0\n",
            "      - 25.0\n",
            "      - 23.0\n",
            "      - 153.0\n",
            "      - 38.0\n",
            "      - 150.0\n",
            "      - 21.0\n",
            "      - 93.0\n",
            "      - 129.0\n",
            "      - 38.0\n",
            "      - 58.0\n",
            "      - 113.0\n",
            "      - 48.0\n",
            "      - 171.0\n",
            "      - 30.0\n",
            "      - 130.0\n",
            "      - 40.0\n",
            "      - 157.0\n",
            "      - 62.0\n",
            "      - 41.0\n",
            "      - 52.0\n",
            "      - 19.0\n",
            "      - 57.0\n",
            "      - 131.0\n",
            "      - 163.0\n",
            "      - 88.0\n",
            "      - 13.0\n",
            "      - 163.0\n",
            "      - 51.0\n",
            "      - 122.0\n",
            "      - 91.0\n",
            "      - 72.0\n",
            "      - 70.0\n",
            "      - 16.0\n",
            "      - 40.0\n",
            "      - 13.0\n",
            "      - 44.0\n",
            "      - 124.0\n",
            "      - 105.0\n",
            "      - 161.0\n",
            "      - 62.0\n",
            "      - 177.0\n",
            "      - 89.0\n",
            "      - 126.0\n",
            "      - 72.0\n",
            "      - 23.0\n",
            "      - 55.0\n",
            "      - 124.0\n",
            "      - 141.0\n",
            "      - 85.0\n",
            "      - 143.0\n",
            "      - 127.0\n",
            "      - 129.0\n",
            "      - 54.0\n",
            "      - 54.0\n",
            "      - 69.0\n",
            "      - 137.0\n",
            "      - 141.0\n",
            "      - 26.0\n",
            "      - 109.0\n",
            "      - 75.0\n",
            "      - 37.0\n",
            "      - 37.0\n",
            "      - 23.0\n",
            "      - 122.0\n",
            "      - 108.0\n",
            "      - 118.0\n",
            "      - 175.0\n",
            "      - 173.0\n",
            "      - 73.0\n",
            "      - 30.0\n",
            "      - 19.0\n",
            "      - 88.0\n",
            "      - 88.0\n",
            "      - 91.0\n",
            "      - 105.0\n",
            "      - 55.0\n",
            "      - 19.0\n",
            "      - 84.0\n",
            "      - 71.0\n",
            "      - 45.0\n",
            "      - 61.0\n",
            "      - 106.0\n",
            "      - 155.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13937613072171384\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10896632676242762\n",
            "      mean_inference_ms: 1.3313584739563442\n",
            "      mean_raw_obs_processing_ms: 0.2509162660426088\n",
            "  time_since_restore: 323.64543437957764\n",
            "  time_this_iter_s: 11.008223295211792\n",
            "  time_total_s: 323.64543437957764\n",
            "  timers:\n",
            "    learn_throughput: 32258.79\n",
            "    learn_time_ms: 6.2\n",
            "    load_throughput: 1019271.932\n",
            "    load_time_ms: 0.196\n",
            "    training_iteration_time_ms: 211.653\n",
            "    update_time_ms: 3.638\n",
            "  timestamp: 1656954194\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 151000\n",
            "  training_iteration: 30\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:19 (running for 00:05:49.15)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         323.645</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">   82.32</td><td style=\"text-align: right;\">                 177</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             82.32</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:24 (running for 00:05:54.27)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         323.645</td><td style=\"text-align: right;\">151000</td><td style=\"text-align: right;\">   82.32</td><td style=\"text-align: right;\">                 177</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             82.32</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 160800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 160800\n",
            "    num_agent_steps_trained: 160800\n",
            "    num_env_steps_sampled: 160800\n",
            "    num_env_steps_trained: 160800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-03-24\n",
            "  done: false\n",
            "  episode_len_mean: 79.59836065573771\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 79.59836065573771\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 122\n",
            "  episodes_total: 1845\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 131.7965087890625\n",
            "          policy_loss: 789.8558349609375\n",
            "          var_gnorm: 24.302820205688477\n",
            "          vf_explained_var: 0.034969329833984375\n",
            "          vf_loss: 8159.044921875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 160800\n",
            "    num_agent_steps_trained: 160800\n",
            "    num_env_steps_sampled: 160800\n",
            "    num_env_steps_trained: 160800\n",
            "  iterations_since_restore: 31\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 160800\n",
            "  num_agent_steps_trained: 160800\n",
            "  num_env_steps_sampled: 160800\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 160800\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.55333333333334\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13902011174879622\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10856685987629511\n",
            "    mean_inference_ms: 1.3274448881475365\n",
            "    mean_raw_obs_processing_ms: 0.24999326275432307\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 79.59836065573771\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 79.59836065573771\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 122\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 61\n",
            "      - 51\n",
            "      - 92\n",
            "      - 13\n",
            "      - 20\n",
            "      - 151\n",
            "      - 75\n",
            "      - 101\n",
            "      - 25\n",
            "      - 30\n",
            "      - 36\n",
            "      - 99\n",
            "      - 155\n",
            "      - 117\n",
            "      - 28\n",
            "      - 62\n",
            "      - 65\n",
            "      - 132\n",
            "      - 88\n",
            "      - 82\n",
            "      - 125\n",
            "      - 26\n",
            "      - 69\n",
            "      - 31\n",
            "      - 121\n",
            "      - 118\n",
            "      - 200\n",
            "      - 15\n",
            "      - 18\n",
            "      - 66\n",
            "      - 20\n",
            "      - 29\n",
            "      - 141\n",
            "      - 43\n",
            "      - 50\n",
            "      - 51\n",
            "      - 118\n",
            "      - 42\n",
            "      - 67\n",
            "      - 200\n",
            "      - 18\n",
            "      - 17\n",
            "      - 111\n",
            "      - 46\n",
            "      - 110\n",
            "      - 90\n",
            "      - 33\n",
            "      - 166\n",
            "      - 151\n",
            "      - 36\n",
            "      - 53\n",
            "      - 185\n",
            "      - 94\n",
            "      - 139\n",
            "      - 19\n",
            "      - 102\n",
            "      - 30\n",
            "      - 160\n",
            "      - 47\n",
            "      - 83\n",
            "      - 115\n",
            "      - 80\n",
            "      - 125\n",
            "      - 47\n",
            "      - 64\n",
            "      - 169\n",
            "      - 108\n",
            "      - 14\n",
            "      - 107\n",
            "      - 140\n",
            "      - 49\n",
            "      - 118\n",
            "      - 145\n",
            "      - 98\n",
            "      - 71\n",
            "      - 74\n",
            "      - 155\n",
            "      - 21\n",
            "      - 107\n",
            "      - 104\n",
            "      - 101\n",
            "      - 23\n",
            "      - 28\n",
            "      - 75\n",
            "      - 169\n",
            "      - 66\n",
            "      - 150\n",
            "      - 63\n",
            "      - 69\n",
            "      - 37\n",
            "      - 31\n",
            "      - 132\n",
            "      - 101\n",
            "      - 91\n",
            "      - 82\n",
            "      - 34\n",
            "      - 13\n",
            "      - 96\n",
            "      - 133\n",
            "      - 45\n",
            "      - 45\n",
            "      - 43\n",
            "      - 18\n",
            "      - 89\n",
            "      - 58\n",
            "      - 36\n",
            "      - 82\n",
            "      - 51\n",
            "      - 50\n",
            "      - 101\n",
            "      - 131\n",
            "      - 93\n",
            "      - 91\n",
            "      - 57\n",
            "      - 57\n",
            "      - 147\n",
            "      - 200\n",
            "      - 26\n",
            "      - 45\n",
            "      - 31\n",
            "      - 17\n",
            "      - 70\n",
            "      episode_reward:\n",
            "      - 61.0\n",
            "      - 51.0\n",
            "      - 92.0\n",
            "      - 13.0\n",
            "      - 20.0\n",
            "      - 151.0\n",
            "      - 75.0\n",
            "      - 101.0\n",
            "      - 25.0\n",
            "      - 30.0\n",
            "      - 36.0\n",
            "      - 99.0\n",
            "      - 155.0\n",
            "      - 117.0\n",
            "      - 28.0\n",
            "      - 62.0\n",
            "      - 65.0\n",
            "      - 132.0\n",
            "      - 88.0\n",
            "      - 82.0\n",
            "      - 125.0\n",
            "      - 26.0\n",
            "      - 69.0\n",
            "      - 31.0\n",
            "      - 121.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 18.0\n",
            "      - 66.0\n",
            "      - 20.0\n",
            "      - 29.0\n",
            "      - 141.0\n",
            "      - 43.0\n",
            "      - 50.0\n",
            "      - 51.0\n",
            "      - 118.0\n",
            "      - 42.0\n",
            "      - 67.0\n",
            "      - 200.0\n",
            "      - 18.0\n",
            "      - 17.0\n",
            "      - 111.0\n",
            "      - 46.0\n",
            "      - 110.0\n",
            "      - 90.0\n",
            "      - 33.0\n",
            "      - 166.0\n",
            "      - 151.0\n",
            "      - 36.0\n",
            "      - 53.0\n",
            "      - 185.0\n",
            "      - 94.0\n",
            "      - 139.0\n",
            "      - 19.0\n",
            "      - 102.0\n",
            "      - 30.0\n",
            "      - 160.0\n",
            "      - 47.0\n",
            "      - 83.0\n",
            "      - 115.0\n",
            "      - 80.0\n",
            "      - 125.0\n",
            "      - 47.0\n",
            "      - 64.0\n",
            "      - 169.0\n",
            "      - 108.0\n",
            "      - 14.0\n",
            "      - 107.0\n",
            "      - 140.0\n",
            "      - 49.0\n",
            "      - 118.0\n",
            "      - 145.0\n",
            "      - 98.0\n",
            "      - 71.0\n",
            "      - 74.0\n",
            "      - 155.0\n",
            "      - 21.0\n",
            "      - 107.0\n",
            "      - 104.0\n",
            "      - 101.0\n",
            "      - 23.0\n",
            "      - 28.0\n",
            "      - 75.0\n",
            "      - 169.0\n",
            "      - 66.0\n",
            "      - 150.0\n",
            "      - 63.0\n",
            "      - 69.0\n",
            "      - 37.0\n",
            "      - 31.0\n",
            "      - 132.0\n",
            "      - 101.0\n",
            "      - 91.0\n",
            "      - 82.0\n",
            "      - 34.0\n",
            "      - 13.0\n",
            "      - 96.0\n",
            "      - 133.0\n",
            "      - 45.0\n",
            "      - 45.0\n",
            "      - 43.0\n",
            "      - 18.0\n",
            "      - 89.0\n",
            "      - 58.0\n",
            "      - 36.0\n",
            "      - 82.0\n",
            "      - 51.0\n",
            "      - 50.0\n",
            "      - 101.0\n",
            "      - 131.0\n",
            "      - 93.0\n",
            "      - 91.0\n",
            "      - 57.0\n",
            "      - 57.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 26.0\n",
            "      - 45.0\n",
            "      - 31.0\n",
            "      - 17.0\n",
            "      - 70.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13902011174879622\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10856685987629511\n",
            "      mean_inference_ms: 1.3274448881475365\n",
            "      mean_raw_obs_processing_ms: 0.24999326275432307\n",
            "  time_since_restore: 333.87454771995544\n",
            "  time_this_iter_s: 10.229113340377808\n",
            "  time_total_s: 333.87454771995544\n",
            "  timers:\n",
            "    learn_throughput: 32233.379\n",
            "    learn_time_ms: 6.205\n",
            "    load_throughput: 1060372.646\n",
            "    load_time_ms: 0.189\n",
            "    training_iteration_time_ms: 208.316\n",
            "    update_time_ms: 3.758\n",
            "  timestamp: 1656954204\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 160800\n",
            "  training_iteration: 31\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:29 (running for 00:05:59.41)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         333.875</td><td style=\"text-align: right;\">160800</td><td style=\"text-align: right;\"> 79.5984</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">           79.5984</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:35 (running for 00:06:04.52)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         333.875</td><td style=\"text-align: right;\">160800</td><td style=\"text-align: right;\"> 79.5984</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">           79.5984</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 161800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 161800\n",
            "    num_agent_steps_trained: 161800\n",
            "    num_env_steps_sampled: 161800\n",
            "    num_env_steps_trained: 161800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-03-35\n",
            "  done: false\n",
            "  episode_len_mean: 76.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 76.25\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 22\n",
            "  episodes_total: 1867\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 86.7\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 86.7\n",
            "    episode_reward_min: 26.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 73\n",
            "      - 28\n",
            "      - 200\n",
            "      - 49\n",
            "      - 95\n",
            "      - 135\n",
            "      - 76\n",
            "      - 34\n",
            "      - 35\n",
            "      - 103\n",
            "      - 119\n",
            "      - 26\n",
            "      - 49\n",
            "      - 31\n",
            "      - 26\n",
            "      - 122\n",
            "      - 97\n",
            "      - 86\n",
            "      - 155\n",
            "      - 195\n",
            "      episode_reward:\n",
            "      - 73.0\n",
            "      - 28.0\n",
            "      - 200.0\n",
            "      - 49.0\n",
            "      - 95.0\n",
            "      - 135.0\n",
            "      - 76.0\n",
            "      - 34.0\n",
            "      - 35.0\n",
            "      - 103.0\n",
            "      - 119.0\n",
            "      - 26.0\n",
            "      - 49.0\n",
            "      - 31.0\n",
            "      - 26.0\n",
            "      - 122.0\n",
            "      - 97.0\n",
            "      - 86.0\n",
            "      - 155.0\n",
            "      - 195.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09995938042752853\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07633061729360342\n",
            "      mean_inference_ms: 0.9750088951059037\n",
            "      mean_raw_obs_processing_ms: 0.10641501624303924\n",
            "    timesteps_this_iter: 1734\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.26727294921875\n",
            "          policy_loss: 520.1435546875\n",
            "          var_gnorm: 24.324113845825195\n",
            "          vf_explained_var: -0.03586161136627197\n",
            "          vf_loss: 7157.7080078125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 161800\n",
            "    num_agent_steps_trained: 161800\n",
            "    num_env_steps_sampled: 161800\n",
            "    num_env_steps_trained: 161800\n",
            "  iterations_since_restore: 32\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 161800\n",
            "  num_agent_steps_trained: 161800\n",
            "  num_env_steps_sampled: 161800\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 161800\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.71333333333332\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1386197557572258\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10862774136617787\n",
            "    mean_inference_ms: 1.3252923461633923\n",
            "    mean_raw_obs_processing_ms: 0.25075733775534237\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 76.25\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 76.25\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 22\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 110\n",
            "      - 90\n",
            "      - 33\n",
            "      - 166\n",
            "      - 151\n",
            "      - 36\n",
            "      - 53\n",
            "      - 185\n",
            "      - 94\n",
            "      - 139\n",
            "      - 19\n",
            "      - 102\n",
            "      - 30\n",
            "      - 160\n",
            "      - 47\n",
            "      - 83\n",
            "      - 115\n",
            "      - 80\n",
            "      - 125\n",
            "      - 47\n",
            "      - 64\n",
            "      - 169\n",
            "      - 108\n",
            "      - 14\n",
            "      - 107\n",
            "      - 140\n",
            "      - 49\n",
            "      - 118\n",
            "      - 145\n",
            "      - 98\n",
            "      - 71\n",
            "      - 74\n",
            "      - 155\n",
            "      - 21\n",
            "      - 107\n",
            "      - 104\n",
            "      - 101\n",
            "      - 23\n",
            "      - 28\n",
            "      - 75\n",
            "      - 169\n",
            "      - 66\n",
            "      - 150\n",
            "      - 63\n",
            "      - 69\n",
            "      - 37\n",
            "      - 31\n",
            "      - 132\n",
            "      - 101\n",
            "      - 91\n",
            "      - 82\n",
            "      - 34\n",
            "      - 13\n",
            "      - 96\n",
            "      - 133\n",
            "      - 45\n",
            "      - 45\n",
            "      - 43\n",
            "      - 18\n",
            "      - 89\n",
            "      - 58\n",
            "      - 36\n",
            "      - 82\n",
            "      - 51\n",
            "      - 50\n",
            "      - 101\n",
            "      - 131\n",
            "      - 93\n",
            "      - 91\n",
            "      - 57\n",
            "      - 57\n",
            "      - 147\n",
            "      - 200\n",
            "      - 26\n",
            "      - 45\n",
            "      - 31\n",
            "      - 17\n",
            "      - 70\n",
            "      - 106\n",
            "      - 48\n",
            "      - 38\n",
            "      - 43\n",
            "      - 79\n",
            "      - 18\n",
            "      - 54\n",
            "      - 24\n",
            "      - 46\n",
            "      - 83\n",
            "      - 36\n",
            "      - 146\n",
            "      - 39\n",
            "      - 101\n",
            "      - 19\n",
            "      - 14\n",
            "      - 23\n",
            "      - 49\n",
            "      - 32\n",
            "      - 26\n",
            "      - 82\n",
            "      - 33\n",
            "      episode_reward:\n",
            "      - 110.0\n",
            "      - 90.0\n",
            "      - 33.0\n",
            "      - 166.0\n",
            "      - 151.0\n",
            "      - 36.0\n",
            "      - 53.0\n",
            "      - 185.0\n",
            "      - 94.0\n",
            "      - 139.0\n",
            "      - 19.0\n",
            "      - 102.0\n",
            "      - 30.0\n",
            "      - 160.0\n",
            "      - 47.0\n",
            "      - 83.0\n",
            "      - 115.0\n",
            "      - 80.0\n",
            "      - 125.0\n",
            "      - 47.0\n",
            "      - 64.0\n",
            "      - 169.0\n",
            "      - 108.0\n",
            "      - 14.0\n",
            "      - 107.0\n",
            "      - 140.0\n",
            "      - 49.0\n",
            "      - 118.0\n",
            "      - 145.0\n",
            "      - 98.0\n",
            "      - 71.0\n",
            "      - 74.0\n",
            "      - 155.0\n",
            "      - 21.0\n",
            "      - 107.0\n",
            "      - 104.0\n",
            "      - 101.0\n",
            "      - 23.0\n",
            "      - 28.0\n",
            "      - 75.0\n",
            "      - 169.0\n",
            "      - 66.0\n",
            "      - 150.0\n",
            "      - 63.0\n",
            "      - 69.0\n",
            "      - 37.0\n",
            "      - 31.0\n",
            "      - 132.0\n",
            "      - 101.0\n",
            "      - 91.0\n",
            "      - 82.0\n",
            "      - 34.0\n",
            "      - 13.0\n",
            "      - 96.0\n",
            "      - 133.0\n",
            "      - 45.0\n",
            "      - 45.0\n",
            "      - 43.0\n",
            "      - 18.0\n",
            "      - 89.0\n",
            "      - 58.0\n",
            "      - 36.0\n",
            "      - 82.0\n",
            "      - 51.0\n",
            "      - 50.0\n",
            "      - 101.0\n",
            "      - 131.0\n",
            "      - 93.0\n",
            "      - 91.0\n",
            "      - 57.0\n",
            "      - 57.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 26.0\n",
            "      - 45.0\n",
            "      - 31.0\n",
            "      - 17.0\n",
            "      - 70.0\n",
            "      - 106.0\n",
            "      - 48.0\n",
            "      - 38.0\n",
            "      - 43.0\n",
            "      - 79.0\n",
            "      - 18.0\n",
            "      - 54.0\n",
            "      - 24.0\n",
            "      - 46.0\n",
            "      - 83.0\n",
            "      - 36.0\n",
            "      - 146.0\n",
            "      - 39.0\n",
            "      - 101.0\n",
            "      - 19.0\n",
            "      - 14.0\n",
            "      - 23.0\n",
            "      - 49.0\n",
            "      - 32.0\n",
            "      - 26.0\n",
            "      - 82.0\n",
            "      - 33.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1386197557572258\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10862774136617787\n",
            "      mean_inference_ms: 1.3252923461633923\n",
            "      mean_raw_obs_processing_ms: 0.25075733775534237\n",
            "  time_since_restore: 344.5437343120575\n",
            "  time_this_iter_s: 10.66918659210205\n",
            "  time_total_s: 344.5437343120575\n",
            "  timers:\n",
            "    learn_throughput: 32641.895\n",
            "    learn_time_ms: 6.127\n",
            "    load_throughput: 1097410.78\n",
            "    load_time_ms: 0.182\n",
            "    training_iteration_time_ms: 215.673\n",
            "    update_time_ms: 3.762\n",
            "  timestamp: 1656954215\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 161800\n",
            "  training_iteration: 32\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:40 (running for 00:06:10.13)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         344.544</td><td style=\"text-align: right;\">161800</td><td style=\"text-align: right;\">   76.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             76.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:45 (running for 00:06:15.24)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         344.544</td><td style=\"text-align: right;\">161800</td><td style=\"text-align: right;\">   76.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             76.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 171400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 171400\n",
            "    num_agent_steps_trained: 171400\n",
            "    num_env_steps_sampled: 171400\n",
            "    num_env_steps_trained: 171400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-03-45\n",
            "  done: false\n",
            "  episode_len_mean: 75.30708661417323\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 75.30708661417323\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 127\n",
            "  episodes_total: 1994\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 129.10838317871094\n",
            "          policy_loss: 841.5570068359375\n",
            "          var_gnorm: 24.534578323364258\n",
            "          vf_explained_var: 0.10194140672683716\n",
            "          vf_loss: 8210.20703125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 171400\n",
            "    num_agent_steps_trained: 171400\n",
            "    num_env_steps_sampled: 171400\n",
            "    num_env_steps_trained: 171400\n",
            "  iterations_since_restore: 33\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 171400\n",
            "  num_agent_steps_trained: 171400\n",
            "  num_env_steps_sampled: 171400\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 171400\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.84\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.138850988308065\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10838088232811251\n",
            "    mean_inference_ms: 1.3252645832134011\n",
            "    mean_raw_obs_processing_ms: 0.24958050702157927\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 75.30708661417323\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 75.30708661417323\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 127\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 50\n",
            "      - 123\n",
            "      - 158\n",
            "      - 160\n",
            "      - 55\n",
            "      - 74\n",
            "      - 26\n",
            "      - 155\n",
            "      - 41\n",
            "      - 200\n",
            "      - 17\n",
            "      - 37\n",
            "      - 119\n",
            "      - 34\n",
            "      - 65\n",
            "      - 22\n",
            "      - 113\n",
            "      - 17\n",
            "      - 22\n",
            "      - 38\n",
            "      - 72\n",
            "      - 111\n",
            "      - 57\n",
            "      - 131\n",
            "      - 21\n",
            "      - 23\n",
            "      - 65\n",
            "      - 110\n",
            "      - 200\n",
            "      - 138\n",
            "      - 58\n",
            "      - 117\n",
            "      - 118\n",
            "      - 32\n",
            "      - 132\n",
            "      - 108\n",
            "      - 25\n",
            "      - 200\n",
            "      - 26\n",
            "      - 91\n",
            "      - 182\n",
            "      - 114\n",
            "      - 27\n",
            "      - 30\n",
            "      - 21\n",
            "      - 119\n",
            "      - 78\n",
            "      - 22\n",
            "      - 29\n",
            "      - 19\n",
            "      - 20\n",
            "      - 60\n",
            "      - 20\n",
            "      - 30\n",
            "      - 32\n",
            "      - 98\n",
            "      - 23\n",
            "      - 67\n",
            "      - 18\n",
            "      - 54\n",
            "      - 38\n",
            "      - 200\n",
            "      - 113\n",
            "      - 20\n",
            "      - 74\n",
            "      - 50\n",
            "      - 27\n",
            "      - 36\n",
            "      - 40\n",
            "      - 44\n",
            "      - 200\n",
            "      - 155\n",
            "      - 26\n",
            "      - 132\n",
            "      - 103\n",
            "      - 33\n",
            "      - 22\n",
            "      - 25\n",
            "      - 35\n",
            "      - 34\n",
            "      - 70\n",
            "      - 76\n",
            "      - 125\n",
            "      - 55\n",
            "      - 28\n",
            "      - 108\n",
            "      - 129\n",
            "      - 25\n",
            "      - 173\n",
            "      - 93\n",
            "      - 131\n",
            "      - 69\n",
            "      - 137\n",
            "      - 37\n",
            "      - 29\n",
            "      - 186\n",
            "      - 194\n",
            "      - 53\n",
            "      - 37\n",
            "      - 103\n",
            "      - 70\n",
            "      - 83\n",
            "      - 32\n",
            "      - 33\n",
            "      - 58\n",
            "      - 43\n",
            "      - 64\n",
            "      - 92\n",
            "      - 17\n",
            "      - 17\n",
            "      - 95\n",
            "      - 46\n",
            "      - 124\n",
            "      - 31\n",
            "      - 22\n",
            "      - 16\n",
            "      - 188\n",
            "      - 151\n",
            "      - 72\n",
            "      - 94\n",
            "      - 181\n",
            "      - 51\n",
            "      - 15\n",
            "      - 12\n",
            "      - 33\n",
            "      - 70\n",
            "      - 165\n",
            "      episode_reward:\n",
            "      - 50.0\n",
            "      - 123.0\n",
            "      - 158.0\n",
            "      - 160.0\n",
            "      - 55.0\n",
            "      - 74.0\n",
            "      - 26.0\n",
            "      - 155.0\n",
            "      - 41.0\n",
            "      - 200.0\n",
            "      - 17.0\n",
            "      - 37.0\n",
            "      - 119.0\n",
            "      - 34.0\n",
            "      - 65.0\n",
            "      - 22.0\n",
            "      - 113.0\n",
            "      - 17.0\n",
            "      - 22.0\n",
            "      - 38.0\n",
            "      - 72.0\n",
            "      - 111.0\n",
            "      - 57.0\n",
            "      - 131.0\n",
            "      - 21.0\n",
            "      - 23.0\n",
            "      - 65.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 58.0\n",
            "      - 117.0\n",
            "      - 118.0\n",
            "      - 32.0\n",
            "      - 132.0\n",
            "      - 108.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 26.0\n",
            "      - 91.0\n",
            "      - 182.0\n",
            "      - 114.0\n",
            "      - 27.0\n",
            "      - 30.0\n",
            "      - 21.0\n",
            "      - 119.0\n",
            "      - 78.0\n",
            "      - 22.0\n",
            "      - 29.0\n",
            "      - 19.0\n",
            "      - 20.0\n",
            "      - 60.0\n",
            "      - 20.0\n",
            "      - 30.0\n",
            "      - 32.0\n",
            "      - 98.0\n",
            "      - 23.0\n",
            "      - 67.0\n",
            "      - 18.0\n",
            "      - 54.0\n",
            "      - 38.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 20.0\n",
            "      - 74.0\n",
            "      - 50.0\n",
            "      - 27.0\n",
            "      - 36.0\n",
            "      - 40.0\n",
            "      - 44.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 26.0\n",
            "      - 132.0\n",
            "      - 103.0\n",
            "      - 33.0\n",
            "      - 22.0\n",
            "      - 25.0\n",
            "      - 35.0\n",
            "      - 34.0\n",
            "      - 70.0\n",
            "      - 76.0\n",
            "      - 125.0\n",
            "      - 55.0\n",
            "      - 28.0\n",
            "      - 108.0\n",
            "      - 129.0\n",
            "      - 25.0\n",
            "      - 173.0\n",
            "      - 93.0\n",
            "      - 131.0\n",
            "      - 69.0\n",
            "      - 137.0\n",
            "      - 37.0\n",
            "      - 29.0\n",
            "      - 186.0\n",
            "      - 194.0\n",
            "      - 53.0\n",
            "      - 37.0\n",
            "      - 103.0\n",
            "      - 70.0\n",
            "      - 83.0\n",
            "      - 32.0\n",
            "      - 33.0\n",
            "      - 58.0\n",
            "      - 43.0\n",
            "      - 64.0\n",
            "      - 92.0\n",
            "      - 17.0\n",
            "      - 17.0\n",
            "      - 95.0\n",
            "      - 46.0\n",
            "      - 124.0\n",
            "      - 31.0\n",
            "      - 22.0\n",
            "      - 16.0\n",
            "      - 188.0\n",
            "      - 151.0\n",
            "      - 72.0\n",
            "      - 94.0\n",
            "      - 181.0\n",
            "      - 51.0\n",
            "      - 15.0\n",
            "      - 12.0\n",
            "      - 33.0\n",
            "      - 70.0\n",
            "      - 165.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.138850988308065\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10838088232811251\n",
            "      mean_inference_ms: 1.3252645832134011\n",
            "      mean_raw_obs_processing_ms: 0.24958050702157927\n",
            "  time_since_restore: 354.768678188324\n",
            "  time_this_iter_s: 10.22494387626648\n",
            "  time_total_s: 354.768678188324\n",
            "  timers:\n",
            "    learn_throughput: 27574.875\n",
            "    learn_time_ms: 7.253\n",
            "    load_throughput: 1012261.132\n",
            "    load_time_ms: 0.198\n",
            "    training_iteration_time_ms: 222.112\n",
            "    update_time_ms: 4.149\n",
            "  timestamp: 1656954225\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 171400\n",
            "  training_iteration: 33\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:50 (running for 00:06:20.39)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         354.769</td><td style=\"text-align: right;\">171400</td><td style=\"text-align: right;\"> 75.3071</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">           75.3071</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:03:56 (running for 00:06:25.48)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         354.769</td><td style=\"text-align: right;\">171400</td><td style=\"text-align: right;\"> 75.3071</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">           75.3071</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 172600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 172600\n",
            "    num_agent_steps_trained: 172600\n",
            "    num_env_steps_sampled: 172600\n",
            "    num_env_steps_trained: 172600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-03-56\n",
            "  done: false\n",
            "  episode_len_mean: 68.01\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 68.01\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 21\n",
            "  episodes_total: 2015\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 67.35\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 67.35\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 100\n",
            "      - 40\n",
            "      - 75\n",
            "      - 200\n",
            "      - 15\n",
            "      - 122\n",
            "      - 26\n",
            "      - 28\n",
            "      - 69\n",
            "      - 79\n",
            "      - 110\n",
            "      - 22\n",
            "      - 34\n",
            "      - 68\n",
            "      - 90\n",
            "      - 36\n",
            "      - 55\n",
            "      - 51\n",
            "      - 82\n",
            "      - 45\n",
            "      episode_reward:\n",
            "      - 100.0\n",
            "      - 40.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 122.0\n",
            "      - 26.0\n",
            "      - 28.0\n",
            "      - 69.0\n",
            "      - 79.0\n",
            "      - 110.0\n",
            "      - 22.0\n",
            "      - 34.0\n",
            "      - 68.0\n",
            "      - 90.0\n",
            "      - 36.0\n",
            "      - 55.0\n",
            "      - 51.0\n",
            "      - 82.0\n",
            "      - 45.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.0997842537414166\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07620826745009876\n",
            "      mean_inference_ms: 0.9726724686296857\n",
            "      mean_raw_obs_processing_ms: 0.1064427315147864\n",
            "    timesteps_this_iter: 1347\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.6634750366211\n",
            "          policy_loss: 607.0740966796875\n",
            "          var_gnorm: 24.560779571533203\n",
            "          vf_explained_var: 0.08411818742752075\n",
            "          vf_loss: 7320.328125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 172600\n",
            "    num_agent_steps_trained: 172600\n",
            "    num_env_steps_sampled: 172600\n",
            "    num_env_steps_trained: 172600\n",
            "  iterations_since_restore: 34\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 172600\n",
            "  num_agent_steps_trained: 172600\n",
            "  num_env_steps_sampled: 172600\n",
            "  num_env_steps_sampled_this_iter: 1200\n",
            "  num_env_steps_trained: 172600\n",
            "  num_env_steps_trained_this_iter: 1200\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.60625\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1384122275356627\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1084556278515184\n",
            "    mean_inference_ms: 1.3228196215511288\n",
            "    mean_raw_obs_processing_ms: 0.2503239897301843\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 68.01\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 68.01\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 21\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 29\n",
            "      - 19\n",
            "      - 20\n",
            "      - 60\n",
            "      - 20\n",
            "      - 30\n",
            "      - 32\n",
            "      - 98\n",
            "      - 23\n",
            "      - 67\n",
            "      - 18\n",
            "      - 54\n",
            "      - 38\n",
            "      - 200\n",
            "      - 113\n",
            "      - 20\n",
            "      - 74\n",
            "      - 50\n",
            "      - 27\n",
            "      - 36\n",
            "      - 40\n",
            "      - 44\n",
            "      - 200\n",
            "      - 155\n",
            "      - 26\n",
            "      - 132\n",
            "      - 103\n",
            "      - 33\n",
            "      - 22\n",
            "      - 25\n",
            "      - 35\n",
            "      - 34\n",
            "      - 70\n",
            "      - 76\n",
            "      - 125\n",
            "      - 55\n",
            "      - 28\n",
            "      - 108\n",
            "      - 129\n",
            "      - 25\n",
            "      - 173\n",
            "      - 93\n",
            "      - 131\n",
            "      - 69\n",
            "      - 137\n",
            "      - 37\n",
            "      - 29\n",
            "      - 186\n",
            "      - 194\n",
            "      - 53\n",
            "      - 37\n",
            "      - 103\n",
            "      - 70\n",
            "      - 83\n",
            "      - 32\n",
            "      - 33\n",
            "      - 58\n",
            "      - 43\n",
            "      - 64\n",
            "      - 92\n",
            "      - 17\n",
            "      - 17\n",
            "      - 95\n",
            "      - 46\n",
            "      - 124\n",
            "      - 31\n",
            "      - 22\n",
            "      - 16\n",
            "      - 188\n",
            "      - 151\n",
            "      - 72\n",
            "      - 94\n",
            "      - 181\n",
            "      - 51\n",
            "      - 15\n",
            "      - 12\n",
            "      - 33\n",
            "      - 70\n",
            "      - 165\n",
            "      - 69\n",
            "      - 37\n",
            "      - 19\n",
            "      - 22\n",
            "      - 24\n",
            "      - 62\n",
            "      - 20\n",
            "      - 132\n",
            "      - 116\n",
            "      - 61\n",
            "      - 80\n",
            "      - 17\n",
            "      - 28\n",
            "      - 50\n",
            "      - 73\n",
            "      - 154\n",
            "      - 91\n",
            "      - 36\n",
            "      - 30\n",
            "      - 41\n",
            "      - 29\n",
            "      episode_reward:\n",
            "      - 29.0\n",
            "      - 19.0\n",
            "      - 20.0\n",
            "      - 60.0\n",
            "      - 20.0\n",
            "      - 30.0\n",
            "      - 32.0\n",
            "      - 98.0\n",
            "      - 23.0\n",
            "      - 67.0\n",
            "      - 18.0\n",
            "      - 54.0\n",
            "      - 38.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 20.0\n",
            "      - 74.0\n",
            "      - 50.0\n",
            "      - 27.0\n",
            "      - 36.0\n",
            "      - 40.0\n",
            "      - 44.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 26.0\n",
            "      - 132.0\n",
            "      - 103.0\n",
            "      - 33.0\n",
            "      - 22.0\n",
            "      - 25.0\n",
            "      - 35.0\n",
            "      - 34.0\n",
            "      - 70.0\n",
            "      - 76.0\n",
            "      - 125.0\n",
            "      - 55.0\n",
            "      - 28.0\n",
            "      - 108.0\n",
            "      - 129.0\n",
            "      - 25.0\n",
            "      - 173.0\n",
            "      - 93.0\n",
            "      - 131.0\n",
            "      - 69.0\n",
            "      - 137.0\n",
            "      - 37.0\n",
            "      - 29.0\n",
            "      - 186.0\n",
            "      - 194.0\n",
            "      - 53.0\n",
            "      - 37.0\n",
            "      - 103.0\n",
            "      - 70.0\n",
            "      - 83.0\n",
            "      - 32.0\n",
            "      - 33.0\n",
            "      - 58.0\n",
            "      - 43.0\n",
            "      - 64.0\n",
            "      - 92.0\n",
            "      - 17.0\n",
            "      - 17.0\n",
            "      - 95.0\n",
            "      - 46.0\n",
            "      - 124.0\n",
            "      - 31.0\n",
            "      - 22.0\n",
            "      - 16.0\n",
            "      - 188.0\n",
            "      - 151.0\n",
            "      - 72.0\n",
            "      - 94.0\n",
            "      - 181.0\n",
            "      - 51.0\n",
            "      - 15.0\n",
            "      - 12.0\n",
            "      - 33.0\n",
            "      - 70.0\n",
            "      - 165.0\n",
            "      - 69.0\n",
            "      - 37.0\n",
            "      - 19.0\n",
            "      - 22.0\n",
            "      - 24.0\n",
            "      - 62.0\n",
            "      - 20.0\n",
            "      - 132.0\n",
            "      - 116.0\n",
            "      - 61.0\n",
            "      - 80.0\n",
            "      - 17.0\n",
            "      - 28.0\n",
            "      - 50.0\n",
            "      - 73.0\n",
            "      - 154.0\n",
            "      - 91.0\n",
            "      - 36.0\n",
            "      - 30.0\n",
            "      - 41.0\n",
            "      - 29.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1384122275356627\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1084556278515184\n",
            "      mean_inference_ms: 1.3228196215511288\n",
            "      mean_raw_obs_processing_ms: 0.2503239897301843\n",
            "  time_since_restore: 365.8002655506134\n",
            "  time_this_iter_s: 11.031587362289429\n",
            "  time_total_s: 365.8002655506134\n",
            "  timers:\n",
            "    learn_throughput: 32093.412\n",
            "    learn_time_ms: 6.232\n",
            "    load_throughput: 1010067.188\n",
            "    load_time_ms: 0.198\n",
            "    training_iteration_time_ms: 214.964\n",
            "    update_time_ms: 3.875\n",
            "  timestamp: 1656954236\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 172600\n",
            "  training_iteration: 34\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:01 (running for 00:06:31.46)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">           365.8</td><td style=\"text-align: right;\">172600</td><td style=\"text-align: right;\">   68.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             68.01</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:07 (running for 00:06:36.57)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">           365.8</td><td style=\"text-align: right;\">172600</td><td style=\"text-align: right;\">   68.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             68.01</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 182200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 182200\n",
            "    num_agent_steps_trained: 182200\n",
            "    num_env_steps_sampled: 182200\n",
            "    num_env_steps_trained: 182200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-04-07\n",
            "  done: false\n",
            "  episode_len_mean: 70.69117647058823\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 70.69117647058823\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 136\n",
            "  episodes_total: 2151\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 128.40560913085938\n",
            "          policy_loss: 638.0521240234375\n",
            "          var_gnorm: 24.771400451660156\n",
            "          vf_explained_var: -0.0007132291793823242\n",
            "          vf_loss: 8597.6328125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 182200\n",
            "    num_agent_steps_trained: 182200\n",
            "    num_env_steps_sampled: 182200\n",
            "    num_env_steps_trained: 182200\n",
            "  iterations_since_restore: 35\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 182200\n",
            "  num_agent_steps_trained: 182200\n",
            "  num_env_steps_sampled: 182200\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 182200\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.13571428571429\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13865589997687422\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1082088192442604\n",
            "    mean_inference_ms: 1.3218167865493562\n",
            "    mean_raw_obs_processing_ms: 0.249278173777411\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 70.69117647058823\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 70.69117647058823\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 136\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 61\n",
            "      - 136\n",
            "      - 47\n",
            "      - 124\n",
            "      - 89\n",
            "      - 16\n",
            "      - 27\n",
            "      - 17\n",
            "      - 200\n",
            "      - 59\n",
            "      - 26\n",
            "      - 20\n",
            "      - 74\n",
            "      - 88\n",
            "      - 146\n",
            "      - 190\n",
            "      - 77\n",
            "      - 78\n",
            "      - 34\n",
            "      - 34\n",
            "      - 199\n",
            "      - 31\n",
            "      - 56\n",
            "      - 45\n",
            "      - 50\n",
            "      - 87\n",
            "      - 177\n",
            "      - 107\n",
            "      - 77\n",
            "      - 38\n",
            "      - 76\n",
            "      - 48\n",
            "      - 26\n",
            "      - 123\n",
            "      - 32\n",
            "      - 61\n",
            "      - 64\n",
            "      - 13\n",
            "      - 67\n",
            "      - 108\n",
            "      - 38\n",
            "      - 25\n",
            "      - 28\n",
            "      - 99\n",
            "      - 25\n",
            "      - 77\n",
            "      - 43\n",
            "      - 16\n",
            "      - 146\n",
            "      - 72\n",
            "      - 20\n",
            "      - 139\n",
            "      - 32\n",
            "      - 20\n",
            "      - 81\n",
            "      - 49\n",
            "      - 130\n",
            "      - 126\n",
            "      - 173\n",
            "      - 68\n",
            "      - 27\n",
            "      - 51\n",
            "      - 109\n",
            "      - 119\n",
            "      - 90\n",
            "      - 25\n",
            "      - 70\n",
            "      - 66\n",
            "      - 71\n",
            "      - 12\n",
            "      - 34\n",
            "      - 24\n",
            "      - 48\n",
            "      - 28\n",
            "      - 75\n",
            "      - 106\n",
            "      - 75\n",
            "      - 104\n",
            "      - 36\n",
            "      - 87\n",
            "      - 28\n",
            "      - 90\n",
            "      - 110\n",
            "      - 126\n",
            "      - 114\n",
            "      - 156\n",
            "      - 24\n",
            "      - 59\n",
            "      - 26\n",
            "      - 28\n",
            "      - 51\n",
            "      - 12\n",
            "      - 31\n",
            "      - 34\n",
            "      - 33\n",
            "      - 133\n",
            "      - 19\n",
            "      - 157\n",
            "      - 17\n",
            "      - 200\n",
            "      - 145\n",
            "      - 40\n",
            "      - 145\n",
            "      - 87\n",
            "      - 66\n",
            "      - 57\n",
            "      - 63\n",
            "      - 114\n",
            "      - 50\n",
            "      - 45\n",
            "      - 114\n",
            "      - 96\n",
            "      - 37\n",
            "      - 34\n",
            "      - 24\n",
            "      - 121\n",
            "      - 30\n",
            "      - 25\n",
            "      - 93\n",
            "      - 13\n",
            "      - 35\n",
            "      - 36\n",
            "      - 102\n",
            "      - 95\n",
            "      - 138\n",
            "      - 26\n",
            "      - 56\n",
            "      - 15\n",
            "      - 159\n",
            "      - 32\n",
            "      - 35\n",
            "      - 103\n",
            "      - 14\n",
            "      - 64\n",
            "      - 117\n",
            "      - 48\n",
            "      episode_reward:\n",
            "      - 61.0\n",
            "      - 136.0\n",
            "      - 47.0\n",
            "      - 124.0\n",
            "      - 89.0\n",
            "      - 16.0\n",
            "      - 27.0\n",
            "      - 17.0\n",
            "      - 200.0\n",
            "      - 59.0\n",
            "      - 26.0\n",
            "      - 20.0\n",
            "      - 74.0\n",
            "      - 88.0\n",
            "      - 146.0\n",
            "      - 190.0\n",
            "      - 77.0\n",
            "      - 78.0\n",
            "      - 34.0\n",
            "      - 34.0\n",
            "      - 199.0\n",
            "      - 31.0\n",
            "      - 56.0\n",
            "      - 45.0\n",
            "      - 50.0\n",
            "      - 87.0\n",
            "      - 177.0\n",
            "      - 107.0\n",
            "      - 77.0\n",
            "      - 38.0\n",
            "      - 76.0\n",
            "      - 48.0\n",
            "      - 26.0\n",
            "      - 123.0\n",
            "      - 32.0\n",
            "      - 61.0\n",
            "      - 64.0\n",
            "      - 13.0\n",
            "      - 67.0\n",
            "      - 108.0\n",
            "      - 38.0\n",
            "      - 25.0\n",
            "      - 28.0\n",
            "      - 99.0\n",
            "      - 25.0\n",
            "      - 77.0\n",
            "      - 43.0\n",
            "      - 16.0\n",
            "      - 146.0\n",
            "      - 72.0\n",
            "      - 20.0\n",
            "      - 139.0\n",
            "      - 32.0\n",
            "      - 20.0\n",
            "      - 81.0\n",
            "      - 49.0\n",
            "      - 130.0\n",
            "      - 126.0\n",
            "      - 173.0\n",
            "      - 68.0\n",
            "      - 27.0\n",
            "      - 51.0\n",
            "      - 109.0\n",
            "      - 119.0\n",
            "      - 90.0\n",
            "      - 25.0\n",
            "      - 70.0\n",
            "      - 66.0\n",
            "      - 71.0\n",
            "      - 12.0\n",
            "      - 34.0\n",
            "      - 24.0\n",
            "      - 48.0\n",
            "      - 28.0\n",
            "      - 75.0\n",
            "      - 106.0\n",
            "      - 75.0\n",
            "      - 104.0\n",
            "      - 36.0\n",
            "      - 87.0\n",
            "      - 28.0\n",
            "      - 90.0\n",
            "      - 110.0\n",
            "      - 126.0\n",
            "      - 114.0\n",
            "      - 156.0\n",
            "      - 24.0\n",
            "      - 59.0\n",
            "      - 26.0\n",
            "      - 28.0\n",
            "      - 51.0\n",
            "      - 12.0\n",
            "      - 31.0\n",
            "      - 34.0\n",
            "      - 33.0\n",
            "      - 133.0\n",
            "      - 19.0\n",
            "      - 157.0\n",
            "      - 17.0\n",
            "      - 200.0\n",
            "      - 145.0\n",
            "      - 40.0\n",
            "      - 145.0\n",
            "      - 87.0\n",
            "      - 66.0\n",
            "      - 57.0\n",
            "      - 63.0\n",
            "      - 114.0\n",
            "      - 50.0\n",
            "      - 45.0\n",
            "      - 114.0\n",
            "      - 96.0\n",
            "      - 37.0\n",
            "      - 34.0\n",
            "      - 24.0\n",
            "      - 121.0\n",
            "      - 30.0\n",
            "      - 25.0\n",
            "      - 93.0\n",
            "      - 13.0\n",
            "      - 35.0\n",
            "      - 36.0\n",
            "      - 102.0\n",
            "      - 95.0\n",
            "      - 138.0\n",
            "      - 26.0\n",
            "      - 56.0\n",
            "      - 15.0\n",
            "      - 159.0\n",
            "      - 32.0\n",
            "      - 35.0\n",
            "      - 103.0\n",
            "      - 14.0\n",
            "      - 64.0\n",
            "      - 117.0\n",
            "      - 48.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13865589997687422\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1082088192442604\n",
            "      mean_inference_ms: 1.3218167865493562\n",
            "      mean_raw_obs_processing_ms: 0.249278173777411\n",
            "  time_since_restore: 375.93605279922485\n",
            "  time_this_iter_s: 10.13578724861145\n",
            "  time_total_s: 375.93605279922485\n",
            "  timers:\n",
            "    learn_throughput: 31545.252\n",
            "    learn_time_ms: 6.34\n",
            "    load_throughput: 1058900.278\n",
            "    load_time_ms: 0.189\n",
            "    training_iteration_time_ms: 208.11\n",
            "    update_time_ms: 4.052\n",
            "  timestamp: 1656954247\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 182200\n",
            "  training_iteration: 35\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:12 (running for 00:06:41.63)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         375.936</td><td style=\"text-align: right;\">182200</td><td style=\"text-align: right;\"> 70.6912</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">           70.6912</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:17 (running for 00:06:46.82)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         375.936</td><td style=\"text-align: right;\">182200</td><td style=\"text-align: right;\"> 70.6912</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">           70.6912</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 183400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 183400\n",
            "    num_agent_steps_trained: 183400\n",
            "    num_env_steps_sampled: 183400\n",
            "    num_env_steps_trained: 183400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-04-18\n",
            "  done: false\n",
            "  episode_len_mean: 64.01\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 64.01\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 24\n",
            "  episodes_total: 2175\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 80.7\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 145.0\n",
            "    episode_reward_mean: 80.7\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 49\n",
            "      - 100\n",
            "      - 145\n",
            "      - 39\n",
            "      - 56\n",
            "      - 87\n",
            "      - 68\n",
            "      - 142\n",
            "      - 36\n",
            "      - 131\n",
            "      - 145\n",
            "      - 103\n",
            "      - 33\n",
            "      - 106\n",
            "      - 105\n",
            "      - 89\n",
            "      - 14\n",
            "      - 12\n",
            "      - 139\n",
            "      - 15\n",
            "      episode_reward:\n",
            "      - 49.0\n",
            "      - 100.0\n",
            "      - 145.0\n",
            "      - 39.0\n",
            "      - 56.0\n",
            "      - 87.0\n",
            "      - 68.0\n",
            "      - 142.0\n",
            "      - 36.0\n",
            "      - 131.0\n",
            "      - 145.0\n",
            "      - 103.0\n",
            "      - 33.0\n",
            "      - 106.0\n",
            "      - 105.0\n",
            "      - 89.0\n",
            "      - 14.0\n",
            "      - 12.0\n",
            "      - 139.0\n",
            "      - 15.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10008870565454815\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07650703706135031\n",
            "      mean_inference_ms: 0.9764846612687104\n",
            "      mean_raw_obs_processing_ms: 0.1069283317571517\n",
            "    timesteps_this_iter: 1614\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 128.03573608398438\n",
            "          policy_loss: 328.685791015625\n",
            "          var_gnorm: 24.797996520996094\n",
            "          vf_explained_var: 0.007555603981018066\n",
            "          vf_loss: 7759.76904296875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 183400\n",
            "    num_agent_steps_trained: 183400\n",
            "    num_env_steps_sampled: 183400\n",
            "    num_env_steps_trained: 183400\n",
            "  iterations_since_restore: 36\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 183400\n",
            "  num_agent_steps_trained: 183400\n",
            "  num_env_steps_sampled: 183400\n",
            "  num_env_steps_sampled_this_iter: 1200\n",
            "  num_env_steps_trained: 183400\n",
            "  num_env_steps_trained_this_iter: 1200\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 76.12352941176471\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13808231042466015\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10833292505990423\n",
            "    mean_inference_ms: 1.319064834399503\n",
            "    mean_raw_obs_processing_ms: 0.250203550177589\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 64.01\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 64.01\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 24\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 27\n",
            "      - 51\n",
            "      - 109\n",
            "      - 119\n",
            "      - 90\n",
            "      - 25\n",
            "      - 70\n",
            "      - 66\n",
            "      - 71\n",
            "      - 12\n",
            "      - 34\n",
            "      - 24\n",
            "      - 48\n",
            "      - 28\n",
            "      - 75\n",
            "      - 106\n",
            "      - 75\n",
            "      - 104\n",
            "      - 36\n",
            "      - 87\n",
            "      - 28\n",
            "      - 90\n",
            "      - 110\n",
            "      - 126\n",
            "      - 114\n",
            "      - 156\n",
            "      - 24\n",
            "      - 59\n",
            "      - 26\n",
            "      - 28\n",
            "      - 51\n",
            "      - 12\n",
            "      - 31\n",
            "      - 34\n",
            "      - 33\n",
            "      - 133\n",
            "      - 19\n",
            "      - 157\n",
            "      - 17\n",
            "      - 200\n",
            "      - 145\n",
            "      - 40\n",
            "      - 145\n",
            "      - 87\n",
            "      - 66\n",
            "      - 57\n",
            "      - 63\n",
            "      - 114\n",
            "      - 50\n",
            "      - 45\n",
            "      - 114\n",
            "      - 96\n",
            "      - 37\n",
            "      - 34\n",
            "      - 24\n",
            "      - 121\n",
            "      - 30\n",
            "      - 25\n",
            "      - 93\n",
            "      - 13\n",
            "      - 35\n",
            "      - 36\n",
            "      - 102\n",
            "      - 95\n",
            "      - 138\n",
            "      - 26\n",
            "      - 56\n",
            "      - 15\n",
            "      - 159\n",
            "      - 32\n",
            "      - 35\n",
            "      - 103\n",
            "      - 14\n",
            "      - 64\n",
            "      - 117\n",
            "      - 48\n",
            "      - 45\n",
            "      - 25\n",
            "      - 94\n",
            "      - 29\n",
            "      - 34\n",
            "      - 22\n",
            "      - 19\n",
            "      - 87\n",
            "      - 84\n",
            "      - 37\n",
            "      - 95\n",
            "      - 37\n",
            "      - 176\n",
            "      - 32\n",
            "      - 28\n",
            "      - 37\n",
            "      - 71\n",
            "      - 47\n",
            "      - 35\n",
            "      - 64\n",
            "      - 37\n",
            "      - 16\n",
            "      - 37\n",
            "      - 34\n",
            "      episode_reward:\n",
            "      - 27.0\n",
            "      - 51.0\n",
            "      - 109.0\n",
            "      - 119.0\n",
            "      - 90.0\n",
            "      - 25.0\n",
            "      - 70.0\n",
            "      - 66.0\n",
            "      - 71.0\n",
            "      - 12.0\n",
            "      - 34.0\n",
            "      - 24.0\n",
            "      - 48.0\n",
            "      - 28.0\n",
            "      - 75.0\n",
            "      - 106.0\n",
            "      - 75.0\n",
            "      - 104.0\n",
            "      - 36.0\n",
            "      - 87.0\n",
            "      - 28.0\n",
            "      - 90.0\n",
            "      - 110.0\n",
            "      - 126.0\n",
            "      - 114.0\n",
            "      - 156.0\n",
            "      - 24.0\n",
            "      - 59.0\n",
            "      - 26.0\n",
            "      - 28.0\n",
            "      - 51.0\n",
            "      - 12.0\n",
            "      - 31.0\n",
            "      - 34.0\n",
            "      - 33.0\n",
            "      - 133.0\n",
            "      - 19.0\n",
            "      - 157.0\n",
            "      - 17.0\n",
            "      - 200.0\n",
            "      - 145.0\n",
            "      - 40.0\n",
            "      - 145.0\n",
            "      - 87.0\n",
            "      - 66.0\n",
            "      - 57.0\n",
            "      - 63.0\n",
            "      - 114.0\n",
            "      - 50.0\n",
            "      - 45.0\n",
            "      - 114.0\n",
            "      - 96.0\n",
            "      - 37.0\n",
            "      - 34.0\n",
            "      - 24.0\n",
            "      - 121.0\n",
            "      - 30.0\n",
            "      - 25.0\n",
            "      - 93.0\n",
            "      - 13.0\n",
            "      - 35.0\n",
            "      - 36.0\n",
            "      - 102.0\n",
            "      - 95.0\n",
            "      - 138.0\n",
            "      - 26.0\n",
            "      - 56.0\n",
            "      - 15.0\n",
            "      - 159.0\n",
            "      - 32.0\n",
            "      - 35.0\n",
            "      - 103.0\n",
            "      - 14.0\n",
            "      - 64.0\n",
            "      - 117.0\n",
            "      - 48.0\n",
            "      - 45.0\n",
            "      - 25.0\n",
            "      - 94.0\n",
            "      - 29.0\n",
            "      - 34.0\n",
            "      - 22.0\n",
            "      - 19.0\n",
            "      - 87.0\n",
            "      - 84.0\n",
            "      - 37.0\n",
            "      - 95.0\n",
            "      - 37.0\n",
            "      - 176.0\n",
            "      - 32.0\n",
            "      - 28.0\n",
            "      - 37.0\n",
            "      - 71.0\n",
            "      - 47.0\n",
            "      - 35.0\n",
            "      - 64.0\n",
            "      - 37.0\n",
            "      - 16.0\n",
            "      - 37.0\n",
            "      - 34.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13808231042466015\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10833292505990423\n",
            "      mean_inference_ms: 1.319064834399503\n",
            "      mean_raw_obs_processing_ms: 0.250203550177589\n",
            "  time_since_restore: 387.629998922348\n",
            "  time_this_iter_s: 11.693946123123169\n",
            "  time_total_s: 387.629998922348\n",
            "  timers:\n",
            "    learn_throughput: 27856.543\n",
            "    learn_time_ms: 7.18\n",
            "    load_throughput: 976782.487\n",
            "    load_time_ms: 0.205\n",
            "    training_iteration_time_ms: 230.829\n",
            "    update_time_ms: 4.794\n",
            "  timestamp: 1656954258\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 183400\n",
            "  training_iteration: 36\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:23 (running for 00:06:53.41)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">          387.63</td><td style=\"text-align: right;\">183400</td><td style=\"text-align: right;\">   64.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             64.01</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:29 (running for 00:06:58.53)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">          387.63</td><td style=\"text-align: right;\">183400</td><td style=\"text-align: right;\">   64.01</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             64.01</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 193000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 193000\n",
            "    num_agent_steps_trained: 193000\n",
            "    num_env_steps_sampled: 193000\n",
            "    num_env_steps_trained: 193000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-04-29\n",
            "  done: false\n",
            "  episode_len_mean: 73.16030534351145\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 73.16030534351145\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 131\n",
            "  episodes_total: 2306\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 125.46226501464844\n",
            "          policy_loss: 469.53973388671875\n",
            "          var_gnorm: 25.001789093017578\n",
            "          vf_explained_var: 0.0882461667060852\n",
            "          vf_loss: 8255.29296875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 193000\n",
            "    num_agent_steps_trained: 193000\n",
            "    num_env_steps_sampled: 193000\n",
            "    num_env_steps_trained: 193000\n",
            "  iterations_since_restore: 37\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 193000\n",
            "  num_agent_steps_trained: 193000\n",
            "  num_env_steps_sampled: 193000\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 193000\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.85714285714288\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13866251021004333\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1080926480350267\n",
            "    mean_inference_ms: 1.321348702184805\n",
            "    mean_raw_obs_processing_ms: 0.24895778760926981\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 73.16030534351145\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 73.16030534351145\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 131\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 44\n",
            "      - 124\n",
            "      - 25\n",
            "      - 17\n",
            "      - 124\n",
            "      - 102\n",
            "      - 24\n",
            "      - 33\n",
            "      - 26\n",
            "      - 29\n",
            "      - 39\n",
            "      - 57\n",
            "      - 29\n",
            "      - 50\n",
            "      - 54\n",
            "      - 39\n",
            "      - 187\n",
            "      - 85\n",
            "      - 22\n",
            "      - 27\n",
            "      - 80\n",
            "      - 77\n",
            "      - 53\n",
            "      - 20\n",
            "      - 23\n",
            "      - 111\n",
            "      - 118\n",
            "      - 42\n",
            "      - 76\n",
            "      - 191\n",
            "      - 119\n",
            "      - 195\n",
            "      - 78\n",
            "      - 191\n",
            "      - 71\n",
            "      - 53\n",
            "      - 22\n",
            "      - 84\n",
            "      - 57\n",
            "      - 45\n",
            "      - 23\n",
            "      - 21\n",
            "      - 98\n",
            "      - 141\n",
            "      - 57\n",
            "      - 46\n",
            "      - 16\n",
            "      - 40\n",
            "      - 66\n",
            "      - 28\n",
            "      - 87\n",
            "      - 102\n",
            "      - 138\n",
            "      - 31\n",
            "      - 21\n",
            "      - 71\n",
            "      - 29\n",
            "      - 132\n",
            "      - 22\n",
            "      - 43\n",
            "      - 59\n",
            "      - 136\n",
            "      - 52\n",
            "      - 56\n",
            "      - 28\n",
            "      - 64\n",
            "      - 123\n",
            "      - 65\n",
            "      - 44\n",
            "      - 56\n",
            "      - 11\n",
            "      - 60\n",
            "      - 82\n",
            "      - 87\n",
            "      - 38\n",
            "      - 24\n",
            "      - 105\n",
            "      - 125\n",
            "      - 79\n",
            "      - 166\n",
            "      - 147\n",
            "      - 174\n",
            "      - 38\n",
            "      - 109\n",
            "      - 39\n",
            "      - 13\n",
            "      - 80\n",
            "      - 63\n",
            "      - 119\n",
            "      - 113\n",
            "      - 54\n",
            "      - 38\n",
            "      - 60\n",
            "      - 79\n",
            "      - 152\n",
            "      - 154\n",
            "      - 59\n",
            "      - 127\n",
            "      - 33\n",
            "      - 19\n",
            "      - 200\n",
            "      - 116\n",
            "      - 133\n",
            "      - 35\n",
            "      - 27\n",
            "      - 21\n",
            "      - 16\n",
            "      - 71\n",
            "      - 74\n",
            "      - 26\n",
            "      - 160\n",
            "      - 26\n",
            "      - 47\n",
            "      - 74\n",
            "      - 95\n",
            "      - 99\n",
            "      - 91\n",
            "      - 97\n",
            "      - 22\n",
            "      - 29\n",
            "      - 79\n",
            "      - 16\n",
            "      - 61\n",
            "      - 123\n",
            "      - 22\n",
            "      - 180\n",
            "      - 24\n",
            "      - 21\n",
            "      - 167\n",
            "      - 170\n",
            "      - 77\n",
            "      episode_reward:\n",
            "      - 44.0\n",
            "      - 124.0\n",
            "      - 25.0\n",
            "      - 17.0\n",
            "      - 124.0\n",
            "      - 102.0\n",
            "      - 24.0\n",
            "      - 33.0\n",
            "      - 26.0\n",
            "      - 29.0\n",
            "      - 39.0\n",
            "      - 57.0\n",
            "      - 29.0\n",
            "      - 50.0\n",
            "      - 54.0\n",
            "      - 39.0\n",
            "      - 187.0\n",
            "      - 85.0\n",
            "      - 22.0\n",
            "      - 27.0\n",
            "      - 80.0\n",
            "      - 77.0\n",
            "      - 53.0\n",
            "      - 20.0\n",
            "      - 23.0\n",
            "      - 111.0\n",
            "      - 118.0\n",
            "      - 42.0\n",
            "      - 76.0\n",
            "      - 191.0\n",
            "      - 119.0\n",
            "      - 195.0\n",
            "      - 78.0\n",
            "      - 191.0\n",
            "      - 71.0\n",
            "      - 53.0\n",
            "      - 22.0\n",
            "      - 84.0\n",
            "      - 57.0\n",
            "      - 45.0\n",
            "      - 23.0\n",
            "      - 21.0\n",
            "      - 98.0\n",
            "      - 141.0\n",
            "      - 57.0\n",
            "      - 46.0\n",
            "      - 16.0\n",
            "      - 40.0\n",
            "      - 66.0\n",
            "      - 28.0\n",
            "      - 87.0\n",
            "      - 102.0\n",
            "      - 138.0\n",
            "      - 31.0\n",
            "      - 21.0\n",
            "      - 71.0\n",
            "      - 29.0\n",
            "      - 132.0\n",
            "      - 22.0\n",
            "      - 43.0\n",
            "      - 59.0\n",
            "      - 136.0\n",
            "      - 52.0\n",
            "      - 56.0\n",
            "      - 28.0\n",
            "      - 64.0\n",
            "      - 123.0\n",
            "      - 65.0\n",
            "      - 44.0\n",
            "      - 56.0\n",
            "      - 11.0\n",
            "      - 60.0\n",
            "      - 82.0\n",
            "      - 87.0\n",
            "      - 38.0\n",
            "      - 24.0\n",
            "      - 105.0\n",
            "      - 125.0\n",
            "      - 79.0\n",
            "      - 166.0\n",
            "      - 147.0\n",
            "      - 174.0\n",
            "      - 38.0\n",
            "      - 109.0\n",
            "      - 39.0\n",
            "      - 13.0\n",
            "      - 80.0\n",
            "      - 63.0\n",
            "      - 119.0\n",
            "      - 113.0\n",
            "      - 54.0\n",
            "      - 38.0\n",
            "      - 60.0\n",
            "      - 79.0\n",
            "      - 152.0\n",
            "      - 154.0\n",
            "      - 59.0\n",
            "      - 127.0\n",
            "      - 33.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 133.0\n",
            "      - 35.0\n",
            "      - 27.0\n",
            "      - 21.0\n",
            "      - 16.0\n",
            "      - 71.0\n",
            "      - 74.0\n",
            "      - 26.0\n",
            "      - 160.0\n",
            "      - 26.0\n",
            "      - 47.0\n",
            "      - 74.0\n",
            "      - 95.0\n",
            "      - 99.0\n",
            "      - 91.0\n",
            "      - 97.0\n",
            "      - 22.0\n",
            "      - 29.0\n",
            "      - 79.0\n",
            "      - 16.0\n",
            "      - 61.0\n",
            "      - 123.0\n",
            "      - 22.0\n",
            "      - 180.0\n",
            "      - 24.0\n",
            "      - 21.0\n",
            "      - 167.0\n",
            "      - 170.0\n",
            "      - 77.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13866251021004333\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1080926480350267\n",
            "      mean_inference_ms: 1.321348702184805\n",
            "      mean_raw_obs_processing_ms: 0.24895778760926981\n",
            "  time_since_restore: 397.8201193809509\n",
            "  time_this_iter_s: 10.190120458602905\n",
            "  time_total_s: 397.8201193809509\n",
            "  timers:\n",
            "    learn_throughput: 33253.158\n",
            "    learn_time_ms: 6.014\n",
            "    load_throughput: 1020760.282\n",
            "    load_time_ms: 0.196\n",
            "    training_iteration_time_ms: 213.343\n",
            "    update_time_ms: 3.2\n",
            "  timestamp: 1656954269\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 193000\n",
            "  training_iteration: 37\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:34 (running for 00:07:03.63)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">          397.82</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\"> 73.1603</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">           73.1603</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:39 (running for 00:07:08.73)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">          397.82</td><td style=\"text-align: right;\">193000</td><td style=\"text-align: right;\"> 73.1603</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">           73.1603</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 194200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 194200\n",
            "    num_agent_steps_trained: 194200\n",
            "    num_env_steps_sampled: 194200\n",
            "    num_env_steps_trained: 194200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-04-41\n",
            "  done: false\n",
            "  episode_len_mean: 75.08\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 75.08\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 14\n",
            "  episodes_total: 2320\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 78.7\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 78.7\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 58\n",
            "      - 162\n",
            "      - 25\n",
            "      - 154\n",
            "      - 130\n",
            "      - 34\n",
            "      - 49\n",
            "      - 200\n",
            "      - 122\n",
            "      - 23\n",
            "      - 89\n",
            "      - 69\n",
            "      - 55\n",
            "      - 15\n",
            "      - 55\n",
            "      - 36\n",
            "      - 132\n",
            "      - 36\n",
            "      - 61\n",
            "      - 69\n",
            "      episode_reward:\n",
            "      - 58.0\n",
            "      - 162.0\n",
            "      - 25.0\n",
            "      - 154.0\n",
            "      - 130.0\n",
            "      - 34.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 23.0\n",
            "      - 89.0\n",
            "      - 69.0\n",
            "      - 55.0\n",
            "      - 15.0\n",
            "      - 55.0\n",
            "      - 36.0\n",
            "      - 132.0\n",
            "      - 36.0\n",
            "      - 61.0\n",
            "      - 69.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09994344283033443\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07638033406866791\n",
            "      mean_inference_ms: 0.9748643747100175\n",
            "      mean_raw_obs_processing_ms: 0.10693849378637306\n",
            "    timesteps_this_iter: 1574\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.09732055664062\n",
            "          policy_loss: 654.98583984375\n",
            "          var_gnorm: 25.026683807373047\n",
            "          vf_explained_var: 0.06764048337936401\n",
            "          vf_loss: 7777.58837890625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 194200\n",
            "    num_agent_steps_trained: 194200\n",
            "    num_env_steps_sampled: 194200\n",
            "    num_env_steps_trained: 194200\n",
            "  iterations_since_restore: 38\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 194200\n",
            "  num_agent_steps_trained: 194200\n",
            "  num_env_steps_sampled: 194200\n",
            "  num_env_steps_sampled_this_iter: 1200\n",
            "  num_env_steps_trained: 194200\n",
            "  num_env_steps_trained_this_iter: 1200\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 67.51111111111112\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13827512203952602\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10814643715122298\n",
            "    mean_inference_ms: 1.3195239740184113\n",
            "    mean_raw_obs_processing_ms: 0.24948699342355343\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 75.08\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 75.08\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 14\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 46\n",
            "      - 16\n",
            "      - 40\n",
            "      - 66\n",
            "      - 28\n",
            "      - 87\n",
            "      - 102\n",
            "      - 138\n",
            "      - 31\n",
            "      - 21\n",
            "      - 71\n",
            "      - 29\n",
            "      - 132\n",
            "      - 22\n",
            "      - 43\n",
            "      - 59\n",
            "      - 136\n",
            "      - 52\n",
            "      - 56\n",
            "      - 28\n",
            "      - 64\n",
            "      - 123\n",
            "      - 65\n",
            "      - 44\n",
            "      - 56\n",
            "      - 11\n",
            "      - 60\n",
            "      - 82\n",
            "      - 87\n",
            "      - 38\n",
            "      - 24\n",
            "      - 105\n",
            "      - 125\n",
            "      - 79\n",
            "      - 166\n",
            "      - 147\n",
            "      - 174\n",
            "      - 38\n",
            "      - 109\n",
            "      - 39\n",
            "      - 13\n",
            "      - 80\n",
            "      - 63\n",
            "      - 119\n",
            "      - 113\n",
            "      - 54\n",
            "      - 38\n",
            "      - 60\n",
            "      - 79\n",
            "      - 152\n",
            "      - 154\n",
            "      - 59\n",
            "      - 127\n",
            "      - 33\n",
            "      - 19\n",
            "      - 200\n",
            "      - 116\n",
            "      - 133\n",
            "      - 35\n",
            "      - 27\n",
            "      - 21\n",
            "      - 16\n",
            "      - 71\n",
            "      - 74\n",
            "      - 26\n",
            "      - 160\n",
            "      - 26\n",
            "      - 47\n",
            "      - 74\n",
            "      - 95\n",
            "      - 99\n",
            "      - 91\n",
            "      - 97\n",
            "      - 22\n",
            "      - 29\n",
            "      - 79\n",
            "      - 16\n",
            "      - 61\n",
            "      - 123\n",
            "      - 22\n",
            "      - 180\n",
            "      - 24\n",
            "      - 21\n",
            "      - 167\n",
            "      - 170\n",
            "      - 77\n",
            "      - 20\n",
            "      - 28\n",
            "      - 85\n",
            "      - 119\n",
            "      - 19\n",
            "      - 130\n",
            "      - 131\n",
            "      - 50\n",
            "      - 76\n",
            "      - 144\n",
            "      - 20\n",
            "      - 140\n",
            "      - 74\n",
            "      - 71\n",
            "      episode_reward:\n",
            "      - 46.0\n",
            "      - 16.0\n",
            "      - 40.0\n",
            "      - 66.0\n",
            "      - 28.0\n",
            "      - 87.0\n",
            "      - 102.0\n",
            "      - 138.0\n",
            "      - 31.0\n",
            "      - 21.0\n",
            "      - 71.0\n",
            "      - 29.0\n",
            "      - 132.0\n",
            "      - 22.0\n",
            "      - 43.0\n",
            "      - 59.0\n",
            "      - 136.0\n",
            "      - 52.0\n",
            "      - 56.0\n",
            "      - 28.0\n",
            "      - 64.0\n",
            "      - 123.0\n",
            "      - 65.0\n",
            "      - 44.0\n",
            "      - 56.0\n",
            "      - 11.0\n",
            "      - 60.0\n",
            "      - 82.0\n",
            "      - 87.0\n",
            "      - 38.0\n",
            "      - 24.0\n",
            "      - 105.0\n",
            "      - 125.0\n",
            "      - 79.0\n",
            "      - 166.0\n",
            "      - 147.0\n",
            "      - 174.0\n",
            "      - 38.0\n",
            "      - 109.0\n",
            "      - 39.0\n",
            "      - 13.0\n",
            "      - 80.0\n",
            "      - 63.0\n",
            "      - 119.0\n",
            "      - 113.0\n",
            "      - 54.0\n",
            "      - 38.0\n",
            "      - 60.0\n",
            "      - 79.0\n",
            "      - 152.0\n",
            "      - 154.0\n",
            "      - 59.0\n",
            "      - 127.0\n",
            "      - 33.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 133.0\n",
            "      - 35.0\n",
            "      - 27.0\n",
            "      - 21.0\n",
            "      - 16.0\n",
            "      - 71.0\n",
            "      - 74.0\n",
            "      - 26.0\n",
            "      - 160.0\n",
            "      - 26.0\n",
            "      - 47.0\n",
            "      - 74.0\n",
            "      - 95.0\n",
            "      - 99.0\n",
            "      - 91.0\n",
            "      - 97.0\n",
            "      - 22.0\n",
            "      - 29.0\n",
            "      - 79.0\n",
            "      - 16.0\n",
            "      - 61.0\n",
            "      - 123.0\n",
            "      - 22.0\n",
            "      - 180.0\n",
            "      - 24.0\n",
            "      - 21.0\n",
            "      - 167.0\n",
            "      - 170.0\n",
            "      - 77.0\n",
            "      - 20.0\n",
            "      - 28.0\n",
            "      - 85.0\n",
            "      - 119.0\n",
            "      - 19.0\n",
            "      - 130.0\n",
            "      - 131.0\n",
            "      - 50.0\n",
            "      - 76.0\n",
            "      - 144.0\n",
            "      - 20.0\n",
            "      - 140.0\n",
            "      - 74.0\n",
            "      - 71.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13827512203952602\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10814643715122298\n",
            "      mean_inference_ms: 1.3195239740184113\n",
            "      mean_raw_obs_processing_ms: 0.24948699342355343\n",
            "  time_since_restore: 409.7371115684509\n",
            "  time_this_iter_s: 11.9169921875\n",
            "  time_total_s: 409.7371115684509\n",
            "  timers:\n",
            "    learn_throughput: 32698.898\n",
            "    learn_time_ms: 6.116\n",
            "    load_throughput: 1088581.365\n",
            "    load_time_ms: 0.184\n",
            "    training_iteration_time_ms: 214.733\n",
            "    update_time_ms: 3.273\n",
            "  timestamp: 1656954281\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 194200\n",
            "  training_iteration: 38\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:46 (running for 00:07:15.58)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         409.737</td><td style=\"text-align: right;\">194200</td><td style=\"text-align: right;\">   75.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             75.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:51 (running for 00:07:20.71)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         409.737</td><td style=\"text-align: right;\">194200</td><td style=\"text-align: right;\">   75.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">             75.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 204000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 204000\n",
            "    num_agent_steps_trained: 204000\n",
            "    num_env_steps_sampled: 204000\n",
            "    num_env_steps_trained: 204000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-04-51\n",
            "  done: false\n",
            "  episode_len_mean: 71.78260869565217\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 71.78260869565217\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 138\n",
            "  episodes_total: 2458\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 129.71157836914062\n",
            "          policy_loss: 819.6629638671875\n",
            "          var_gnorm: 25.226699829101562\n",
            "          vf_explained_var: 0.04925113916397095\n",
            "          vf_loss: 8066.8046875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 204000\n",
            "    num_agent_steps_trained: 204000\n",
            "    num_env_steps_sampled: 204000\n",
            "    num_env_steps_trained: 204000\n",
            "  iterations_since_restore: 39\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 204000\n",
            "  num_agent_steps_trained: 204000\n",
            "  num_env_steps_sampled: 204000\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 204000\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.55714285714285\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1383554767586643\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1079994923045237\n",
            "    mean_inference_ms: 1.3179324601160713\n",
            "    mean_raw_obs_processing_ms: 0.24878925385392436\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 71.78260869565217\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 71.78260869565217\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 138\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 130\n",
            "      - 36\n",
            "      - 106\n",
            "      - 26\n",
            "      - 43\n",
            "      - 117\n",
            "      - 105\n",
            "      - 63\n",
            "      - 78\n",
            "      - 41\n",
            "      - 47\n",
            "      - 82\n",
            "      - 58\n",
            "      - 24\n",
            "      - 70\n",
            "      - 39\n",
            "      - 84\n",
            "      - 100\n",
            "      - 14\n",
            "      - 153\n",
            "      - 116\n",
            "      - 132\n",
            "      - 53\n",
            "      - 70\n",
            "      - 120\n",
            "      - 86\n",
            "      - 80\n",
            "      - 110\n",
            "      - 33\n",
            "      - 171\n",
            "      - 49\n",
            "      - 200\n",
            "      - 69\n",
            "      - 68\n",
            "      - 114\n",
            "      - 20\n",
            "      - 117\n",
            "      - 44\n",
            "      - 34\n",
            "      - 116\n",
            "      - 171\n",
            "      - 18\n",
            "      - 40\n",
            "      - 18\n",
            "      - 74\n",
            "      - 100\n",
            "      - 91\n",
            "      - 83\n",
            "      - 33\n",
            "      - 113\n",
            "      - 42\n",
            "      - 39\n",
            "      - 73\n",
            "      - 131\n",
            "      - 84\n",
            "      - 50\n",
            "      - 101\n",
            "      - 18\n",
            "      - 21\n",
            "      - 61\n",
            "      - 27\n",
            "      - 71\n",
            "      - 42\n",
            "      - 79\n",
            "      - 107\n",
            "      - 139\n",
            "      - 51\n",
            "      - 40\n",
            "      - 27\n",
            "      - 67\n",
            "      - 21\n",
            "      - 34\n",
            "      - 89\n",
            "      - 130\n",
            "      - 18\n",
            "      - 21\n",
            "      - 89\n",
            "      - 48\n",
            "      - 37\n",
            "      - 103\n",
            "      - 55\n",
            "      - 121\n",
            "      - 23\n",
            "      - 43\n",
            "      - 105\n",
            "      - 109\n",
            "      - 73\n",
            "      - 23\n",
            "      - 24\n",
            "      - 58\n",
            "      - 50\n",
            "      - 26\n",
            "      - 98\n",
            "      - 115\n",
            "      - 57\n",
            "      - 18\n",
            "      - 19\n",
            "      - 46\n",
            "      - 23\n",
            "      - 67\n",
            "      - 61\n",
            "      - 114\n",
            "      - 38\n",
            "      - 106\n",
            "      - 174\n",
            "      - 111\n",
            "      - 46\n",
            "      - 143\n",
            "      - 30\n",
            "      - 15\n",
            "      - 165\n",
            "      - 31\n",
            "      - 74\n",
            "      - 74\n",
            "      - 48\n",
            "      - 26\n",
            "      - 43\n",
            "      - 124\n",
            "      - 30\n",
            "      - 24\n",
            "      - 33\n",
            "      - 57\n",
            "      - 32\n",
            "      - 24\n",
            "      - 132\n",
            "      - 88\n",
            "      - 45\n",
            "      - 85\n",
            "      - 29\n",
            "      - 122\n",
            "      - 143\n",
            "      - 132\n",
            "      - 36\n",
            "      - 49\n",
            "      - 29\n",
            "      - 80\n",
            "      - 141\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 130.0\n",
            "      - 36.0\n",
            "      - 106.0\n",
            "      - 26.0\n",
            "      - 43.0\n",
            "      - 117.0\n",
            "      - 105.0\n",
            "      - 63.0\n",
            "      - 78.0\n",
            "      - 41.0\n",
            "      - 47.0\n",
            "      - 82.0\n",
            "      - 58.0\n",
            "      - 24.0\n",
            "      - 70.0\n",
            "      - 39.0\n",
            "      - 84.0\n",
            "      - 100.0\n",
            "      - 14.0\n",
            "      - 153.0\n",
            "      - 116.0\n",
            "      - 132.0\n",
            "      - 53.0\n",
            "      - 70.0\n",
            "      - 120.0\n",
            "      - 86.0\n",
            "      - 80.0\n",
            "      - 110.0\n",
            "      - 33.0\n",
            "      - 171.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 68.0\n",
            "      - 114.0\n",
            "      - 20.0\n",
            "      - 117.0\n",
            "      - 44.0\n",
            "      - 34.0\n",
            "      - 116.0\n",
            "      - 171.0\n",
            "      - 18.0\n",
            "      - 40.0\n",
            "      - 18.0\n",
            "      - 74.0\n",
            "      - 100.0\n",
            "      - 91.0\n",
            "      - 83.0\n",
            "      - 33.0\n",
            "      - 113.0\n",
            "      - 42.0\n",
            "      - 39.0\n",
            "      - 73.0\n",
            "      - 131.0\n",
            "      - 84.0\n",
            "      - 50.0\n",
            "      - 101.0\n",
            "      - 18.0\n",
            "      - 21.0\n",
            "      - 61.0\n",
            "      - 27.0\n",
            "      - 71.0\n",
            "      - 42.0\n",
            "      - 79.0\n",
            "      - 107.0\n",
            "      - 139.0\n",
            "      - 51.0\n",
            "      - 40.0\n",
            "      - 27.0\n",
            "      - 67.0\n",
            "      - 21.0\n",
            "      - 34.0\n",
            "      - 89.0\n",
            "      - 130.0\n",
            "      - 18.0\n",
            "      - 21.0\n",
            "      - 89.0\n",
            "      - 48.0\n",
            "      - 37.0\n",
            "      - 103.0\n",
            "      - 55.0\n",
            "      - 121.0\n",
            "      - 23.0\n",
            "      - 43.0\n",
            "      - 105.0\n",
            "      - 109.0\n",
            "      - 73.0\n",
            "      - 23.0\n",
            "      - 24.0\n",
            "      - 58.0\n",
            "      - 50.0\n",
            "      - 26.0\n",
            "      - 98.0\n",
            "      - 115.0\n",
            "      - 57.0\n",
            "      - 18.0\n",
            "      - 19.0\n",
            "      - 46.0\n",
            "      - 23.0\n",
            "      - 67.0\n",
            "      - 61.0\n",
            "      - 114.0\n",
            "      - 38.0\n",
            "      - 106.0\n",
            "      - 174.0\n",
            "      - 111.0\n",
            "      - 46.0\n",
            "      - 143.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 165.0\n",
            "      - 31.0\n",
            "      - 74.0\n",
            "      - 74.0\n",
            "      - 48.0\n",
            "      - 26.0\n",
            "      - 43.0\n",
            "      - 124.0\n",
            "      - 30.0\n",
            "      - 24.0\n",
            "      - 33.0\n",
            "      - 57.0\n",
            "      - 32.0\n",
            "      - 24.0\n",
            "      - 132.0\n",
            "      - 88.0\n",
            "      - 45.0\n",
            "      - 85.0\n",
            "      - 29.0\n",
            "      - 122.0\n",
            "      - 143.0\n",
            "      - 132.0\n",
            "      - 36.0\n",
            "      - 49.0\n",
            "      - 29.0\n",
            "      - 80.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1383554767586643\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1079994923045237\n",
            "      mean_inference_ms: 1.3179324601160713\n",
            "      mean_raw_obs_processing_ms: 0.24878925385392436\n",
            "  time_since_restore: 419.9751327037811\n",
            "  time_this_iter_s: 10.2380211353302\n",
            "  time_total_s: 419.9751327037811\n",
            "  timers:\n",
            "    learn_throughput: 33206.297\n",
            "    learn_time_ms: 6.023\n",
            "    load_throughput: 1043359.204\n",
            "    load_time_ms: 0.192\n",
            "    training_iteration_time_ms: 212.018\n",
            "    update_time_ms: 3.875\n",
            "  timestamp: 1656954291\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 204000\n",
            "  training_iteration: 39\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:04:56 (running for 00:07:25.86)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         419.975</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> 71.7826</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">           71.7826</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:01 (running for 00:07:30.94)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         419.975</td><td style=\"text-align: right;\">204000</td><td style=\"text-align: right;\"> 71.7826</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">           71.7826</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 205200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 205200\n",
            "    num_agent_steps_trained: 205200\n",
            "    num_env_steps_sampled: 205200\n",
            "    num_env_steps_trained: 205200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-05-03\n",
            "  done: false\n",
            "  episode_len_mean: 68.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 68.25\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 16\n",
            "  episodes_total: 2474\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 71.3\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 196.0\n",
            "    episode_reward_mean: 71.3\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 129\n",
            "      - 112\n",
            "      - 144\n",
            "      - 46\n",
            "      - 105\n",
            "      - 15\n",
            "      - 70\n",
            "      - 100\n",
            "      - 22\n",
            "      - 75\n",
            "      - 22\n",
            "      - 52\n",
            "      - 57\n",
            "      - 136\n",
            "      - 37\n",
            "      - 19\n",
            "      - 37\n",
            "      - 22\n",
            "      - 30\n",
            "      - 196\n",
            "      episode_reward:\n",
            "      - 129.0\n",
            "      - 112.0\n",
            "      - 144.0\n",
            "      - 46.0\n",
            "      - 105.0\n",
            "      - 15.0\n",
            "      - 70.0\n",
            "      - 100.0\n",
            "      - 22.0\n",
            "      - 75.0\n",
            "      - 22.0\n",
            "      - 52.0\n",
            "      - 57.0\n",
            "      - 136.0\n",
            "      - 37.0\n",
            "      - 19.0\n",
            "      - 37.0\n",
            "      - 22.0\n",
            "      - 30.0\n",
            "      - 196.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09980458929396882\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07627820947167327\n",
            "      mean_inference_ms: 0.9729905786855445\n",
            "      mean_raw_obs_processing_ms: 0.10688954287315071\n",
            "    timesteps_this_iter: 1426\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.78396606445312\n",
            "          policy_loss: 725.3228759765625\n",
            "          var_gnorm: 25.249576568603516\n",
            "          vf_explained_var: 0.013516783714294434\n",
            "          vf_loss: 8282.732421875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 205200\n",
            "    num_agent_steps_trained: 205200\n",
            "    num_env_steps_sampled: 205200\n",
            "    num_env_steps_trained: 205200\n",
            "  iterations_since_restore: 40\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 205200\n",
            "  num_agent_steps_trained: 205200\n",
            "  num_env_steps_sampled: 205200\n",
            "  num_env_steps_sampled_this_iter: 1200\n",
            "  num_env_steps_trained: 205200\n",
            "  num_env_steps_trained_this_iter: 1200\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 67.16470588235295\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13782870859653215\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10817293768426918\n",
            "    mean_inference_ms: 1.3154271669365876\n",
            "    mean_raw_obs_processing_ms: 0.24955810666735928\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 68.25\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 68.25\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 16\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 84\n",
            "      - 50\n",
            "      - 101\n",
            "      - 18\n",
            "      - 21\n",
            "      - 61\n",
            "      - 27\n",
            "      - 71\n",
            "      - 42\n",
            "      - 79\n",
            "      - 107\n",
            "      - 139\n",
            "      - 51\n",
            "      - 40\n",
            "      - 27\n",
            "      - 67\n",
            "      - 21\n",
            "      - 34\n",
            "      - 89\n",
            "      - 130\n",
            "      - 18\n",
            "      - 21\n",
            "      - 89\n",
            "      - 48\n",
            "      - 37\n",
            "      - 103\n",
            "      - 55\n",
            "      - 121\n",
            "      - 23\n",
            "      - 43\n",
            "      - 105\n",
            "      - 109\n",
            "      - 73\n",
            "      - 23\n",
            "      - 24\n",
            "      - 58\n",
            "      - 50\n",
            "      - 26\n",
            "      - 98\n",
            "      - 115\n",
            "      - 57\n",
            "      - 18\n",
            "      - 19\n",
            "      - 46\n",
            "      - 23\n",
            "      - 67\n",
            "      - 61\n",
            "      - 114\n",
            "      - 38\n",
            "      - 106\n",
            "      - 174\n",
            "      - 111\n",
            "      - 46\n",
            "      - 143\n",
            "      - 30\n",
            "      - 15\n",
            "      - 165\n",
            "      - 31\n",
            "      - 74\n",
            "      - 74\n",
            "      - 48\n",
            "      - 26\n",
            "      - 43\n",
            "      - 124\n",
            "      - 30\n",
            "      - 24\n",
            "      - 33\n",
            "      - 57\n",
            "      - 32\n",
            "      - 24\n",
            "      - 132\n",
            "      - 88\n",
            "      - 45\n",
            "      - 85\n",
            "      - 29\n",
            "      - 122\n",
            "      - 143\n",
            "      - 132\n",
            "      - 36\n",
            "      - 49\n",
            "      - 29\n",
            "      - 80\n",
            "      - 141\n",
            "      - 200\n",
            "      - 52\n",
            "      - 116\n",
            "      - 124\n",
            "      - 35\n",
            "      - 93\n",
            "      - 63\n",
            "      - 118\n",
            "      - 31\n",
            "      - 29\n",
            "      - 101\n",
            "      - 26\n",
            "      - 65\n",
            "      - 108\n",
            "      - 64\n",
            "      - 19\n",
            "      - 119\n",
            "      episode_reward:\n",
            "      - 84.0\n",
            "      - 50.0\n",
            "      - 101.0\n",
            "      - 18.0\n",
            "      - 21.0\n",
            "      - 61.0\n",
            "      - 27.0\n",
            "      - 71.0\n",
            "      - 42.0\n",
            "      - 79.0\n",
            "      - 107.0\n",
            "      - 139.0\n",
            "      - 51.0\n",
            "      - 40.0\n",
            "      - 27.0\n",
            "      - 67.0\n",
            "      - 21.0\n",
            "      - 34.0\n",
            "      - 89.0\n",
            "      - 130.0\n",
            "      - 18.0\n",
            "      - 21.0\n",
            "      - 89.0\n",
            "      - 48.0\n",
            "      - 37.0\n",
            "      - 103.0\n",
            "      - 55.0\n",
            "      - 121.0\n",
            "      - 23.0\n",
            "      - 43.0\n",
            "      - 105.0\n",
            "      - 109.0\n",
            "      - 73.0\n",
            "      - 23.0\n",
            "      - 24.0\n",
            "      - 58.0\n",
            "      - 50.0\n",
            "      - 26.0\n",
            "      - 98.0\n",
            "      - 115.0\n",
            "      - 57.0\n",
            "      - 18.0\n",
            "      - 19.0\n",
            "      - 46.0\n",
            "      - 23.0\n",
            "      - 67.0\n",
            "      - 61.0\n",
            "      - 114.0\n",
            "      - 38.0\n",
            "      - 106.0\n",
            "      - 174.0\n",
            "      - 111.0\n",
            "      - 46.0\n",
            "      - 143.0\n",
            "      - 30.0\n",
            "      - 15.0\n",
            "      - 165.0\n",
            "      - 31.0\n",
            "      - 74.0\n",
            "      - 74.0\n",
            "      - 48.0\n",
            "      - 26.0\n",
            "      - 43.0\n",
            "      - 124.0\n",
            "      - 30.0\n",
            "      - 24.0\n",
            "      - 33.0\n",
            "      - 57.0\n",
            "      - 32.0\n",
            "      - 24.0\n",
            "      - 132.0\n",
            "      - 88.0\n",
            "      - 45.0\n",
            "      - 85.0\n",
            "      - 29.0\n",
            "      - 122.0\n",
            "      - 143.0\n",
            "      - 132.0\n",
            "      - 36.0\n",
            "      - 49.0\n",
            "      - 29.0\n",
            "      - 80.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 116.0\n",
            "      - 124.0\n",
            "      - 35.0\n",
            "      - 93.0\n",
            "      - 63.0\n",
            "      - 118.0\n",
            "      - 31.0\n",
            "      - 29.0\n",
            "      - 101.0\n",
            "      - 26.0\n",
            "      - 65.0\n",
            "      - 108.0\n",
            "      - 64.0\n",
            "      - 19.0\n",
            "      - 119.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13782870859653215\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10817293768426918\n",
            "      mean_inference_ms: 1.3154271669365876\n",
            "      mean_raw_obs_processing_ms: 0.24955810666735928\n",
            "  time_since_restore: 431.90846037864685\n",
            "  time_this_iter_s: 11.933327674865723\n",
            "  time_total_s: 431.90846037864685\n",
            "  timers:\n",
            "    learn_throughput: 32701.957\n",
            "    learn_time_ms: 6.116\n",
            "    load_throughput: 1063734.213\n",
            "    load_time_ms: 0.188\n",
            "    training_iteration_time_ms: 216.809\n",
            "    update_time_ms: 4.256\n",
            "  timestamp: 1656954303\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 205200\n",
            "  training_iteration: 40\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:08 (running for 00:07:37.83)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         431.908</td><td style=\"text-align: right;\">205200</td><td style=\"text-align: right;\">   68.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">             68.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:13 (running for 00:07:42.94)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         431.908</td><td style=\"text-align: right;\">205200</td><td style=\"text-align: right;\">   68.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">             68.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 214800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 214800\n",
            "    num_agent_steps_trained: 214800\n",
            "    num_env_steps_sampled: 214800\n",
            "    num_env_steps_trained: 214800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-05-13\n",
            "  done: false\n",
            "  episode_len_mean: 80.29411764705883\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 80.29411764705883\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 119\n",
            "  episodes_total: 2593\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 125.2105712890625\n",
            "          policy_loss: 741.5333251953125\n",
            "          var_gnorm: 25.439327239990234\n",
            "          vf_explained_var: 0.15507984161376953\n",
            "          vf_loss: 7172.611328125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 214800\n",
            "    num_agent_steps_trained: 214800\n",
            "    num_env_steps_sampled: 214800\n",
            "    num_env_steps_trained: 214800\n",
            "  iterations_since_restore: 41\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 214800\n",
            "  num_agent_steps_trained: 214800\n",
            "  num_env_steps_sampled: 214800\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 214800\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.75333333333334\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13828596226282536\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10799235486016537\n",
            "    mean_inference_ms: 1.3168539276627567\n",
            "    mean_raw_obs_processing_ms: 0.24856997906278708\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 80.29411764705883\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 80.29411764705883\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 119\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 152\n",
            "      - 67\n",
            "      - 66\n",
            "      - 152\n",
            "      - 130\n",
            "      - 115\n",
            "      - 21\n",
            "      - 73\n",
            "      - 92\n",
            "      - 150\n",
            "      - 35\n",
            "      - 47\n",
            "      - 52\n",
            "      - 86\n",
            "      - 18\n",
            "      - 115\n",
            "      - 16\n",
            "      - 21\n",
            "      - 70\n",
            "      - 57\n",
            "      - 177\n",
            "      - 75\n",
            "      - 139\n",
            "      - 119\n",
            "      - 57\n",
            "      - 33\n",
            "      - 138\n",
            "      - 25\n",
            "      - 106\n",
            "      - 22\n",
            "      - 200\n",
            "      - 12\n",
            "      - 55\n",
            "      - 56\n",
            "      - 100\n",
            "      - 165\n",
            "      - 63\n",
            "      - 72\n",
            "      - 73\n",
            "      - 14\n",
            "      - 96\n",
            "      - 50\n",
            "      - 30\n",
            "      - 31\n",
            "      - 51\n",
            "      - 98\n",
            "      - 74\n",
            "      - 19\n",
            "      - 163\n",
            "      - 60\n",
            "      - 153\n",
            "      - 157\n",
            "      - 28\n",
            "      - 79\n",
            "      - 42\n",
            "      - 44\n",
            "      - 92\n",
            "      - 121\n",
            "      - 93\n",
            "      - 125\n",
            "      - 16\n",
            "      - 33\n",
            "      - 123\n",
            "      - 156\n",
            "      - 89\n",
            "      - 138\n",
            "      - 147\n",
            "      - 48\n",
            "      - 31\n",
            "      - 156\n",
            "      - 44\n",
            "      - 72\n",
            "      - 58\n",
            "      - 88\n",
            "      - 150\n",
            "      - 179\n",
            "      - 35\n",
            "      - 183\n",
            "      - 34\n",
            "      - 97\n",
            "      - 30\n",
            "      - 54\n",
            "      - 32\n",
            "      - 27\n",
            "      - 81\n",
            "      - 24\n",
            "      - 65\n",
            "      - 21\n",
            "      - 125\n",
            "      - 98\n",
            "      - 14\n",
            "      - 11\n",
            "      - 95\n",
            "      - 63\n",
            "      - 52\n",
            "      - 67\n",
            "      - 147\n",
            "      - 143\n",
            "      - 145\n",
            "      - 133\n",
            "      - 151\n",
            "      - 83\n",
            "      - 34\n",
            "      - 133\n",
            "      - 64\n",
            "      - 28\n",
            "      - 32\n",
            "      - 25\n",
            "      - 101\n",
            "      - 91\n",
            "      - 16\n",
            "      - 100\n",
            "      - 89\n",
            "      - 63\n",
            "      - 90\n",
            "      - 59\n",
            "      - 94\n",
            "      - 69\n",
            "      - 87\n",
            "      episode_reward:\n",
            "      - 152.0\n",
            "      - 67.0\n",
            "      - 66.0\n",
            "      - 152.0\n",
            "      - 130.0\n",
            "      - 115.0\n",
            "      - 21.0\n",
            "      - 73.0\n",
            "      - 92.0\n",
            "      - 150.0\n",
            "      - 35.0\n",
            "      - 47.0\n",
            "      - 52.0\n",
            "      - 86.0\n",
            "      - 18.0\n",
            "      - 115.0\n",
            "      - 16.0\n",
            "      - 21.0\n",
            "      - 70.0\n",
            "      - 57.0\n",
            "      - 177.0\n",
            "      - 75.0\n",
            "      - 139.0\n",
            "      - 119.0\n",
            "      - 57.0\n",
            "      - 33.0\n",
            "      - 138.0\n",
            "      - 25.0\n",
            "      - 106.0\n",
            "      - 22.0\n",
            "      - 200.0\n",
            "      - 12.0\n",
            "      - 55.0\n",
            "      - 56.0\n",
            "      - 100.0\n",
            "      - 165.0\n",
            "      - 63.0\n",
            "      - 72.0\n",
            "      - 73.0\n",
            "      - 14.0\n",
            "      - 96.0\n",
            "      - 50.0\n",
            "      - 30.0\n",
            "      - 31.0\n",
            "      - 51.0\n",
            "      - 98.0\n",
            "      - 74.0\n",
            "      - 19.0\n",
            "      - 163.0\n",
            "      - 60.0\n",
            "      - 153.0\n",
            "      - 157.0\n",
            "      - 28.0\n",
            "      - 79.0\n",
            "      - 42.0\n",
            "      - 44.0\n",
            "      - 92.0\n",
            "      - 121.0\n",
            "      - 93.0\n",
            "      - 125.0\n",
            "      - 16.0\n",
            "      - 33.0\n",
            "      - 123.0\n",
            "      - 156.0\n",
            "      - 89.0\n",
            "      - 138.0\n",
            "      - 147.0\n",
            "      - 48.0\n",
            "      - 31.0\n",
            "      - 156.0\n",
            "      - 44.0\n",
            "      - 72.0\n",
            "      - 58.0\n",
            "      - 88.0\n",
            "      - 150.0\n",
            "      - 179.0\n",
            "      - 35.0\n",
            "      - 183.0\n",
            "      - 34.0\n",
            "      - 97.0\n",
            "      - 30.0\n",
            "      - 54.0\n",
            "      - 32.0\n",
            "      - 27.0\n",
            "      - 81.0\n",
            "      - 24.0\n",
            "      - 65.0\n",
            "      - 21.0\n",
            "      - 125.0\n",
            "      - 98.0\n",
            "      - 14.0\n",
            "      - 11.0\n",
            "      - 95.0\n",
            "      - 63.0\n",
            "      - 52.0\n",
            "      - 67.0\n",
            "      - 147.0\n",
            "      - 143.0\n",
            "      - 145.0\n",
            "      - 133.0\n",
            "      - 151.0\n",
            "      - 83.0\n",
            "      - 34.0\n",
            "      - 133.0\n",
            "      - 64.0\n",
            "      - 28.0\n",
            "      - 32.0\n",
            "      - 25.0\n",
            "      - 101.0\n",
            "      - 91.0\n",
            "      - 16.0\n",
            "      - 100.0\n",
            "      - 89.0\n",
            "      - 63.0\n",
            "      - 90.0\n",
            "      - 59.0\n",
            "      - 94.0\n",
            "      - 69.0\n",
            "      - 87.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13828596226282536\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10799235486016537\n",
            "      mean_inference_ms: 1.3168539276627567\n",
            "      mean_raw_obs_processing_ms: 0.24856997906278708\n",
            "  time_since_restore: 442.12306356430054\n",
            "  time_this_iter_s: 10.214603185653687\n",
            "  time_total_s: 442.12306356430054\n",
            "  timers:\n",
            "    learn_throughput: 34290.022\n",
            "    learn_time_ms: 5.833\n",
            "    load_throughput: 1032317.007\n",
            "    load_time_ms: 0.194\n",
            "    training_iteration_time_ms: 210.126\n",
            "    update_time_ms: 3.601\n",
            "  timestamp: 1656954313\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 214800\n",
            "  training_iteration: 41\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:18 (running for 00:07:48.08)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         442.123</td><td style=\"text-align: right;\">214800</td><td style=\"text-align: right;\"> 80.2941</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">           80.2941</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:23 (running for 00:07:53.17)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         442.123</td><td style=\"text-align: right;\">214800</td><td style=\"text-align: right;\"> 80.2941</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">           80.2941</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 215800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 215800\n",
            "    num_agent_steps_trained: 215800\n",
            "    num_env_steps_sampled: 215800\n",
            "    num_env_steps_trained: 215800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-05-24\n",
            "  done: false\n",
            "  episode_len_mean: 77.2\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 183.0\n",
            "  episode_reward_mean: 77.2\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 15\n",
            "  episodes_total: 2608\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 70.7\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 70.7\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 31\n",
            "      - 25\n",
            "      - 97\n",
            "      - 200\n",
            "      - 15\n",
            "      - 126\n",
            "      - 41\n",
            "      - 52\n",
            "      - 54\n",
            "      - 69\n",
            "      - 49\n",
            "      - 14\n",
            "      - 27\n",
            "      - 85\n",
            "      - 22\n",
            "      - 54\n",
            "      - 39\n",
            "      - 160\n",
            "      - 143\n",
            "      - 111\n",
            "      episode_reward:\n",
            "      - 31.0\n",
            "      - 25.0\n",
            "      - 97.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 126.0\n",
            "      - 41.0\n",
            "      - 52.0\n",
            "      - 54.0\n",
            "      - 69.0\n",
            "      - 49.0\n",
            "      - 14.0\n",
            "      - 27.0\n",
            "      - 85.0\n",
            "      - 22.0\n",
            "      - 54.0\n",
            "      - 39.0\n",
            "      - 160.0\n",
            "      - 143.0\n",
            "      - 111.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09979094071821734\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07628358598188918\n",
            "      mean_inference_ms: 0.9724483711936257\n",
            "      mean_raw_obs_processing_ms: 0.1069208263050426\n",
            "    timesteps_this_iter: 1414\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 129.22354125976562\n",
            "          policy_loss: 858.3687744140625\n",
            "          var_gnorm: 25.45795249938965\n",
            "          vf_explained_var: 0.06319421529769897\n",
            "          vf_loss: 7927.7431640625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 215800\n",
            "    num_agent_steps_trained: 215800\n",
            "    num_env_steps_sampled: 215800\n",
            "    num_env_steps_trained: 215800\n",
            "  iterations_since_restore: 42\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 215800\n",
            "  num_agent_steps_trained: 215800\n",
            "  num_env_steps_sampled: 215800\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 215800\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.95333333333332\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13797039241796205\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10807729666102688\n",
            "    mean_inference_ms: 1.3154457539571005\n",
            "    mean_raw_obs_processing_ms: 0.24901766082819918\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 77.2\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 183.0\n",
            "    episode_reward_mean: 77.2\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 15\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 100\n",
            "      - 165\n",
            "      - 63\n",
            "      - 72\n",
            "      - 73\n",
            "      - 14\n",
            "      - 96\n",
            "      - 50\n",
            "      - 30\n",
            "      - 31\n",
            "      - 51\n",
            "      - 98\n",
            "      - 74\n",
            "      - 19\n",
            "      - 163\n",
            "      - 60\n",
            "      - 153\n",
            "      - 157\n",
            "      - 28\n",
            "      - 79\n",
            "      - 42\n",
            "      - 44\n",
            "      - 92\n",
            "      - 121\n",
            "      - 93\n",
            "      - 125\n",
            "      - 16\n",
            "      - 33\n",
            "      - 123\n",
            "      - 156\n",
            "      - 89\n",
            "      - 138\n",
            "      - 147\n",
            "      - 48\n",
            "      - 31\n",
            "      - 156\n",
            "      - 44\n",
            "      - 72\n",
            "      - 58\n",
            "      - 88\n",
            "      - 150\n",
            "      - 179\n",
            "      - 35\n",
            "      - 183\n",
            "      - 34\n",
            "      - 97\n",
            "      - 30\n",
            "      - 54\n",
            "      - 32\n",
            "      - 27\n",
            "      - 81\n",
            "      - 24\n",
            "      - 65\n",
            "      - 21\n",
            "      - 125\n",
            "      - 98\n",
            "      - 14\n",
            "      - 11\n",
            "      - 95\n",
            "      - 63\n",
            "      - 52\n",
            "      - 67\n",
            "      - 147\n",
            "      - 143\n",
            "      - 145\n",
            "      - 133\n",
            "      - 151\n",
            "      - 83\n",
            "      - 34\n",
            "      - 133\n",
            "      - 64\n",
            "      - 28\n",
            "      - 32\n",
            "      - 25\n",
            "      - 101\n",
            "      - 91\n",
            "      - 16\n",
            "      - 100\n",
            "      - 89\n",
            "      - 63\n",
            "      - 90\n",
            "      - 59\n",
            "      - 94\n",
            "      - 69\n",
            "      - 87\n",
            "      - 101\n",
            "      - 77\n",
            "      - 19\n",
            "      - 139\n",
            "      - 27\n",
            "      - 40\n",
            "      - 75\n",
            "      - 76\n",
            "      - 54\n",
            "      - 74\n",
            "      - 23\n",
            "      - 16\n",
            "      - 90\n",
            "      - 52\n",
            "      - 51\n",
            "      episode_reward:\n",
            "      - 100.0\n",
            "      - 165.0\n",
            "      - 63.0\n",
            "      - 72.0\n",
            "      - 73.0\n",
            "      - 14.0\n",
            "      - 96.0\n",
            "      - 50.0\n",
            "      - 30.0\n",
            "      - 31.0\n",
            "      - 51.0\n",
            "      - 98.0\n",
            "      - 74.0\n",
            "      - 19.0\n",
            "      - 163.0\n",
            "      - 60.0\n",
            "      - 153.0\n",
            "      - 157.0\n",
            "      - 28.0\n",
            "      - 79.0\n",
            "      - 42.0\n",
            "      - 44.0\n",
            "      - 92.0\n",
            "      - 121.0\n",
            "      - 93.0\n",
            "      - 125.0\n",
            "      - 16.0\n",
            "      - 33.0\n",
            "      - 123.0\n",
            "      - 156.0\n",
            "      - 89.0\n",
            "      - 138.0\n",
            "      - 147.0\n",
            "      - 48.0\n",
            "      - 31.0\n",
            "      - 156.0\n",
            "      - 44.0\n",
            "      - 72.0\n",
            "      - 58.0\n",
            "      - 88.0\n",
            "      - 150.0\n",
            "      - 179.0\n",
            "      - 35.0\n",
            "      - 183.0\n",
            "      - 34.0\n",
            "      - 97.0\n",
            "      - 30.0\n",
            "      - 54.0\n",
            "      - 32.0\n",
            "      - 27.0\n",
            "      - 81.0\n",
            "      - 24.0\n",
            "      - 65.0\n",
            "      - 21.0\n",
            "      - 125.0\n",
            "      - 98.0\n",
            "      - 14.0\n",
            "      - 11.0\n",
            "      - 95.0\n",
            "      - 63.0\n",
            "      - 52.0\n",
            "      - 67.0\n",
            "      - 147.0\n",
            "      - 143.0\n",
            "      - 145.0\n",
            "      - 133.0\n",
            "      - 151.0\n",
            "      - 83.0\n",
            "      - 34.0\n",
            "      - 133.0\n",
            "      - 64.0\n",
            "      - 28.0\n",
            "      - 32.0\n",
            "      - 25.0\n",
            "      - 101.0\n",
            "      - 91.0\n",
            "      - 16.0\n",
            "      - 100.0\n",
            "      - 89.0\n",
            "      - 63.0\n",
            "      - 90.0\n",
            "      - 59.0\n",
            "      - 94.0\n",
            "      - 69.0\n",
            "      - 87.0\n",
            "      - 101.0\n",
            "      - 77.0\n",
            "      - 19.0\n",
            "      - 139.0\n",
            "      - 27.0\n",
            "      - 40.0\n",
            "      - 75.0\n",
            "      - 76.0\n",
            "      - 54.0\n",
            "      - 74.0\n",
            "      - 23.0\n",
            "      - 16.0\n",
            "      - 90.0\n",
            "      - 52.0\n",
            "      - 51.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13797039241796205\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10807729666102688\n",
            "      mean_inference_ms: 1.3154457539571005\n",
            "      mean_raw_obs_processing_ms: 0.24901766082819918\n",
            "  time_since_restore: 452.53898000717163\n",
            "  time_this_iter_s: 10.415916442871094\n",
            "  time_total_s: 452.53898000717163\n",
            "  timers:\n",
            "    learn_throughput: 33266.741\n",
            "    learn_time_ms: 6.012\n",
            "    load_throughput: 1057698.651\n",
            "    load_time_ms: 0.189\n",
            "    training_iteration_time_ms: 213.643\n",
            "    update_time_ms: 3.566\n",
            "  timestamp: 1656954324\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 215800\n",
            "  training_iteration: 42\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:29 (running for 00:07:58.54)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         452.539</td><td style=\"text-align: right;\">215800</td><td style=\"text-align: right;\">    77.2</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">              77.2</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:34 (running for 00:08:03.67)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         452.539</td><td style=\"text-align: right;\">215800</td><td style=\"text-align: right;\">    77.2</td><td style=\"text-align: right;\">                 183</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">              77.2</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 225400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 225400\n",
            "    num_agent_steps_trained: 225400\n",
            "    num_env_steps_sampled: 225400\n",
            "    num_env_steps_trained: 225400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-05-34\n",
            "  done: false\n",
            "  episode_len_mean: 78.62096774193549\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 78.62096774193549\n",
            "  episode_reward_min: 11.0\n",
            "  episodes_this_iter: 124\n",
            "  episodes_total: 2732\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 125.36703491210938\n",
            "          policy_loss: 277.15484619140625\n",
            "          var_gnorm: 25.626392364501953\n",
            "          vf_explained_var: 0.06621968746185303\n",
            "          vf_loss: 9745.509765625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 225400\n",
            "    num_agent_steps_trained: 225400\n",
            "    num_env_steps_sampled: 225400\n",
            "    num_env_steps_trained: 225400\n",
            "  iterations_since_restore: 43\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 225400\n",
            "  num_agent_steps_trained: 225400\n",
            "  num_env_steps_sampled: 225400\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 225400\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.79285714285716\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13822364684552932\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10793581831882976\n",
            "    mean_inference_ms: 1.3154050651451892\n",
            "    mean_raw_obs_processing_ms: 0.24841669661108975\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 78.62096774193549\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 78.62096774193549\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 124\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 127\n",
            "      - 23\n",
            "      - 109\n",
            "      - 107\n",
            "      - 39\n",
            "      - 35\n",
            "      - 78\n",
            "      - 35\n",
            "      - 26\n",
            "      - 56\n",
            "      - 79\n",
            "      - 12\n",
            "      - 29\n",
            "      - 102\n",
            "      - 200\n",
            "      - 61\n",
            "      - 178\n",
            "      - 140\n",
            "      - 17\n",
            "      - 126\n",
            "      - 110\n",
            "      - 31\n",
            "      - 47\n",
            "      - 76\n",
            "      - 20\n",
            "      - 43\n",
            "      - 52\n",
            "      - 42\n",
            "      - 19\n",
            "      - 11\n",
            "      - 141\n",
            "      - 200\n",
            "      - 39\n",
            "      - 67\n",
            "      - 112\n",
            "      - 149\n",
            "      - 20\n",
            "      - 128\n",
            "      - 59\n",
            "      - 78\n",
            "      - 42\n",
            "      - 106\n",
            "      - 179\n",
            "      - 99\n",
            "      - 152\n",
            "      - 22\n",
            "      - 43\n",
            "      - 18\n",
            "      - 62\n",
            "      - 20\n",
            "      - 109\n",
            "      - 47\n",
            "      - 29\n",
            "      - 147\n",
            "      - 79\n",
            "      - 114\n",
            "      - 157\n",
            "      - 89\n",
            "      - 200\n",
            "      - 63\n",
            "      - 76\n",
            "      - 122\n",
            "      - 36\n",
            "      - 32\n",
            "      - 43\n",
            "      - 36\n",
            "      - 187\n",
            "      - 62\n",
            "      - 96\n",
            "      - 200\n",
            "      - 47\n",
            "      - 60\n",
            "      - 110\n",
            "      - 60\n",
            "      - 56\n",
            "      - 35\n",
            "      - 182\n",
            "      - 46\n",
            "      - 56\n",
            "      - 41\n",
            "      - 200\n",
            "      - 18\n",
            "      - 93\n",
            "      - 153\n",
            "      - 15\n",
            "      - 71\n",
            "      - 16\n",
            "      - 64\n",
            "      - 73\n",
            "      - 59\n",
            "      - 31\n",
            "      - 51\n",
            "      - 27\n",
            "      - 36\n",
            "      - 37\n",
            "      - 44\n",
            "      - 200\n",
            "      - 39\n",
            "      - 38\n",
            "      - 146\n",
            "      - 81\n",
            "      - 38\n",
            "      - 162\n",
            "      - 57\n",
            "      - 46\n",
            "      - 79\n",
            "      - 53\n",
            "      - 59\n",
            "      - 112\n",
            "      - 38\n",
            "      - 49\n",
            "      - 200\n",
            "      - 34\n",
            "      - 64\n",
            "      - 47\n",
            "      - 133\n",
            "      - 159\n",
            "      - 21\n",
            "      - 81\n",
            "      - 112\n",
            "      - 116\n",
            "      - 52\n",
            "      - 140\n",
            "      - 22\n",
            "      episode_reward:\n",
            "      - 127.0\n",
            "      - 23.0\n",
            "      - 109.0\n",
            "      - 107.0\n",
            "      - 39.0\n",
            "      - 35.0\n",
            "      - 78.0\n",
            "      - 35.0\n",
            "      - 26.0\n",
            "      - 56.0\n",
            "      - 79.0\n",
            "      - 12.0\n",
            "      - 29.0\n",
            "      - 102.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 178.0\n",
            "      - 140.0\n",
            "      - 17.0\n",
            "      - 126.0\n",
            "      - 110.0\n",
            "      - 31.0\n",
            "      - 47.0\n",
            "      - 76.0\n",
            "      - 20.0\n",
            "      - 43.0\n",
            "      - 52.0\n",
            "      - 42.0\n",
            "      - 19.0\n",
            "      - 11.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 39.0\n",
            "      - 67.0\n",
            "      - 112.0\n",
            "      - 149.0\n",
            "      - 20.0\n",
            "      - 128.0\n",
            "      - 59.0\n",
            "      - 78.0\n",
            "      - 42.0\n",
            "      - 106.0\n",
            "      - 179.0\n",
            "      - 99.0\n",
            "      - 152.0\n",
            "      - 22.0\n",
            "      - 43.0\n",
            "      - 18.0\n",
            "      - 62.0\n",
            "      - 20.0\n",
            "      - 109.0\n",
            "      - 47.0\n",
            "      - 29.0\n",
            "      - 147.0\n",
            "      - 79.0\n",
            "      - 114.0\n",
            "      - 157.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 63.0\n",
            "      - 76.0\n",
            "      - 122.0\n",
            "      - 36.0\n",
            "      - 32.0\n",
            "      - 43.0\n",
            "      - 36.0\n",
            "      - 187.0\n",
            "      - 62.0\n",
            "      - 96.0\n",
            "      - 200.0\n",
            "      - 47.0\n",
            "      - 60.0\n",
            "      - 110.0\n",
            "      - 60.0\n",
            "      - 56.0\n",
            "      - 35.0\n",
            "      - 182.0\n",
            "      - 46.0\n",
            "      - 56.0\n",
            "      - 41.0\n",
            "      - 200.0\n",
            "      - 18.0\n",
            "      - 93.0\n",
            "      - 153.0\n",
            "      - 15.0\n",
            "      - 71.0\n",
            "      - 16.0\n",
            "      - 64.0\n",
            "      - 73.0\n",
            "      - 59.0\n",
            "      - 31.0\n",
            "      - 51.0\n",
            "      - 27.0\n",
            "      - 36.0\n",
            "      - 37.0\n",
            "      - 44.0\n",
            "      - 200.0\n",
            "      - 39.0\n",
            "      - 38.0\n",
            "      - 146.0\n",
            "      - 81.0\n",
            "      - 38.0\n",
            "      - 162.0\n",
            "      - 57.0\n",
            "      - 46.0\n",
            "      - 79.0\n",
            "      - 53.0\n",
            "      - 59.0\n",
            "      - 112.0\n",
            "      - 38.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 34.0\n",
            "      - 64.0\n",
            "      - 47.0\n",
            "      - 133.0\n",
            "      - 159.0\n",
            "      - 21.0\n",
            "      - 81.0\n",
            "      - 112.0\n",
            "      - 116.0\n",
            "      - 52.0\n",
            "      - 140.0\n",
            "      - 22.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13822364684552932\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10793581831882976\n",
            "      mean_inference_ms: 1.3154050651451892\n",
            "      mean_raw_obs_processing_ms: 0.24841669661108975\n",
            "  time_since_restore: 462.7357704639435\n",
            "  time_this_iter_s: 10.19679045677185\n",
            "  time_total_s: 462.7357704639435\n",
            "  timers:\n",
            "    learn_throughput: 31107.284\n",
            "    learn_time_ms: 6.429\n",
            "    load_throughput: 1071342.018\n",
            "    load_time_ms: 0.187\n",
            "    training_iteration_time_ms: 209.181\n",
            "    update_time_ms: 3.431\n",
            "  timestamp: 1656954334\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 225400\n",
            "  training_iteration: 43\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:39 (running for 00:08:08.76)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         462.736</td><td style=\"text-align: right;\">225400</td><td style=\"text-align: right;\">  78.621</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            78.621</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:44 (running for 00:08:13.86)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         462.736</td><td style=\"text-align: right;\">225400</td><td style=\"text-align: right;\">  78.621</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  11</td><td style=\"text-align: right;\">            78.621</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 226400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 226400\n",
            "    num_agent_steps_trained: 226400\n",
            "    num_env_steps_sampled: 226400\n",
            "    num_env_steps_trained: 226400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-05-45\n",
            "  done: false\n",
            "  episode_len_mean: 80.84\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 80.84\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 11\n",
            "  episodes_total: 2743\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 61.05\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 130.0\n",
            "    episode_reward_mean: 61.05\n",
            "    episode_reward_min: 20.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 25\n",
            "      - 49\n",
            "      - 30\n",
            "      - 22\n",
            "      - 83\n",
            "      - 68\n",
            "      - 72\n",
            "      - 84\n",
            "      - 27\n",
            "      - 47\n",
            "      - 114\n",
            "      - 47\n",
            "      - 54\n",
            "      - 111\n",
            "      - 78\n",
            "      - 42\n",
            "      - 72\n",
            "      - 46\n",
            "      - 20\n",
            "      - 130\n",
            "      episode_reward:\n",
            "      - 25.0\n",
            "      - 49.0\n",
            "      - 30.0\n",
            "      - 22.0\n",
            "      - 83.0\n",
            "      - 68.0\n",
            "      - 72.0\n",
            "      - 84.0\n",
            "      - 27.0\n",
            "      - 47.0\n",
            "      - 114.0\n",
            "      - 47.0\n",
            "      - 54.0\n",
            "      - 111.0\n",
            "      - 78.0\n",
            "      - 42.0\n",
            "      - 72.0\n",
            "      - 46.0\n",
            "      - 20.0\n",
            "      - 130.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.09978515082787758\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07627674511405999\n",
            "      mean_inference_ms: 0.971833344976553\n",
            "      mean_raw_obs_processing_ms: 0.10695735760997234\n",
            "    timesteps_this_iter: 1221\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 125.53892517089844\n",
            "          policy_loss: 187.05996704101562\n",
            "          var_gnorm: 25.644245147705078\n",
            "          vf_explained_var: 0.08391386270523071\n",
            "          vf_loss: 9526.357421875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 226400\n",
            "    num_agent_steps_trained: 226400\n",
            "    num_env_steps_sampled: 226400\n",
            "    num_env_steps_trained: 226400\n",
            "  iterations_since_restore: 44\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 226400\n",
            "  num_agent_steps_trained: 226400\n",
            "  num_env_steps_sampled: 226400\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 226400\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 67.35625\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13794083576986416\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10800274750874546\n",
            "    mean_inference_ms: 1.3139819459541409\n",
            "    mean_raw_obs_processing_ms: 0.24888077840777278\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 80.84\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 80.84\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 11\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 149\n",
            "      - 20\n",
            "      - 128\n",
            "      - 59\n",
            "      - 78\n",
            "      - 42\n",
            "      - 106\n",
            "      - 179\n",
            "      - 99\n",
            "      - 152\n",
            "      - 22\n",
            "      - 43\n",
            "      - 18\n",
            "      - 62\n",
            "      - 20\n",
            "      - 109\n",
            "      - 47\n",
            "      - 29\n",
            "      - 147\n",
            "      - 79\n",
            "      - 114\n",
            "      - 157\n",
            "      - 89\n",
            "      - 200\n",
            "      - 63\n",
            "      - 76\n",
            "      - 122\n",
            "      - 36\n",
            "      - 32\n",
            "      - 43\n",
            "      - 36\n",
            "      - 187\n",
            "      - 62\n",
            "      - 96\n",
            "      - 200\n",
            "      - 47\n",
            "      - 60\n",
            "      - 110\n",
            "      - 60\n",
            "      - 56\n",
            "      - 35\n",
            "      - 182\n",
            "      - 46\n",
            "      - 56\n",
            "      - 41\n",
            "      - 200\n",
            "      - 18\n",
            "      - 93\n",
            "      - 153\n",
            "      - 15\n",
            "      - 71\n",
            "      - 16\n",
            "      - 64\n",
            "      - 73\n",
            "      - 59\n",
            "      - 31\n",
            "      - 51\n",
            "      - 27\n",
            "      - 36\n",
            "      - 37\n",
            "      - 44\n",
            "      - 200\n",
            "      - 39\n",
            "      - 38\n",
            "      - 146\n",
            "      - 81\n",
            "      - 38\n",
            "      - 162\n",
            "      - 57\n",
            "      - 46\n",
            "      - 79\n",
            "      - 53\n",
            "      - 59\n",
            "      - 112\n",
            "      - 38\n",
            "      - 49\n",
            "      - 200\n",
            "      - 34\n",
            "      - 64\n",
            "      - 47\n",
            "      - 133\n",
            "      - 159\n",
            "      - 21\n",
            "      - 81\n",
            "      - 112\n",
            "      - 116\n",
            "      - 52\n",
            "      - 140\n",
            "      - 22\n",
            "      - 87\n",
            "      - 172\n",
            "      - 31\n",
            "      - 50\n",
            "      - 125\n",
            "      - 38\n",
            "      - 45\n",
            "      - 160\n",
            "      - 88\n",
            "      - 79\n",
            "      - 49\n",
            "      episode_reward:\n",
            "      - 149.0\n",
            "      - 20.0\n",
            "      - 128.0\n",
            "      - 59.0\n",
            "      - 78.0\n",
            "      - 42.0\n",
            "      - 106.0\n",
            "      - 179.0\n",
            "      - 99.0\n",
            "      - 152.0\n",
            "      - 22.0\n",
            "      - 43.0\n",
            "      - 18.0\n",
            "      - 62.0\n",
            "      - 20.0\n",
            "      - 109.0\n",
            "      - 47.0\n",
            "      - 29.0\n",
            "      - 147.0\n",
            "      - 79.0\n",
            "      - 114.0\n",
            "      - 157.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 63.0\n",
            "      - 76.0\n",
            "      - 122.0\n",
            "      - 36.0\n",
            "      - 32.0\n",
            "      - 43.0\n",
            "      - 36.0\n",
            "      - 187.0\n",
            "      - 62.0\n",
            "      - 96.0\n",
            "      - 200.0\n",
            "      - 47.0\n",
            "      - 60.0\n",
            "      - 110.0\n",
            "      - 60.0\n",
            "      - 56.0\n",
            "      - 35.0\n",
            "      - 182.0\n",
            "      - 46.0\n",
            "      - 56.0\n",
            "      - 41.0\n",
            "      - 200.0\n",
            "      - 18.0\n",
            "      - 93.0\n",
            "      - 153.0\n",
            "      - 15.0\n",
            "      - 71.0\n",
            "      - 16.0\n",
            "      - 64.0\n",
            "      - 73.0\n",
            "      - 59.0\n",
            "      - 31.0\n",
            "      - 51.0\n",
            "      - 27.0\n",
            "      - 36.0\n",
            "      - 37.0\n",
            "      - 44.0\n",
            "      - 200.0\n",
            "      - 39.0\n",
            "      - 38.0\n",
            "      - 146.0\n",
            "      - 81.0\n",
            "      - 38.0\n",
            "      - 162.0\n",
            "      - 57.0\n",
            "      - 46.0\n",
            "      - 79.0\n",
            "      - 53.0\n",
            "      - 59.0\n",
            "      - 112.0\n",
            "      - 38.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 34.0\n",
            "      - 64.0\n",
            "      - 47.0\n",
            "      - 133.0\n",
            "      - 159.0\n",
            "      - 21.0\n",
            "      - 81.0\n",
            "      - 112.0\n",
            "      - 116.0\n",
            "      - 52.0\n",
            "      - 140.0\n",
            "      - 22.0\n",
            "      - 87.0\n",
            "      - 172.0\n",
            "      - 31.0\n",
            "      - 50.0\n",
            "      - 125.0\n",
            "      - 38.0\n",
            "      - 45.0\n",
            "      - 160.0\n",
            "      - 88.0\n",
            "      - 79.0\n",
            "      - 49.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13794083576986416\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10800274750874546\n",
            "      mean_inference_ms: 1.3139819459541409\n",
            "      mean_raw_obs_processing_ms: 0.24888077840777278\n",
            "  time_since_restore: 473.5937645435333\n",
            "  time_this_iter_s: 10.857994079589844\n",
            "  time_total_s: 473.5937645435333\n",
            "  timers:\n",
            "    learn_throughput: 31910.408\n",
            "    learn_time_ms: 6.268\n",
            "    load_throughput: 1019643.612\n",
            "    load_time_ms: 0.196\n",
            "    training_iteration_time_ms: 211.938\n",
            "    update_time_ms: 3.468\n",
            "  timestamp: 1656954345\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 226400\n",
            "  training_iteration: 44\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:50 (running for 00:08:19.66)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         473.594</td><td style=\"text-align: right;\">226400</td><td style=\"text-align: right;\">   80.84</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">             80.84</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:05:55 (running for 00:08:24.78)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         473.594</td><td style=\"text-align: right;\">226400</td><td style=\"text-align: right;\">   80.84</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">             80.84</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 236000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 236000\n",
            "    num_agent_steps_trained: 236000\n",
            "    num_env_steps_sampled: 236000\n",
            "    num_env_steps_trained: 236000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-05-55\n",
            "  done: false\n",
            "  episode_len_mean: 80.50833333333334\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 80.50833333333334\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 120\n",
            "  episodes_total: 2863\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 122.37687683105469\n",
            "          policy_loss: 98.83683776855469\n",
            "          var_gnorm: 25.807680130004883\n",
            "          vf_explained_var: 0.12010210752487183\n",
            "          vf_loss: 10651.2958984375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 236000\n",
            "    num_agent_steps_trained: 236000\n",
            "    num_env_steps_sampled: 236000\n",
            "    num_env_steps_trained: 236000\n",
            "  iterations_since_restore: 45\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 236000\n",
            "  num_agent_steps_trained: 236000\n",
            "  num_env_steps_sampled: 236000\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 236000\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.56000000000002\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1382699195690544\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10796756795139209\n",
            "    mean_inference_ms: 1.3144580191747062\n",
            "    mean_raw_obs_processing_ms: 0.24812375043117615\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 80.50833333333334\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 80.50833333333334\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 120\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 128\n",
            "      - 105\n",
            "      - 35\n",
            "      - 83\n",
            "      - 108\n",
            "      - 200\n",
            "      - 27\n",
            "      - 123\n",
            "      - 21\n",
            "      - 125\n",
            "      - 73\n",
            "      - 54\n",
            "      - 20\n",
            "      - 20\n",
            "      - 40\n",
            "      - 80\n",
            "      - 19\n",
            "      - 156\n",
            "      - 120\n",
            "      - 199\n",
            "      - 30\n",
            "      - 25\n",
            "      - 58\n",
            "      - 38\n",
            "      - 29\n",
            "      - 17\n",
            "      - 95\n",
            "      - 24\n",
            "      - 86\n",
            "      - 94\n",
            "      - 95\n",
            "      - 120\n",
            "      - 47\n",
            "      - 127\n",
            "      - 59\n",
            "      - 26\n",
            "      - 151\n",
            "      - 123\n",
            "      - 42\n",
            "      - 95\n",
            "      - 200\n",
            "      - 131\n",
            "      - 151\n",
            "      - 106\n",
            "      - 78\n",
            "      - 33\n",
            "      - 66\n",
            "      - 67\n",
            "      - 24\n",
            "      - 65\n",
            "      - 22\n",
            "      - 170\n",
            "      - 32\n",
            "      - 79\n",
            "      - 57\n",
            "      - 84\n",
            "      - 80\n",
            "      - 169\n",
            "      - 74\n",
            "      - 16\n",
            "      - 131\n",
            "      - 36\n",
            "      - 26\n",
            "      - 107\n",
            "      - 78\n",
            "      - 31\n",
            "      - 157\n",
            "      - 51\n",
            "      - 53\n",
            "      - 88\n",
            "      - 12\n",
            "      - 119\n",
            "      - 20\n",
            "      - 88\n",
            "      - 179\n",
            "      - 66\n",
            "      - 25\n",
            "      - 20\n",
            "      - 113\n",
            "      - 73\n",
            "      - 148\n",
            "      - 62\n",
            "      - 177\n",
            "      - 84\n",
            "      - 58\n",
            "      - 32\n",
            "      - 38\n",
            "      - 133\n",
            "      - 75\n",
            "      - 42\n",
            "      - 55\n",
            "      - 38\n",
            "      - 49\n",
            "      - 124\n",
            "      - 137\n",
            "      - 142\n",
            "      - 49\n",
            "      - 38\n",
            "      - 173\n",
            "      - 32\n",
            "      - 31\n",
            "      - 34\n",
            "      - 62\n",
            "      - 64\n",
            "      - 196\n",
            "      - 44\n",
            "      - 142\n",
            "      - 73\n",
            "      - 50\n",
            "      - 133\n",
            "      - 151\n",
            "      - 42\n",
            "      - 73\n",
            "      - 200\n",
            "      - 63\n",
            "      - 81\n",
            "      - 71\n",
            "      - 65\n",
            "      - 66\n",
            "      - 40\n",
            "      episode_reward:\n",
            "      - 128.0\n",
            "      - 105.0\n",
            "      - 35.0\n",
            "      - 83.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 123.0\n",
            "      - 21.0\n",
            "      - 125.0\n",
            "      - 73.0\n",
            "      - 54.0\n",
            "      - 20.0\n",
            "      - 20.0\n",
            "      - 40.0\n",
            "      - 80.0\n",
            "      - 19.0\n",
            "      - 156.0\n",
            "      - 120.0\n",
            "      - 199.0\n",
            "      - 30.0\n",
            "      - 25.0\n",
            "      - 58.0\n",
            "      - 38.0\n",
            "      - 29.0\n",
            "      - 17.0\n",
            "      - 95.0\n",
            "      - 24.0\n",
            "      - 86.0\n",
            "      - 94.0\n",
            "      - 95.0\n",
            "      - 120.0\n",
            "      - 47.0\n",
            "      - 127.0\n",
            "      - 59.0\n",
            "      - 26.0\n",
            "      - 151.0\n",
            "      - 123.0\n",
            "      - 42.0\n",
            "      - 95.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 151.0\n",
            "      - 106.0\n",
            "      - 78.0\n",
            "      - 33.0\n",
            "      - 66.0\n",
            "      - 67.0\n",
            "      - 24.0\n",
            "      - 65.0\n",
            "      - 22.0\n",
            "      - 170.0\n",
            "      - 32.0\n",
            "      - 79.0\n",
            "      - 57.0\n",
            "      - 84.0\n",
            "      - 80.0\n",
            "      - 169.0\n",
            "      - 74.0\n",
            "      - 16.0\n",
            "      - 131.0\n",
            "      - 36.0\n",
            "      - 26.0\n",
            "      - 107.0\n",
            "      - 78.0\n",
            "      - 31.0\n",
            "      - 157.0\n",
            "      - 51.0\n",
            "      - 53.0\n",
            "      - 88.0\n",
            "      - 12.0\n",
            "      - 119.0\n",
            "      - 20.0\n",
            "      - 88.0\n",
            "      - 179.0\n",
            "      - 66.0\n",
            "      - 25.0\n",
            "      - 20.0\n",
            "      - 113.0\n",
            "      - 73.0\n",
            "      - 148.0\n",
            "      - 62.0\n",
            "      - 177.0\n",
            "      - 84.0\n",
            "      - 58.0\n",
            "      - 32.0\n",
            "      - 38.0\n",
            "      - 133.0\n",
            "      - 75.0\n",
            "      - 42.0\n",
            "      - 55.0\n",
            "      - 38.0\n",
            "      - 49.0\n",
            "      - 124.0\n",
            "      - 137.0\n",
            "      - 142.0\n",
            "      - 49.0\n",
            "      - 38.0\n",
            "      - 173.0\n",
            "      - 32.0\n",
            "      - 31.0\n",
            "      - 34.0\n",
            "      - 62.0\n",
            "      - 64.0\n",
            "      - 196.0\n",
            "      - 44.0\n",
            "      - 142.0\n",
            "      - 73.0\n",
            "      - 50.0\n",
            "      - 133.0\n",
            "      - 151.0\n",
            "      - 42.0\n",
            "      - 73.0\n",
            "      - 200.0\n",
            "      - 63.0\n",
            "      - 81.0\n",
            "      - 71.0\n",
            "      - 65.0\n",
            "      - 66.0\n",
            "      - 40.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1382699195690544\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10796756795139209\n",
            "      mean_inference_ms: 1.3144580191747062\n",
            "      mean_raw_obs_processing_ms: 0.24812375043117615\n",
            "  time_since_restore: 483.867413520813\n",
            "  time_this_iter_s: 10.273648977279663\n",
            "  time_total_s: 483.867413520813\n",
            "  timers:\n",
            "    learn_throughput: 32985.498\n",
            "    learn_time_ms: 6.063\n",
            "    load_throughput: 1004383.142\n",
            "    load_time_ms: 0.199\n",
            "    training_iteration_time_ms: 214.025\n",
            "    update_time_ms: 3.304\n",
            "  timestamp: 1656954355\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 236000\n",
            "  training_iteration: 45\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:00 (running for 00:08:29.99)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         483.867</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> 80.5083</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">           80.5083</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:05 (running for 00:08:35.09)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         483.867</td><td style=\"text-align: right;\">236000</td><td style=\"text-align: right;\"> 80.5083</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">           80.5083</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 236800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 236800\n",
            "    num_agent_steps_trained: 236800\n",
            "    num_env_steps_sampled: 236800\n",
            "    num_env_steps_trained: 236800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-06-06\n",
            "  done: false\n",
            "  episode_len_mean: 81.69\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 81.69\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 10\n",
            "  episodes_total: 2873\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 100.05\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 100.05\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 21\n",
            "      - 166\n",
            "      - 15\n",
            "      - 83\n",
            "      - 200\n",
            "      - 96\n",
            "      - 36\n",
            "      - 133\n",
            "      - 134\n",
            "      - 140\n",
            "      - 73\n",
            "      - 155\n",
            "      - 146\n",
            "      - 97\n",
            "      - 43\n",
            "      - 190\n",
            "      - 200\n",
            "      - 33\n",
            "      - 17\n",
            "      - 23\n",
            "      episode_reward:\n",
            "      - 21.0\n",
            "      - 166.0\n",
            "      - 15.0\n",
            "      - 83.0\n",
            "      - 200.0\n",
            "      - 96.0\n",
            "      - 36.0\n",
            "      - 133.0\n",
            "      - 134.0\n",
            "      - 140.0\n",
            "      - 73.0\n",
            "      - 155.0\n",
            "      - 146.0\n",
            "      - 97.0\n",
            "      - 43.0\n",
            "      - 190.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 17.0\n",
            "      - 23.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10020965399748454\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07667476268672464\n",
            "      mean_inference_ms: 0.9764777439452809\n",
            "      mean_raw_obs_processing_ms: 0.10743042645386851\n",
            "    timesteps_this_iter: 2001\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 123.37065124511719\n",
            "          policy_loss: 632.039306640625\n",
            "          var_gnorm: 25.820693969726562\n",
            "          vf_explained_var: 0.19208437204360962\n",
            "          vf_loss: 7639.8349609375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 236800\n",
            "    num_agent_steps_trained: 236800\n",
            "    num_env_steps_sampled: 236800\n",
            "    num_env_steps_trained: 236800\n",
            "  iterations_since_restore: 46\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 236800\n",
            "  num_agent_steps_trained: 236800\n",
            "  num_env_steps_sampled: 236800\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 236800\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 76.18124999999999\n",
            "    ram_util_percent: 21.625\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13805341307157637\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10804050878709168\n",
            "    mean_inference_ms: 1.3134669786433306\n",
            "    mean_raw_obs_processing_ms: 0.24854295722544006\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 81.69\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 81.69\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 10\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 95\n",
            "      - 120\n",
            "      - 47\n",
            "      - 127\n",
            "      - 59\n",
            "      - 26\n",
            "      - 151\n",
            "      - 123\n",
            "      - 42\n",
            "      - 95\n",
            "      - 200\n",
            "      - 131\n",
            "      - 151\n",
            "      - 106\n",
            "      - 78\n",
            "      - 33\n",
            "      - 66\n",
            "      - 67\n",
            "      - 24\n",
            "      - 65\n",
            "      - 22\n",
            "      - 170\n",
            "      - 32\n",
            "      - 79\n",
            "      - 57\n",
            "      - 84\n",
            "      - 80\n",
            "      - 169\n",
            "      - 74\n",
            "      - 16\n",
            "      - 131\n",
            "      - 36\n",
            "      - 26\n",
            "      - 107\n",
            "      - 78\n",
            "      - 31\n",
            "      - 157\n",
            "      - 51\n",
            "      - 53\n",
            "      - 88\n",
            "      - 12\n",
            "      - 119\n",
            "      - 20\n",
            "      - 88\n",
            "      - 179\n",
            "      - 66\n",
            "      - 25\n",
            "      - 20\n",
            "      - 113\n",
            "      - 73\n",
            "      - 148\n",
            "      - 62\n",
            "      - 177\n",
            "      - 84\n",
            "      - 58\n",
            "      - 32\n",
            "      - 38\n",
            "      - 133\n",
            "      - 75\n",
            "      - 42\n",
            "      - 55\n",
            "      - 38\n",
            "      - 49\n",
            "      - 124\n",
            "      - 137\n",
            "      - 142\n",
            "      - 49\n",
            "      - 38\n",
            "      - 173\n",
            "      - 32\n",
            "      - 31\n",
            "      - 34\n",
            "      - 62\n",
            "      - 64\n",
            "      - 196\n",
            "      - 44\n",
            "      - 142\n",
            "      - 73\n",
            "      - 50\n",
            "      - 133\n",
            "      - 151\n",
            "      - 42\n",
            "      - 73\n",
            "      - 200\n",
            "      - 63\n",
            "      - 81\n",
            "      - 71\n",
            "      - 65\n",
            "      - 66\n",
            "      - 40\n",
            "      - 102\n",
            "      - 48\n",
            "      - 121\n",
            "      - 38\n",
            "      - 67\n",
            "      - 161\n",
            "      - 62\n",
            "      - 25\n",
            "      - 31\n",
            "      - 85\n",
            "      episode_reward:\n",
            "      - 95.0\n",
            "      - 120.0\n",
            "      - 47.0\n",
            "      - 127.0\n",
            "      - 59.0\n",
            "      - 26.0\n",
            "      - 151.0\n",
            "      - 123.0\n",
            "      - 42.0\n",
            "      - 95.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 151.0\n",
            "      - 106.0\n",
            "      - 78.0\n",
            "      - 33.0\n",
            "      - 66.0\n",
            "      - 67.0\n",
            "      - 24.0\n",
            "      - 65.0\n",
            "      - 22.0\n",
            "      - 170.0\n",
            "      - 32.0\n",
            "      - 79.0\n",
            "      - 57.0\n",
            "      - 84.0\n",
            "      - 80.0\n",
            "      - 169.0\n",
            "      - 74.0\n",
            "      - 16.0\n",
            "      - 131.0\n",
            "      - 36.0\n",
            "      - 26.0\n",
            "      - 107.0\n",
            "      - 78.0\n",
            "      - 31.0\n",
            "      - 157.0\n",
            "      - 51.0\n",
            "      - 53.0\n",
            "      - 88.0\n",
            "      - 12.0\n",
            "      - 119.0\n",
            "      - 20.0\n",
            "      - 88.0\n",
            "      - 179.0\n",
            "      - 66.0\n",
            "      - 25.0\n",
            "      - 20.0\n",
            "      - 113.0\n",
            "      - 73.0\n",
            "      - 148.0\n",
            "      - 62.0\n",
            "      - 177.0\n",
            "      - 84.0\n",
            "      - 58.0\n",
            "      - 32.0\n",
            "      - 38.0\n",
            "      - 133.0\n",
            "      - 75.0\n",
            "      - 42.0\n",
            "      - 55.0\n",
            "      - 38.0\n",
            "      - 49.0\n",
            "      - 124.0\n",
            "      - 137.0\n",
            "      - 142.0\n",
            "      - 49.0\n",
            "      - 38.0\n",
            "      - 173.0\n",
            "      - 32.0\n",
            "      - 31.0\n",
            "      - 34.0\n",
            "      - 62.0\n",
            "      - 64.0\n",
            "      - 196.0\n",
            "      - 44.0\n",
            "      - 142.0\n",
            "      - 73.0\n",
            "      - 50.0\n",
            "      - 133.0\n",
            "      - 151.0\n",
            "      - 42.0\n",
            "      - 73.0\n",
            "      - 200.0\n",
            "      - 63.0\n",
            "      - 81.0\n",
            "      - 71.0\n",
            "      - 65.0\n",
            "      - 66.0\n",
            "      - 40.0\n",
            "      - 102.0\n",
            "      - 48.0\n",
            "      - 121.0\n",
            "      - 38.0\n",
            "      - 67.0\n",
            "      - 161.0\n",
            "      - 62.0\n",
            "      - 25.0\n",
            "      - 31.0\n",
            "      - 85.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13805341307157637\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10804050878709168\n",
            "      mean_inference_ms: 1.3134669786433306\n",
            "      mean_raw_obs_processing_ms: 0.24854295722544006\n",
            "  time_since_restore: 495.2726423740387\n",
            "  time_this_iter_s: 11.405228853225708\n",
            "  time_total_s: 495.2726423740387\n",
            "  timers:\n",
            "    learn_throughput: 31329.3\n",
            "    learn_time_ms: 6.384\n",
            "    load_throughput: 964762.277\n",
            "    load_time_ms: 0.207\n",
            "    training_iteration_time_ms: 247.658\n",
            "    update_time_ms: 4.214\n",
            "  timestamp: 1656954366\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 236800\n",
            "  training_iteration: 46\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:11 (running for 00:08:41.43)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         495.273</td><td style=\"text-align: right;\">236800</td><td style=\"text-align: right;\">   81.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             81.69</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:17 (running for 00:08:46.56)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         495.273</td><td style=\"text-align: right;\">236800</td><td style=\"text-align: right;\">   81.69</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">             81.69</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 246400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 246400\n",
            "    num_agent_steps_trained: 246400\n",
            "    num_env_steps_sampled: 246400\n",
            "    num_env_steps_trained: 246400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-06-17\n",
            "  done: false\n",
            "  episode_len_mean: 94.88118811881188\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 94.88118811881188\n",
            "  episode_reward_min: 9.0\n",
            "  episodes_this_iter: 101\n",
            "  episodes_total: 2974\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 128.28306579589844\n",
            "          policy_loss: 497.353271484375\n",
            "          var_gnorm: 25.986557006835938\n",
            "          vf_explained_var: 0.1413549780845642\n",
            "          vf_loss: 8363.138671875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 246400\n",
            "    num_agent_steps_trained: 246400\n",
            "    num_env_steps_sampled: 246400\n",
            "    num_env_steps_trained: 246400\n",
            "  iterations_since_restore: 47\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 246400\n",
            "  num_agent_steps_trained: 246400\n",
            "  num_env_steps_sampled: 246400\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 246400\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.6\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13836430388271959\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10804809642672585\n",
            "    mean_inference_ms: 1.3150467331816291\n",
            "    mean_raw_obs_processing_ms: 0.24801626169546176\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 94.88118811881188\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 94.88118811881188\n",
            "    episode_reward_min: 9.0\n",
            "    episodes_this_iter: 101\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 181\n",
            "      - 106\n",
            "      - 171\n",
            "      - 60\n",
            "      - 9\n",
            "      - 22\n",
            "      - 162\n",
            "      - 96\n",
            "      - 200\n",
            "      - 38\n",
            "      - 145\n",
            "      - 51\n",
            "      - 18\n",
            "      - 82\n",
            "      - 56\n",
            "      - 115\n",
            "      - 163\n",
            "      - 90\n",
            "      - 164\n",
            "      - 128\n",
            "      - 49\n",
            "      - 130\n",
            "      - 23\n",
            "      - 133\n",
            "      - 199\n",
            "      - 23\n",
            "      - 70\n",
            "      - 161\n",
            "      - 200\n",
            "      - 78\n",
            "      - 84\n",
            "      - 113\n",
            "      - 75\n",
            "      - 117\n",
            "      - 27\n",
            "      - 23\n",
            "      - 145\n",
            "      - 105\n",
            "      - 174\n",
            "      - 51\n",
            "      - 124\n",
            "      - 65\n",
            "      - 86\n",
            "      - 18\n",
            "      - 17\n",
            "      - 53\n",
            "      - 32\n",
            "      - 22\n",
            "      - 53\n",
            "      - 45\n",
            "      - 163\n",
            "      - 60\n",
            "      - 109\n",
            "      - 200\n",
            "      - 132\n",
            "      - 200\n",
            "      - 126\n",
            "      - 99\n",
            "      - 19\n",
            "      - 60\n",
            "      - 139\n",
            "      - 23\n",
            "      - 131\n",
            "      - 103\n",
            "      - 62\n",
            "      - 55\n",
            "      - 48\n",
            "      - 117\n",
            "      - 163\n",
            "      - 160\n",
            "      - 126\n",
            "      - 22\n",
            "      - 132\n",
            "      - 54\n",
            "      - 112\n",
            "      - 25\n",
            "      - 127\n",
            "      - 41\n",
            "      - 31\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 98\n",
            "      - 36\n",
            "      - 23\n",
            "      - 99\n",
            "      - 99\n",
            "      - 100\n",
            "      - 141\n",
            "      - 149\n",
            "      - 18\n",
            "      - 85\n",
            "      - 192\n",
            "      - 29\n",
            "      - 163\n",
            "      - 59\n",
            "      - 76\n",
            "      - 91\n",
            "      - 43\n",
            "      - 21\n",
            "      - 160\n",
            "      episode_reward:\n",
            "      - 181.0\n",
            "      - 106.0\n",
            "      - 171.0\n",
            "      - 60.0\n",
            "      - 9.0\n",
            "      - 22.0\n",
            "      - 162.0\n",
            "      - 96.0\n",
            "      - 200.0\n",
            "      - 38.0\n",
            "      - 145.0\n",
            "      - 51.0\n",
            "      - 18.0\n",
            "      - 82.0\n",
            "      - 56.0\n",
            "      - 115.0\n",
            "      - 163.0\n",
            "      - 90.0\n",
            "      - 164.0\n",
            "      - 128.0\n",
            "      - 49.0\n",
            "      - 130.0\n",
            "      - 23.0\n",
            "      - 133.0\n",
            "      - 199.0\n",
            "      - 23.0\n",
            "      - 70.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 78.0\n",
            "      - 84.0\n",
            "      - 113.0\n",
            "      - 75.0\n",
            "      - 117.0\n",
            "      - 27.0\n",
            "      - 23.0\n",
            "      - 145.0\n",
            "      - 105.0\n",
            "      - 174.0\n",
            "      - 51.0\n",
            "      - 124.0\n",
            "      - 65.0\n",
            "      - 86.0\n",
            "      - 18.0\n",
            "      - 17.0\n",
            "      - 53.0\n",
            "      - 32.0\n",
            "      - 22.0\n",
            "      - 53.0\n",
            "      - 45.0\n",
            "      - 163.0\n",
            "      - 60.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 99.0\n",
            "      - 19.0\n",
            "      - 60.0\n",
            "      - 139.0\n",
            "      - 23.0\n",
            "      - 131.0\n",
            "      - 103.0\n",
            "      - 62.0\n",
            "      - 55.0\n",
            "      - 48.0\n",
            "      - 117.0\n",
            "      - 163.0\n",
            "      - 160.0\n",
            "      - 126.0\n",
            "      - 22.0\n",
            "      - 132.0\n",
            "      - 54.0\n",
            "      - 112.0\n",
            "      - 25.0\n",
            "      - 127.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 98.0\n",
            "      - 36.0\n",
            "      - 23.0\n",
            "      - 99.0\n",
            "      - 99.0\n",
            "      - 100.0\n",
            "      - 141.0\n",
            "      - 149.0\n",
            "      - 18.0\n",
            "      - 85.0\n",
            "      - 192.0\n",
            "      - 29.0\n",
            "      - 163.0\n",
            "      - 59.0\n",
            "      - 76.0\n",
            "      - 91.0\n",
            "      - 43.0\n",
            "      - 21.0\n",
            "      - 160.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13836430388271959\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10804809642672585\n",
            "      mean_inference_ms: 1.3150467331816291\n",
            "      mean_raw_obs_processing_ms: 0.24801626169546176\n",
            "  time_since_restore: 505.49816036224365\n",
            "  time_this_iter_s: 10.225517988204956\n",
            "  time_total_s: 505.49816036224365\n",
            "  timers:\n",
            "    learn_throughput: 33353.643\n",
            "    learn_time_ms: 5.996\n",
            "    load_throughput: 1081982.2\n",
            "    load_time_ms: 0.185\n",
            "    training_iteration_time_ms: 216.915\n",
            "    update_time_ms: 4.047\n",
            "  timestamp: 1656954377\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 246400\n",
            "  training_iteration: 47\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:22 (running for 00:08:51.69)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         505.498</td><td style=\"text-align: right;\">246400</td><td style=\"text-align: right;\"> 94.8812</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           94.8812</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:27 (running for 00:08:56.79)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         505.498</td><td style=\"text-align: right;\">246400</td><td style=\"text-align: right;\"> 94.8812</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                   9</td><td style=\"text-align: right;\">           94.8812</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 247200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 247200\n",
            "    num_agent_steps_trained: 247200\n",
            "    num_env_steps_sampled: 247200\n",
            "    num_env_steps_trained: 247200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-06-27\n",
            "  done: false\n",
            "  episode_len_mean: 96.57\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 96.57\n",
            "  episode_reward_min: 17.0\n",
            "  episodes_this_iter: 6\n",
            "  episodes_total: 2980\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 84.05\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 180.0\n",
            "    episode_reward_mean: 84.05\n",
            "    episode_reward_min: 20.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 97\n",
            "      - 118\n",
            "      - 28\n",
            "      - 21\n",
            "      - 155\n",
            "      - 108\n",
            "      - 140\n",
            "      - 154\n",
            "      - 32\n",
            "      - 71\n",
            "      - 92\n",
            "      - 84\n",
            "      - 180\n",
            "      - 64\n",
            "      - 20\n",
            "      - 66\n",
            "      - 117\n",
            "      - 52\n",
            "      - 50\n",
            "      - 32\n",
            "      episode_reward:\n",
            "      - 97.0\n",
            "      - 118.0\n",
            "      - 28.0\n",
            "      - 21.0\n",
            "      - 155.0\n",
            "      - 108.0\n",
            "      - 140.0\n",
            "      - 154.0\n",
            "      - 32.0\n",
            "      - 71.0\n",
            "      - 92.0\n",
            "      - 84.0\n",
            "      - 180.0\n",
            "      - 64.0\n",
            "      - 20.0\n",
            "      - 66.0\n",
            "      - 117.0\n",
            "      - 52.0\n",
            "      - 50.0\n",
            "      - 32.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1002321720160106\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07667730977250116\n",
            "      mean_inference_ms: 0.976663806881021\n",
            "      mean_raw_obs_processing_ms: 0.10745463818085436\n",
            "    timesteps_this_iter: 1681\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 123.66017150878906\n",
            "          policy_loss: 618.1370849609375\n",
            "          var_gnorm: 25.999650955200195\n",
            "          vf_explained_var: 0.06549447774887085\n",
            "          vf_loss: 8936.546875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 247200\n",
            "    num_agent_steps_trained: 247200\n",
            "    num_env_steps_sampled: 247200\n",
            "    num_env_steps_trained: 247200\n",
            "  iterations_since_restore: 48\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 247200\n",
            "  num_agent_steps_trained: 247200\n",
            "  num_env_steps_sampled: 247200\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 247200\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.15714285714286\n",
            "    ram_util_percent: 21.6\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13832817409267714\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10805626888475184\n",
            "    mean_inference_ms: 1.3148523628389353\n",
            "    mean_raw_obs_processing_ms: 0.24808087172612006\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 96.57\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 96.57\n",
            "    episode_reward_min: 17.0\n",
            "    episodes_this_iter: 6\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 96\n",
            "      - 200\n",
            "      - 38\n",
            "      - 145\n",
            "      - 51\n",
            "      - 18\n",
            "      - 82\n",
            "      - 56\n",
            "      - 115\n",
            "      - 163\n",
            "      - 90\n",
            "      - 164\n",
            "      - 128\n",
            "      - 49\n",
            "      - 130\n",
            "      - 23\n",
            "      - 133\n",
            "      - 199\n",
            "      - 23\n",
            "      - 70\n",
            "      - 161\n",
            "      - 200\n",
            "      - 78\n",
            "      - 84\n",
            "      - 113\n",
            "      - 75\n",
            "      - 117\n",
            "      - 27\n",
            "      - 23\n",
            "      - 145\n",
            "      - 105\n",
            "      - 174\n",
            "      - 51\n",
            "      - 124\n",
            "      - 65\n",
            "      - 86\n",
            "      - 18\n",
            "      - 17\n",
            "      - 53\n",
            "      - 32\n",
            "      - 22\n",
            "      - 53\n",
            "      - 45\n",
            "      - 163\n",
            "      - 60\n",
            "      - 109\n",
            "      - 200\n",
            "      - 132\n",
            "      - 200\n",
            "      - 126\n",
            "      - 99\n",
            "      - 19\n",
            "      - 60\n",
            "      - 139\n",
            "      - 23\n",
            "      - 131\n",
            "      - 103\n",
            "      - 62\n",
            "      - 55\n",
            "      - 48\n",
            "      - 117\n",
            "      - 163\n",
            "      - 160\n",
            "      - 126\n",
            "      - 22\n",
            "      - 132\n",
            "      - 54\n",
            "      - 112\n",
            "      - 25\n",
            "      - 127\n",
            "      - 41\n",
            "      - 31\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 98\n",
            "      - 36\n",
            "      - 23\n",
            "      - 99\n",
            "      - 99\n",
            "      - 100\n",
            "      - 141\n",
            "      - 149\n",
            "      - 18\n",
            "      - 85\n",
            "      - 192\n",
            "      - 29\n",
            "      - 163\n",
            "      - 59\n",
            "      - 76\n",
            "      - 91\n",
            "      - 43\n",
            "      - 21\n",
            "      - 160\n",
            "      - 171\n",
            "      - 200\n",
            "      - 51\n",
            "      - 32\n",
            "      - 131\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 96.0\n",
            "      - 200.0\n",
            "      - 38.0\n",
            "      - 145.0\n",
            "      - 51.0\n",
            "      - 18.0\n",
            "      - 82.0\n",
            "      - 56.0\n",
            "      - 115.0\n",
            "      - 163.0\n",
            "      - 90.0\n",
            "      - 164.0\n",
            "      - 128.0\n",
            "      - 49.0\n",
            "      - 130.0\n",
            "      - 23.0\n",
            "      - 133.0\n",
            "      - 199.0\n",
            "      - 23.0\n",
            "      - 70.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 78.0\n",
            "      - 84.0\n",
            "      - 113.0\n",
            "      - 75.0\n",
            "      - 117.0\n",
            "      - 27.0\n",
            "      - 23.0\n",
            "      - 145.0\n",
            "      - 105.0\n",
            "      - 174.0\n",
            "      - 51.0\n",
            "      - 124.0\n",
            "      - 65.0\n",
            "      - 86.0\n",
            "      - 18.0\n",
            "      - 17.0\n",
            "      - 53.0\n",
            "      - 32.0\n",
            "      - 22.0\n",
            "      - 53.0\n",
            "      - 45.0\n",
            "      - 163.0\n",
            "      - 60.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 99.0\n",
            "      - 19.0\n",
            "      - 60.0\n",
            "      - 139.0\n",
            "      - 23.0\n",
            "      - 131.0\n",
            "      - 103.0\n",
            "      - 62.0\n",
            "      - 55.0\n",
            "      - 48.0\n",
            "      - 117.0\n",
            "      - 163.0\n",
            "      - 160.0\n",
            "      - 126.0\n",
            "      - 22.0\n",
            "      - 132.0\n",
            "      - 54.0\n",
            "      - 112.0\n",
            "      - 25.0\n",
            "      - 127.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 98.0\n",
            "      - 36.0\n",
            "      - 23.0\n",
            "      - 99.0\n",
            "      - 99.0\n",
            "      - 100.0\n",
            "      - 141.0\n",
            "      - 149.0\n",
            "      - 18.0\n",
            "      - 85.0\n",
            "      - 192.0\n",
            "      - 29.0\n",
            "      - 163.0\n",
            "      - 59.0\n",
            "      - 76.0\n",
            "      - 91.0\n",
            "      - 43.0\n",
            "      - 21.0\n",
            "      - 160.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 51.0\n",
            "      - 32.0\n",
            "      - 131.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13832817409267714\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10805626888475184\n",
            "      mean_inference_ms: 1.3148523628389353\n",
            "      mean_raw_obs_processing_ms: 0.24808087172612006\n",
            "  time_since_restore: 515.7727515697479\n",
            "  time_this_iter_s: 10.274591207504272\n",
            "  time_total_s: 515.7727515697479\n",
            "  timers:\n",
            "    learn_throughput: 32591.294\n",
            "    learn_time_ms: 6.137\n",
            "    load_throughput: 1031555.337\n",
            "    load_time_ms: 0.194\n",
            "    training_iteration_time_ms: 213.648\n",
            "    update_time_ms: 4.237\n",
            "  timestamp: 1656954387\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 247200\n",
            "  training_iteration: 48\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:32 (running for 00:09:02.01)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         515.773</td><td style=\"text-align: right;\">247200</td><td style=\"text-align: right;\">   96.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">             96.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:37 (running for 00:09:07.14)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         515.773</td><td style=\"text-align: right;\">247200</td><td style=\"text-align: right;\">   96.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">             96.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 256800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 256800\n",
            "    num_agent_steps_trained: 256800\n",
            "    num_env_steps_sampled: 256800\n",
            "    num_env_steps_trained: 256800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-06-37\n",
            "  done: false\n",
            "  episode_len_mean: 85.56637168141593\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 85.56637168141593\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 113\n",
            "  episodes_total: 3093\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 129.4515838623047\n",
            "          policy_loss: 358.04736328125\n",
            "          var_gnorm: 26.143434524536133\n",
            "          vf_explained_var: 0.15241682529449463\n",
            "          vf_loss: 9705.9580078125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 256800\n",
            "    num_agent_steps_trained: 256800\n",
            "    num_env_steps_sampled: 256800\n",
            "    num_env_steps_trained: 256800\n",
            "  iterations_since_restore: 49\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 256800\n",
            "  num_agent_steps_trained: 256800\n",
            "  num_env_steps_sampled: 256800\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 256800\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.13333333333335\n",
            "    ram_util_percent: 21.600000000000005\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13837111245247802\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1079397760210652\n",
            "    mean_inference_ms: 1.3140419516310593\n",
            "    mean_raw_obs_processing_ms: 0.24793197700734051\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 85.56637168141593\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 85.56637168141593\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 113\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 18\n",
            "      - 113\n",
            "      - 69\n",
            "      - 29\n",
            "      - 200\n",
            "      - 70\n",
            "      - 28\n",
            "      - 43\n",
            "      - 103\n",
            "      - 156\n",
            "      - 133\n",
            "      - 28\n",
            "      - 162\n",
            "      - 200\n",
            "      - 172\n",
            "      - 40\n",
            "      - 146\n",
            "      - 152\n",
            "      - 94\n",
            "      - 50\n",
            "      - 14\n",
            "      - 13\n",
            "      - 126\n",
            "      - 23\n",
            "      - 45\n",
            "      - 133\n",
            "      - 73\n",
            "      - 64\n",
            "      - 96\n",
            "      - 32\n",
            "      - 87\n",
            "      - 52\n",
            "      - 86\n",
            "      - 54\n",
            "      - 53\n",
            "      - 33\n",
            "      - 102\n",
            "      - 52\n",
            "      - 32\n",
            "      - 44\n",
            "      - 116\n",
            "      - 74\n",
            "      - 108\n",
            "      - 57\n",
            "      - 134\n",
            "      - 21\n",
            "      - 148\n",
            "      - 21\n",
            "      - 66\n",
            "      - 103\n",
            "      - 109\n",
            "      - 80\n",
            "      - 97\n",
            "      - 86\n",
            "      - 17\n",
            "      - 121\n",
            "      - 81\n",
            "      - 74\n",
            "      - 157\n",
            "      - 172\n",
            "      - 200\n",
            "      - 41\n",
            "      - 80\n",
            "      - 57\n",
            "      - 13\n",
            "      - 116\n",
            "      - 50\n",
            "      - 114\n",
            "      - 161\n",
            "      - 63\n",
            "      - 38\n",
            "      - 23\n",
            "      - 200\n",
            "      - 49\n",
            "      - 144\n",
            "      - 87\n",
            "      - 200\n",
            "      - 200\n",
            "      - 56\n",
            "      - 124\n",
            "      - 84\n",
            "      - 64\n",
            "      - 18\n",
            "      - 38\n",
            "      - 31\n",
            "      - 153\n",
            "      - 29\n",
            "      - 41\n",
            "      - 95\n",
            "      - 139\n",
            "      - 19\n",
            "      - 200\n",
            "      - 43\n",
            "      - 45\n",
            "      - 40\n",
            "      - 117\n",
            "      - 14\n",
            "      - 37\n",
            "      - 56\n",
            "      - 151\n",
            "      - 100\n",
            "      - 25\n",
            "      - 92\n",
            "      - 47\n",
            "      - 149\n",
            "      - 154\n",
            "      - 54\n",
            "      - 124\n",
            "      - 118\n",
            "      - 38\n",
            "      - 17\n",
            "      - 134\n",
            "      - 125\n",
            "      episode_reward:\n",
            "      - 18.0\n",
            "      - 113.0\n",
            "      - 69.0\n",
            "      - 29.0\n",
            "      - 200.0\n",
            "      - 70.0\n",
            "      - 28.0\n",
            "      - 43.0\n",
            "      - 103.0\n",
            "      - 156.0\n",
            "      - 133.0\n",
            "      - 28.0\n",
            "      - 162.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 40.0\n",
            "      - 146.0\n",
            "      - 152.0\n",
            "      - 94.0\n",
            "      - 50.0\n",
            "      - 14.0\n",
            "      - 13.0\n",
            "      - 126.0\n",
            "      - 23.0\n",
            "      - 45.0\n",
            "      - 133.0\n",
            "      - 73.0\n",
            "      - 64.0\n",
            "      - 96.0\n",
            "      - 32.0\n",
            "      - 87.0\n",
            "      - 52.0\n",
            "      - 86.0\n",
            "      - 54.0\n",
            "      - 53.0\n",
            "      - 33.0\n",
            "      - 102.0\n",
            "      - 52.0\n",
            "      - 32.0\n",
            "      - 44.0\n",
            "      - 116.0\n",
            "      - 74.0\n",
            "      - 108.0\n",
            "      - 57.0\n",
            "      - 134.0\n",
            "      - 21.0\n",
            "      - 148.0\n",
            "      - 21.0\n",
            "      - 66.0\n",
            "      - 103.0\n",
            "      - 109.0\n",
            "      - 80.0\n",
            "      - 97.0\n",
            "      - 86.0\n",
            "      - 17.0\n",
            "      - 121.0\n",
            "      - 81.0\n",
            "      - 74.0\n",
            "      - 157.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 41.0\n",
            "      - 80.0\n",
            "      - 57.0\n",
            "      - 13.0\n",
            "      - 116.0\n",
            "      - 50.0\n",
            "      - 114.0\n",
            "      - 161.0\n",
            "      - 63.0\n",
            "      - 38.0\n",
            "      - 23.0\n",
            "      - 200.0\n",
            "      - 49.0\n",
            "      - 144.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 124.0\n",
            "      - 84.0\n",
            "      - 64.0\n",
            "      - 18.0\n",
            "      - 38.0\n",
            "      - 31.0\n",
            "      - 153.0\n",
            "      - 29.0\n",
            "      - 41.0\n",
            "      - 95.0\n",
            "      - 139.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 45.0\n",
            "      - 40.0\n",
            "      - 117.0\n",
            "      - 14.0\n",
            "      - 37.0\n",
            "      - 56.0\n",
            "      - 151.0\n",
            "      - 100.0\n",
            "      - 25.0\n",
            "      - 92.0\n",
            "      - 47.0\n",
            "      - 149.0\n",
            "      - 154.0\n",
            "      - 54.0\n",
            "      - 124.0\n",
            "      - 118.0\n",
            "      - 38.0\n",
            "      - 17.0\n",
            "      - 134.0\n",
            "      - 125.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13837111245247802\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1079397760210652\n",
            "      mean_inference_ms: 1.3140419516310593\n",
            "      mean_raw_obs_processing_ms: 0.24793197700734051\n",
            "  time_since_restore: 526.0461497306824\n",
            "  time_this_iter_s: 10.273398160934448\n",
            "  time_total_s: 526.0461497306824\n",
            "  timers:\n",
            "    learn_throughput: 34388.561\n",
            "    learn_time_ms: 5.816\n",
            "    load_throughput: 1093548.168\n",
            "    load_time_ms: 0.183\n",
            "    training_iteration_time_ms: 213.893\n",
            "    update_time_ms: 3.552\n",
            "  timestamp: 1656954397\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 256800\n",
            "  training_iteration: 49\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:42 (running for 00:09:12.31)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         526.046</td><td style=\"text-align: right;\">256800</td><td style=\"text-align: right;\"> 85.5664</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">           85.5664</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:47 (running for 00:09:17.41)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         526.046</td><td style=\"text-align: right;\">256800</td><td style=\"text-align: right;\"> 85.5664</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">           85.5664</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 257800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 257800\n",
            "    num_agent_steps_trained: 257800\n",
            "    num_env_steps_sampled: 257800\n",
            "    num_env_steps_trained: 257800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-06-49\n",
            "  done: false\n",
            "  episode_len_mean: 84.47\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 84.47\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 11\n",
            "  episodes_total: 3104\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 99.15\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 99.15\n",
            "    episode_reward_min: 21.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 151\n",
            "      - 133\n",
            "      - 83\n",
            "      - 177\n",
            "      - 141\n",
            "      - 27\n",
            "      - 21\n",
            "      - 47\n",
            "      - 171\n",
            "      - 200\n",
            "      - 200\n",
            "      - 34\n",
            "      - 34\n",
            "      - 70\n",
            "      - 74\n",
            "      - 21\n",
            "      - 133\n",
            "      - 86\n",
            "      - 37\n",
            "      - 143\n",
            "      episode_reward:\n",
            "      - 151.0\n",
            "      - 133.0\n",
            "      - 83.0\n",
            "      - 177.0\n",
            "      - 141.0\n",
            "      - 27.0\n",
            "      - 21.0\n",
            "      - 47.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 34.0\n",
            "      - 34.0\n",
            "      - 70.0\n",
            "      - 74.0\n",
            "      - 21.0\n",
            "      - 133.0\n",
            "      - 86.0\n",
            "      - 37.0\n",
            "      - 143.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1001779218094299\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07660874605740495\n",
            "      mean_inference_ms: 0.975494342323908\n",
            "      mean_raw_obs_processing_ms: 0.10739888178843135\n",
            "    timesteps_this_iter: 1983\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 124.8267822265625\n",
            "          policy_loss: 285.85479736328125\n",
            "          var_gnorm: 26.158233642578125\n",
            "          vf_explained_var: 0.11889892816543579\n",
            "          vf_loss: 9864.439453125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 257800\n",
            "    num_agent_steps_trained: 257800\n",
            "    num_env_steps_sampled: 257800\n",
            "    num_env_steps_trained: 257800\n",
            "  iterations_since_restore: 50\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 257800\n",
            "  num_agent_steps_trained: 257800\n",
            "  num_env_steps_sampled: 257800\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 257800\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.12941176470588\n",
            "    ram_util_percent: 21.61764705882353\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13822363000302748\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10798229125572552\n",
            "    mean_inference_ms: 1.3132793802087954\n",
            "    mean_raw_obs_processing_ms: 0.24815541615521383\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 84.47\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 84.47\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 11\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 45\n",
            "      - 133\n",
            "      - 73\n",
            "      - 64\n",
            "      - 96\n",
            "      - 32\n",
            "      - 87\n",
            "      - 52\n",
            "      - 86\n",
            "      - 54\n",
            "      - 53\n",
            "      - 33\n",
            "      - 102\n",
            "      - 52\n",
            "      - 32\n",
            "      - 44\n",
            "      - 116\n",
            "      - 74\n",
            "      - 108\n",
            "      - 57\n",
            "      - 134\n",
            "      - 21\n",
            "      - 148\n",
            "      - 21\n",
            "      - 66\n",
            "      - 103\n",
            "      - 109\n",
            "      - 80\n",
            "      - 97\n",
            "      - 86\n",
            "      - 17\n",
            "      - 121\n",
            "      - 81\n",
            "      - 74\n",
            "      - 157\n",
            "      - 172\n",
            "      - 200\n",
            "      - 41\n",
            "      - 80\n",
            "      - 57\n",
            "      - 13\n",
            "      - 116\n",
            "      - 50\n",
            "      - 114\n",
            "      - 161\n",
            "      - 63\n",
            "      - 38\n",
            "      - 23\n",
            "      - 200\n",
            "      - 49\n",
            "      - 144\n",
            "      - 87\n",
            "      - 200\n",
            "      - 200\n",
            "      - 56\n",
            "      - 124\n",
            "      - 84\n",
            "      - 64\n",
            "      - 18\n",
            "      - 38\n",
            "      - 31\n",
            "      - 153\n",
            "      - 29\n",
            "      - 41\n",
            "      - 95\n",
            "      - 139\n",
            "      - 19\n",
            "      - 200\n",
            "      - 43\n",
            "      - 45\n",
            "      - 40\n",
            "      - 117\n",
            "      - 14\n",
            "      - 37\n",
            "      - 56\n",
            "      - 151\n",
            "      - 100\n",
            "      - 25\n",
            "      - 92\n",
            "      - 47\n",
            "      - 149\n",
            "      - 154\n",
            "      - 54\n",
            "      - 124\n",
            "      - 118\n",
            "      - 38\n",
            "      - 17\n",
            "      - 134\n",
            "      - 125\n",
            "      - 112\n",
            "      - 73\n",
            "      - 81\n",
            "      - 17\n",
            "      - 139\n",
            "      - 19\n",
            "      - 58\n",
            "      - 33\n",
            "      - 200\n",
            "      - 55\n",
            "      - 173\n",
            "      episode_reward:\n",
            "      - 45.0\n",
            "      - 133.0\n",
            "      - 73.0\n",
            "      - 64.0\n",
            "      - 96.0\n",
            "      - 32.0\n",
            "      - 87.0\n",
            "      - 52.0\n",
            "      - 86.0\n",
            "      - 54.0\n",
            "      - 53.0\n",
            "      - 33.0\n",
            "      - 102.0\n",
            "      - 52.0\n",
            "      - 32.0\n",
            "      - 44.0\n",
            "      - 116.0\n",
            "      - 74.0\n",
            "      - 108.0\n",
            "      - 57.0\n",
            "      - 134.0\n",
            "      - 21.0\n",
            "      - 148.0\n",
            "      - 21.0\n",
            "      - 66.0\n",
            "      - 103.0\n",
            "      - 109.0\n",
            "      - 80.0\n",
            "      - 97.0\n",
            "      - 86.0\n",
            "      - 17.0\n",
            "      - 121.0\n",
            "      - 81.0\n",
            "      - 74.0\n",
            "      - 157.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 41.0\n",
            "      - 80.0\n",
            "      - 57.0\n",
            "      - 13.0\n",
            "      - 116.0\n",
            "      - 50.0\n",
            "      - 114.0\n",
            "      - 161.0\n",
            "      - 63.0\n",
            "      - 38.0\n",
            "      - 23.0\n",
            "      - 200.0\n",
            "      - 49.0\n",
            "      - 144.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 124.0\n",
            "      - 84.0\n",
            "      - 64.0\n",
            "      - 18.0\n",
            "      - 38.0\n",
            "      - 31.0\n",
            "      - 153.0\n",
            "      - 29.0\n",
            "      - 41.0\n",
            "      - 95.0\n",
            "      - 139.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 45.0\n",
            "      - 40.0\n",
            "      - 117.0\n",
            "      - 14.0\n",
            "      - 37.0\n",
            "      - 56.0\n",
            "      - 151.0\n",
            "      - 100.0\n",
            "      - 25.0\n",
            "      - 92.0\n",
            "      - 47.0\n",
            "      - 149.0\n",
            "      - 154.0\n",
            "      - 54.0\n",
            "      - 124.0\n",
            "      - 118.0\n",
            "      - 38.0\n",
            "      - 17.0\n",
            "      - 134.0\n",
            "      - 125.0\n",
            "      - 112.0\n",
            "      - 73.0\n",
            "      - 81.0\n",
            "      - 17.0\n",
            "      - 139.0\n",
            "      - 19.0\n",
            "      - 58.0\n",
            "      - 33.0\n",
            "      - 200.0\n",
            "      - 55.0\n",
            "      - 173.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13822363000302748\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10798229125572552\n",
            "      mean_inference_ms: 1.3132793802087954\n",
            "      mean_raw_obs_processing_ms: 0.24815541615521383\n",
            "  time_since_restore: 537.7943665981293\n",
            "  time_this_iter_s: 11.7482168674469\n",
            "  time_total_s: 537.7943665981293\n",
            "  timers:\n",
            "    learn_throughput: 32765.44\n",
            "    learn_time_ms: 6.104\n",
            "    load_throughput: 1041545.567\n",
            "    load_time_ms: 0.192\n",
            "    training_iteration_time_ms: 216.04\n",
            "    update_time_ms: 3.51\n",
            "  timestamp: 1656954409\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 257800\n",
            "  training_iteration: 50\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:54 (running for 00:09:24.10)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         537.794</td><td style=\"text-align: right;\">257800</td><td style=\"text-align: right;\">   84.47</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             84.47</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:06:59 (running for 00:09:29.24)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         537.794</td><td style=\"text-align: right;\">257800</td><td style=\"text-align: right;\">   84.47</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">             84.47</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 267400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 267400\n",
            "    num_agent_steps_trained: 267400\n",
            "    num_env_steps_sampled: 267400\n",
            "    num_env_steps_trained: 267400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-06-59\n",
            "  done: false\n",
            "  episode_len_mean: 87.60550458715596\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 87.60550458715596\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 109\n",
            "  episodes_total: 3213\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 127.88350677490234\n",
            "          policy_loss: 535.6482543945312\n",
            "          var_gnorm: 26.28493881225586\n",
            "          vf_explained_var: 0.004204392433166504\n",
            "          vf_loss: 8772.595703125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 267400\n",
            "    num_agent_steps_trained: 267400\n",
            "    num_env_steps_sampled: 267400\n",
            "    num_env_steps_trained: 267400\n",
            "  iterations_since_restore: 51\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 267400\n",
            "  num_agent_steps_trained: 267400\n",
            "  num_env_steps_sampled: 267400\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 267400\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.93571428571431\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13832224879270041\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10788164165042319\n",
            "    mean_inference_ms: 1.3127428282777078\n",
            "    mean_raw_obs_processing_ms: 0.24776375292310832\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 87.60550458715596\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 87.60550458715596\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 109\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 99\n",
            "      - 27\n",
            "      - 33\n",
            "      - 32\n",
            "      - 117\n",
            "      - 64\n",
            "      - 67\n",
            "      - 200\n",
            "      - 53\n",
            "      - 148\n",
            "      - 59\n",
            "      - 125\n",
            "      - 72\n",
            "      - 29\n",
            "      - 56\n",
            "      - 34\n",
            "      - 165\n",
            "      - 67\n",
            "      - 42\n",
            "      - 107\n",
            "      - 42\n",
            "      - 36\n",
            "      - 88\n",
            "      - 130\n",
            "      - 53\n",
            "      - 101\n",
            "      - 128\n",
            "      - 68\n",
            "      - 25\n",
            "      - 95\n",
            "      - 63\n",
            "      - 141\n",
            "      - 38\n",
            "      - 128\n",
            "      - 71\n",
            "      - 30\n",
            "      - 47\n",
            "      - 142\n",
            "      - 71\n",
            "      - 116\n",
            "      - 34\n",
            "      - 200\n",
            "      - 69\n",
            "      - 64\n",
            "      - 128\n",
            "      - 150\n",
            "      - 129\n",
            "      - 97\n",
            "      - 52\n",
            "      - 31\n",
            "      - 16\n",
            "      - 39\n",
            "      - 27\n",
            "      - 74\n",
            "      - 70\n",
            "      - 137\n",
            "      - 78\n",
            "      - 105\n",
            "      - 73\n",
            "      - 60\n",
            "      - 54\n",
            "      - 200\n",
            "      - 81\n",
            "      - 38\n",
            "      - 22\n",
            "      - 86\n",
            "      - 37\n",
            "      - 127\n",
            "      - 200\n",
            "      - 17\n",
            "      - 72\n",
            "      - 46\n",
            "      - 114\n",
            "      - 81\n",
            "      - 44\n",
            "      - 43\n",
            "      - 16\n",
            "      - 111\n",
            "      - 136\n",
            "      - 136\n",
            "      - 46\n",
            "      - 30\n",
            "      - 111\n",
            "      - 154\n",
            "      - 151\n",
            "      - 129\n",
            "      - 102\n",
            "      - 117\n",
            "      - 49\n",
            "      - 175\n",
            "      - 48\n",
            "      - 35\n",
            "      - 116\n",
            "      - 39\n",
            "      - 114\n",
            "      - 109\n",
            "      - 200\n",
            "      - 124\n",
            "      - 146\n",
            "      - 43\n",
            "      - 133\n",
            "      - 27\n",
            "      - 81\n",
            "      - 65\n",
            "      - 200\n",
            "      - 153\n",
            "      - 83\n",
            "      - 66\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 99.0\n",
            "      - 27.0\n",
            "      - 33.0\n",
            "      - 32.0\n",
            "      - 117.0\n",
            "      - 64.0\n",
            "      - 67.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 148.0\n",
            "      - 59.0\n",
            "      - 125.0\n",
            "      - 72.0\n",
            "      - 29.0\n",
            "      - 56.0\n",
            "      - 34.0\n",
            "      - 165.0\n",
            "      - 67.0\n",
            "      - 42.0\n",
            "      - 107.0\n",
            "      - 42.0\n",
            "      - 36.0\n",
            "      - 88.0\n",
            "      - 130.0\n",
            "      - 53.0\n",
            "      - 101.0\n",
            "      - 128.0\n",
            "      - 68.0\n",
            "      - 25.0\n",
            "      - 95.0\n",
            "      - 63.0\n",
            "      - 141.0\n",
            "      - 38.0\n",
            "      - 128.0\n",
            "      - 71.0\n",
            "      - 30.0\n",
            "      - 47.0\n",
            "      - 142.0\n",
            "      - 71.0\n",
            "      - 116.0\n",
            "      - 34.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 64.0\n",
            "      - 128.0\n",
            "      - 150.0\n",
            "      - 129.0\n",
            "      - 97.0\n",
            "      - 52.0\n",
            "      - 31.0\n",
            "      - 16.0\n",
            "      - 39.0\n",
            "      - 27.0\n",
            "      - 74.0\n",
            "      - 70.0\n",
            "      - 137.0\n",
            "      - 78.0\n",
            "      - 105.0\n",
            "      - 73.0\n",
            "      - 60.0\n",
            "      - 54.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 38.0\n",
            "      - 22.0\n",
            "      - 86.0\n",
            "      - 37.0\n",
            "      - 127.0\n",
            "      - 200.0\n",
            "      - 17.0\n",
            "      - 72.0\n",
            "      - 46.0\n",
            "      - 114.0\n",
            "      - 81.0\n",
            "      - 44.0\n",
            "      - 43.0\n",
            "      - 16.0\n",
            "      - 111.0\n",
            "      - 136.0\n",
            "      - 136.0\n",
            "      - 46.0\n",
            "      - 30.0\n",
            "      - 111.0\n",
            "      - 154.0\n",
            "      - 151.0\n",
            "      - 129.0\n",
            "      - 102.0\n",
            "      - 117.0\n",
            "      - 49.0\n",
            "      - 175.0\n",
            "      - 48.0\n",
            "      - 35.0\n",
            "      - 116.0\n",
            "      - 39.0\n",
            "      - 114.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 146.0\n",
            "      - 43.0\n",
            "      - 133.0\n",
            "      - 27.0\n",
            "      - 81.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 83.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13832224879270041\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10788164165042319\n",
            "      mean_inference_ms: 1.3127428282777078\n",
            "      mean_raw_obs_processing_ms: 0.24776375292310832\n",
            "  time_since_restore: 547.9731466770172\n",
            "  time_this_iter_s: 10.17878007888794\n",
            "  time_total_s: 547.9731466770172\n",
            "  timers:\n",
            "    learn_throughput: 32285.732\n",
            "    learn_time_ms: 6.195\n",
            "    load_throughput: 1086607.254\n",
            "    load_time_ms: 0.184\n",
            "    training_iteration_time_ms: 216.667\n",
            "    update_time_ms: 3.835\n",
            "  timestamp: 1656954419\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 267400\n",
            "  training_iteration: 51\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:04 (running for 00:09:34.32)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         547.973</td><td style=\"text-align: right;\">267400</td><td style=\"text-align: right;\"> 87.6055</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">           87.6055</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:09 (running for 00:09:39.41)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         547.973</td><td style=\"text-align: right;\">267400</td><td style=\"text-align: right;\"> 87.6055</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">           87.6055</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 268400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 268400\n",
            "    num_agent_steps_trained: 268400\n",
            "    num_env_steps_sampled: 268400\n",
            "    num_env_steps_trained: 268400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-07-12\n",
            "  done: false\n",
            "  episode_len_mean: 92.62\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 92.62\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 9\n",
            "  episodes_total: 3222\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 106.6\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 106.6\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 113\n",
            "      - 139\n",
            "      - 60\n",
            "      - 112\n",
            "      - 152\n",
            "      - 31\n",
            "      - 58\n",
            "      - 155\n",
            "      - 30\n",
            "      - 23\n",
            "      - 15\n",
            "      - 190\n",
            "      - 75\n",
            "      - 80\n",
            "      - 86\n",
            "      - 200\n",
            "      - 200\n",
            "      - 163\n",
            "      - 80\n",
            "      - 170\n",
            "      episode_reward:\n",
            "      - 113.0\n",
            "      - 139.0\n",
            "      - 60.0\n",
            "      - 112.0\n",
            "      - 152.0\n",
            "      - 31.0\n",
            "      - 58.0\n",
            "      - 155.0\n",
            "      - 30.0\n",
            "      - 23.0\n",
            "      - 15.0\n",
            "      - 190.0\n",
            "      - 75.0\n",
            "      - 80.0\n",
            "      - 86.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 80.0\n",
            "      - 170.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10014324895137872\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0765691188797603\n",
            "      mean_inference_ms: 0.9752513260731878\n",
            "      mean_raw_obs_processing_ms: 0.10737176558321945\n",
            "    timesteps_this_iter: 2132\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 123.25117492675781\n",
            "          policy_loss: 130.8037872314453\n",
            "          var_gnorm: 26.29884147644043\n",
            "          vf_explained_var: 0.05982768535614014\n",
            "          vf_loss: 11757.814453125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 268400\n",
            "    num_agent_steps_trained: 268400\n",
            "    num_env_steps_sampled: 268400\n",
            "    num_env_steps_trained: 268400\n",
            "  iterations_since_restore: 52\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 268400\n",
            "  num_agent_steps_trained: 268400\n",
            "  num_env_steps_sampled: 268400\n",
            "  num_env_steps_sampled_this_iter: 1000\n",
            "  num_env_steps_trained: 268400\n",
            "  num_env_steps_trained_this_iter: 1000\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.95555555555555\n",
            "    ram_util_percent: 21.7\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.138202628445439\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10792995868832372\n",
            "    mean_inference_ms: 1.3122019648196344\n",
            "    mean_raw_obs_processing_ms: 0.2479214924860839\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 92.62\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 92.62\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 9\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 42\n",
            "      - 107\n",
            "      - 42\n",
            "      - 36\n",
            "      - 88\n",
            "      - 130\n",
            "      - 53\n",
            "      - 101\n",
            "      - 128\n",
            "      - 68\n",
            "      - 25\n",
            "      - 95\n",
            "      - 63\n",
            "      - 141\n",
            "      - 38\n",
            "      - 128\n",
            "      - 71\n",
            "      - 30\n",
            "      - 47\n",
            "      - 142\n",
            "      - 71\n",
            "      - 116\n",
            "      - 34\n",
            "      - 200\n",
            "      - 69\n",
            "      - 64\n",
            "      - 128\n",
            "      - 150\n",
            "      - 129\n",
            "      - 97\n",
            "      - 52\n",
            "      - 31\n",
            "      - 16\n",
            "      - 39\n",
            "      - 27\n",
            "      - 74\n",
            "      - 70\n",
            "      - 137\n",
            "      - 78\n",
            "      - 105\n",
            "      - 73\n",
            "      - 60\n",
            "      - 54\n",
            "      - 200\n",
            "      - 81\n",
            "      - 38\n",
            "      - 22\n",
            "      - 86\n",
            "      - 37\n",
            "      - 127\n",
            "      - 200\n",
            "      - 17\n",
            "      - 72\n",
            "      - 46\n",
            "      - 114\n",
            "      - 81\n",
            "      - 44\n",
            "      - 43\n",
            "      - 16\n",
            "      - 111\n",
            "      - 136\n",
            "      - 136\n",
            "      - 46\n",
            "      - 30\n",
            "      - 111\n",
            "      - 154\n",
            "      - 151\n",
            "      - 129\n",
            "      - 102\n",
            "      - 117\n",
            "      - 49\n",
            "      - 175\n",
            "      - 48\n",
            "      - 35\n",
            "      - 116\n",
            "      - 39\n",
            "      - 114\n",
            "      - 109\n",
            "      - 200\n",
            "      - 124\n",
            "      - 146\n",
            "      - 43\n",
            "      - 133\n",
            "      - 27\n",
            "      - 81\n",
            "      - 65\n",
            "      - 200\n",
            "      - 153\n",
            "      - 83\n",
            "      - 66\n",
            "      - 200\n",
            "      - 200\n",
            "      - 180\n",
            "      - 48\n",
            "      - 161\n",
            "      - 83\n",
            "      - 110\n",
            "      - 155\n",
            "      - 94\n",
            "      - 129\n",
            "      episode_reward:\n",
            "      - 42.0\n",
            "      - 107.0\n",
            "      - 42.0\n",
            "      - 36.0\n",
            "      - 88.0\n",
            "      - 130.0\n",
            "      - 53.0\n",
            "      - 101.0\n",
            "      - 128.0\n",
            "      - 68.0\n",
            "      - 25.0\n",
            "      - 95.0\n",
            "      - 63.0\n",
            "      - 141.0\n",
            "      - 38.0\n",
            "      - 128.0\n",
            "      - 71.0\n",
            "      - 30.0\n",
            "      - 47.0\n",
            "      - 142.0\n",
            "      - 71.0\n",
            "      - 116.0\n",
            "      - 34.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 64.0\n",
            "      - 128.0\n",
            "      - 150.0\n",
            "      - 129.0\n",
            "      - 97.0\n",
            "      - 52.0\n",
            "      - 31.0\n",
            "      - 16.0\n",
            "      - 39.0\n",
            "      - 27.0\n",
            "      - 74.0\n",
            "      - 70.0\n",
            "      - 137.0\n",
            "      - 78.0\n",
            "      - 105.0\n",
            "      - 73.0\n",
            "      - 60.0\n",
            "      - 54.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 38.0\n",
            "      - 22.0\n",
            "      - 86.0\n",
            "      - 37.0\n",
            "      - 127.0\n",
            "      - 200.0\n",
            "      - 17.0\n",
            "      - 72.0\n",
            "      - 46.0\n",
            "      - 114.0\n",
            "      - 81.0\n",
            "      - 44.0\n",
            "      - 43.0\n",
            "      - 16.0\n",
            "      - 111.0\n",
            "      - 136.0\n",
            "      - 136.0\n",
            "      - 46.0\n",
            "      - 30.0\n",
            "      - 111.0\n",
            "      - 154.0\n",
            "      - 151.0\n",
            "      - 129.0\n",
            "      - 102.0\n",
            "      - 117.0\n",
            "      - 49.0\n",
            "      - 175.0\n",
            "      - 48.0\n",
            "      - 35.0\n",
            "      - 116.0\n",
            "      - 39.0\n",
            "      - 114.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 146.0\n",
            "      - 43.0\n",
            "      - 133.0\n",
            "      - 27.0\n",
            "      - 81.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 83.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 48.0\n",
            "      - 161.0\n",
            "      - 83.0\n",
            "      - 110.0\n",
            "      - 155.0\n",
            "      - 94.0\n",
            "      - 129.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.138202628445439\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10792995868832372\n",
            "      mean_inference_ms: 1.3122019648196344\n",
            "      mean_raw_obs_processing_ms: 0.2479214924860839\n",
            "  time_since_restore: 560.4538297653198\n",
            "  time_this_iter_s: 12.480683088302612\n",
            "  time_total_s: 560.4538297653198\n",
            "  timers:\n",
            "    learn_throughput: 31991.579\n",
            "    learn_time_ms: 6.252\n",
            "    load_throughput: 1081145.508\n",
            "    load_time_ms: 0.185\n",
            "    training_iteration_time_ms: 221.597\n",
            "    update_time_ms: 4.243\n",
            "  timestamp: 1656954432\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 268400\n",
            "  training_iteration: 52\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:17 (running for 00:09:46.83)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         560.454</td><td style=\"text-align: right;\">268400</td><td style=\"text-align: right;\">   92.62</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">             92.62</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:22 (running for 00:09:51.95)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         560.454</td><td style=\"text-align: right;\">268400</td><td style=\"text-align: right;\">   92.62</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">             92.62</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 277600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 277600\n",
            "    num_agent_steps_trained: 277600\n",
            "    num_env_steps_sampled: 277600\n",
            "    num_env_steps_trained: 277600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-07-22\n",
            "  done: false\n",
            "  episode_len_mean: 96.16\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 96.16\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 96\n",
            "  episodes_total: 3318\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 119.50631713867188\n",
            "          policy_loss: -203.34385681152344\n",
            "          var_gnorm: 26.41245460510254\n",
            "          vf_explained_var: 0.13105762004852295\n",
            "          vf_loss: 13026.52734375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 277600\n",
            "    num_agent_steps_trained: 277600\n",
            "    num_env_steps_sampled: 277600\n",
            "    num_env_steps_trained: 277600\n",
            "  iterations_since_restore: 53\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 277600\n",
            "  num_agent_steps_trained: 277600\n",
            "  num_env_steps_sampled: 277600\n",
            "  num_env_steps_sampled_this_iter: 9200\n",
            "  num_env_steps_trained: 277600\n",
            "  num_env_steps_trained_this_iter: 9200\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.87333333333335\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13843453330395752\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10801831887909913\n",
            "    mean_inference_ms: 1.3131309609028567\n",
            "    mean_raw_obs_processing_ms: 0.24799263811031214\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 96.16\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 96.16\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 96\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 110\n",
            "      - 155\n",
            "      - 94\n",
            "      - 129\n",
            "      - 152\n",
            "      - 97\n",
            "      - 143\n",
            "      - 25\n",
            "      - 168\n",
            "      - 200\n",
            "      - 89\n",
            "      - 141\n",
            "      - 76\n",
            "      - 31\n",
            "      - 121\n",
            "      - 35\n",
            "      - 84\n",
            "      - 42\n",
            "      - 37\n",
            "      - 143\n",
            "      - 149\n",
            "      - 20\n",
            "      - 179\n",
            "      - 130\n",
            "      - 132\n",
            "      - 150\n",
            "      - 123\n",
            "      - 57\n",
            "      - 14\n",
            "      - 118\n",
            "      - 156\n",
            "      - 121\n",
            "      - 29\n",
            "      - 43\n",
            "      - 72\n",
            "      - 104\n",
            "      - 71\n",
            "      - 30\n",
            "      - 140\n",
            "      - 29\n",
            "      - 123\n",
            "      - 24\n",
            "      - 29\n",
            "      - 74\n",
            "      - 147\n",
            "      - 86\n",
            "      - 132\n",
            "      - 142\n",
            "      - 59\n",
            "      - 95\n",
            "      - 38\n",
            "      - 139\n",
            "      - 37\n",
            "      - 146\n",
            "      - 22\n",
            "      - 65\n",
            "      - 36\n",
            "      - 21\n",
            "      - 178\n",
            "      - 93\n",
            "      - 105\n",
            "      - 186\n",
            "      - 92\n",
            "      - 143\n",
            "      - 117\n",
            "      - 47\n",
            "      - 166\n",
            "      - 67\n",
            "      - 24\n",
            "      - 32\n",
            "      - 124\n",
            "      - 48\n",
            "      - 37\n",
            "      - 189\n",
            "      - 46\n",
            "      - 32\n",
            "      - 54\n",
            "      - 189\n",
            "      - 81\n",
            "      - 135\n",
            "      - 26\n",
            "      - 112\n",
            "      - 136\n",
            "      - 57\n",
            "      - 200\n",
            "      - 121\n",
            "      - 200\n",
            "      - 33\n",
            "      - 60\n",
            "      - 200\n",
            "      - 48\n",
            "      - 200\n",
            "      - 169\n",
            "      - 42\n",
            "      - 116\n",
            "      - 132\n",
            "      - 69\n",
            "      - 50\n",
            "      - 49\n",
            "      - 57\n",
            "      episode_reward:\n",
            "      - 110.0\n",
            "      - 155.0\n",
            "      - 94.0\n",
            "      - 129.0\n",
            "      - 152.0\n",
            "      - 97.0\n",
            "      - 143.0\n",
            "      - 25.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 89.0\n",
            "      - 141.0\n",
            "      - 76.0\n",
            "      - 31.0\n",
            "      - 121.0\n",
            "      - 35.0\n",
            "      - 84.0\n",
            "      - 42.0\n",
            "      - 37.0\n",
            "      - 143.0\n",
            "      - 149.0\n",
            "      - 20.0\n",
            "      - 179.0\n",
            "      - 130.0\n",
            "      - 132.0\n",
            "      - 150.0\n",
            "      - 123.0\n",
            "      - 57.0\n",
            "      - 14.0\n",
            "      - 118.0\n",
            "      - 156.0\n",
            "      - 121.0\n",
            "      - 29.0\n",
            "      - 43.0\n",
            "      - 72.0\n",
            "      - 104.0\n",
            "      - 71.0\n",
            "      - 30.0\n",
            "      - 140.0\n",
            "      - 29.0\n",
            "      - 123.0\n",
            "      - 24.0\n",
            "      - 29.0\n",
            "      - 74.0\n",
            "      - 147.0\n",
            "      - 86.0\n",
            "      - 132.0\n",
            "      - 142.0\n",
            "      - 59.0\n",
            "      - 95.0\n",
            "      - 38.0\n",
            "      - 139.0\n",
            "      - 37.0\n",
            "      - 146.0\n",
            "      - 22.0\n",
            "      - 65.0\n",
            "      - 36.0\n",
            "      - 21.0\n",
            "      - 178.0\n",
            "      - 93.0\n",
            "      - 105.0\n",
            "      - 186.0\n",
            "      - 92.0\n",
            "      - 143.0\n",
            "      - 117.0\n",
            "      - 47.0\n",
            "      - 166.0\n",
            "      - 67.0\n",
            "      - 24.0\n",
            "      - 32.0\n",
            "      - 124.0\n",
            "      - 48.0\n",
            "      - 37.0\n",
            "      - 189.0\n",
            "      - 46.0\n",
            "      - 32.0\n",
            "      - 54.0\n",
            "      - 189.0\n",
            "      - 81.0\n",
            "      - 135.0\n",
            "      - 26.0\n",
            "      - 112.0\n",
            "      - 136.0\n",
            "      - 57.0\n",
            "      - 200.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 60.0\n",
            "      - 200.0\n",
            "      - 48.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 42.0\n",
            "      - 116.0\n",
            "      - 132.0\n",
            "      - 69.0\n",
            "      - 50.0\n",
            "      - 49.0\n",
            "      - 57.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13843453330395752\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10801831887909913\n",
            "      mean_inference_ms: 1.3131309609028567\n",
            "      mean_raw_obs_processing_ms: 0.24799263811031214\n",
            "  time_since_restore: 570.5691249370575\n",
            "  time_this_iter_s: 10.115295171737671\n",
            "  time_total_s: 570.5691249370575\n",
            "  timers:\n",
            "    learn_throughput: 31227.717\n",
            "    learn_time_ms: 6.405\n",
            "    load_throughput: 1034226.113\n",
            "    load_time_ms: 0.193\n",
            "    training_iteration_time_ms: 221.25\n",
            "    update_time_ms: 3.626\n",
            "  timestamp: 1656954442\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 277600\n",
            "  training_iteration: 53\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:27 (running for 00:09:57.00)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         570.569</td><td style=\"text-align: right;\">277600</td><td style=\"text-align: right;\">   96.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             96.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:32 (running for 00:10:02.09)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         570.569</td><td style=\"text-align: right;\">277600</td><td style=\"text-align: right;\">   96.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             96.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 278400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 278400\n",
            "    num_agent_steps_trained: 278400\n",
            "    num_env_steps_sampled: 278400\n",
            "    num_env_steps_trained: 278400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-07-33\n",
            "  done: false\n",
            "  episode_len_mean: 94.41\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 94.41\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 7\n",
            "  episodes_total: 3325\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 97.75\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 97.75\n",
            "    episode_reward_min: 18.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 142\n",
            "      - 18\n",
            "      - 71\n",
            "      - 138\n",
            "      - 97\n",
            "      - 111\n",
            "      - 126\n",
            "      - 111\n",
            "      - 29\n",
            "      - 61\n",
            "      - 200\n",
            "      - 119\n",
            "      - 39\n",
            "      - 124\n",
            "      - 116\n",
            "      - 67\n",
            "      - 63\n",
            "      - 163\n",
            "      - 95\n",
            "      - 65\n",
            "      episode_reward:\n",
            "      - 142.0\n",
            "      - 18.0\n",
            "      - 71.0\n",
            "      - 138.0\n",
            "      - 97.0\n",
            "      - 111.0\n",
            "      - 126.0\n",
            "      - 111.0\n",
            "      - 29.0\n",
            "      - 61.0\n",
            "      - 200.0\n",
            "      - 119.0\n",
            "      - 39.0\n",
            "      - 124.0\n",
            "      - 116.0\n",
            "      - 67.0\n",
            "      - 63.0\n",
            "      - 163.0\n",
            "      - 95.0\n",
            "      - 65.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10010727310508391\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07651778616656964\n",
            "      mean_inference_ms: 0.974562877434791\n",
            "      mean_raw_obs_processing_ms: 0.10725548783117497\n",
            "    timesteps_this_iter: 1955\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 124.17286682128906\n",
            "          policy_loss: 213.46267700195312\n",
            "          var_gnorm: 26.420732498168945\n",
            "          vf_explained_var: 0.0812755823135376\n",
            "          vf_loss: 10847.46875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 278400\n",
            "    num_agent_steps_trained: 278400\n",
            "    num_env_steps_sampled: 278400\n",
            "    num_env_steps_trained: 278400\n",
            "  iterations_since_restore: 54\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 278400\n",
            "  num_agent_steps_trained: 278400\n",
            "  num_env_steps_sampled: 278400\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 278400\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.70666666666668\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13842796396627877\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10802931575741344\n",
            "    mean_inference_ms: 1.3130772673844644\n",
            "    mean_raw_obs_processing_ms: 0.24801916744110014\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 94.41\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 94.41\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 7\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 25\n",
            "      - 168\n",
            "      - 200\n",
            "      - 89\n",
            "      - 141\n",
            "      - 76\n",
            "      - 31\n",
            "      - 121\n",
            "      - 35\n",
            "      - 84\n",
            "      - 42\n",
            "      - 37\n",
            "      - 143\n",
            "      - 149\n",
            "      - 20\n",
            "      - 179\n",
            "      - 130\n",
            "      - 132\n",
            "      - 150\n",
            "      - 123\n",
            "      - 57\n",
            "      - 14\n",
            "      - 118\n",
            "      - 156\n",
            "      - 121\n",
            "      - 29\n",
            "      - 43\n",
            "      - 72\n",
            "      - 104\n",
            "      - 71\n",
            "      - 30\n",
            "      - 140\n",
            "      - 29\n",
            "      - 123\n",
            "      - 24\n",
            "      - 29\n",
            "      - 74\n",
            "      - 147\n",
            "      - 86\n",
            "      - 132\n",
            "      - 142\n",
            "      - 59\n",
            "      - 95\n",
            "      - 38\n",
            "      - 139\n",
            "      - 37\n",
            "      - 146\n",
            "      - 22\n",
            "      - 65\n",
            "      - 36\n",
            "      - 21\n",
            "      - 178\n",
            "      - 93\n",
            "      - 105\n",
            "      - 186\n",
            "      - 92\n",
            "      - 143\n",
            "      - 117\n",
            "      - 47\n",
            "      - 166\n",
            "      - 67\n",
            "      - 24\n",
            "      - 32\n",
            "      - 124\n",
            "      - 48\n",
            "      - 37\n",
            "      - 189\n",
            "      - 46\n",
            "      - 32\n",
            "      - 54\n",
            "      - 189\n",
            "      - 81\n",
            "      - 135\n",
            "      - 26\n",
            "      - 112\n",
            "      - 136\n",
            "      - 57\n",
            "      - 200\n",
            "      - 121\n",
            "      - 200\n",
            "      - 33\n",
            "      - 60\n",
            "      - 200\n",
            "      - 48\n",
            "      - 200\n",
            "      - 169\n",
            "      - 42\n",
            "      - 116\n",
            "      - 132\n",
            "      - 69\n",
            "      - 50\n",
            "      - 49\n",
            "      - 57\n",
            "      - 121\n",
            "      - 200\n",
            "      - 79\n",
            "      - 67\n",
            "      - 33\n",
            "      - 161\n",
            "      - 44\n",
            "      episode_reward:\n",
            "      - 25.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 89.0\n",
            "      - 141.0\n",
            "      - 76.0\n",
            "      - 31.0\n",
            "      - 121.0\n",
            "      - 35.0\n",
            "      - 84.0\n",
            "      - 42.0\n",
            "      - 37.0\n",
            "      - 143.0\n",
            "      - 149.0\n",
            "      - 20.0\n",
            "      - 179.0\n",
            "      - 130.0\n",
            "      - 132.0\n",
            "      - 150.0\n",
            "      - 123.0\n",
            "      - 57.0\n",
            "      - 14.0\n",
            "      - 118.0\n",
            "      - 156.0\n",
            "      - 121.0\n",
            "      - 29.0\n",
            "      - 43.0\n",
            "      - 72.0\n",
            "      - 104.0\n",
            "      - 71.0\n",
            "      - 30.0\n",
            "      - 140.0\n",
            "      - 29.0\n",
            "      - 123.0\n",
            "      - 24.0\n",
            "      - 29.0\n",
            "      - 74.0\n",
            "      - 147.0\n",
            "      - 86.0\n",
            "      - 132.0\n",
            "      - 142.0\n",
            "      - 59.0\n",
            "      - 95.0\n",
            "      - 38.0\n",
            "      - 139.0\n",
            "      - 37.0\n",
            "      - 146.0\n",
            "      - 22.0\n",
            "      - 65.0\n",
            "      - 36.0\n",
            "      - 21.0\n",
            "      - 178.0\n",
            "      - 93.0\n",
            "      - 105.0\n",
            "      - 186.0\n",
            "      - 92.0\n",
            "      - 143.0\n",
            "      - 117.0\n",
            "      - 47.0\n",
            "      - 166.0\n",
            "      - 67.0\n",
            "      - 24.0\n",
            "      - 32.0\n",
            "      - 124.0\n",
            "      - 48.0\n",
            "      - 37.0\n",
            "      - 189.0\n",
            "      - 46.0\n",
            "      - 32.0\n",
            "      - 54.0\n",
            "      - 189.0\n",
            "      - 81.0\n",
            "      - 135.0\n",
            "      - 26.0\n",
            "      - 112.0\n",
            "      - 136.0\n",
            "      - 57.0\n",
            "      - 200.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 60.0\n",
            "      - 200.0\n",
            "      - 48.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 42.0\n",
            "      - 116.0\n",
            "      - 132.0\n",
            "      - 69.0\n",
            "      - 50.0\n",
            "      - 49.0\n",
            "      - 57.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 79.0\n",
            "      - 67.0\n",
            "      - 33.0\n",
            "      - 161.0\n",
            "      - 44.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13842796396627877\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10802931575741344\n",
            "      mean_inference_ms: 1.3130772673844644\n",
            "      mean_raw_obs_processing_ms: 0.24801916744110014\n",
            "  time_since_restore: 581.4791369438171\n",
            "  time_this_iter_s: 10.910012006759644\n",
            "  time_total_s: 581.4791369438171\n",
            "  timers:\n",
            "    learn_throughput: 30732.568\n",
            "    learn_time_ms: 6.508\n",
            "    load_throughput: 1033079.803\n",
            "    load_time_ms: 0.194\n",
            "    training_iteration_time_ms: 218.852\n",
            "    update_time_ms: 3.783\n",
            "  timestamp: 1656954453\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 278400\n",
            "  training_iteration: 54\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:38 (running for 00:10:07.94)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         581.479</td><td style=\"text-align: right;\">278400</td><td style=\"text-align: right;\">   94.41</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             94.41</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:43 (running for 00:10:13.09)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         581.479</td><td style=\"text-align: right;\">278400</td><td style=\"text-align: right;\">   94.41</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">             94.41</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 288000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 288000\n",
            "    num_agent_steps_trained: 288000\n",
            "    num_env_steps_sampled: 288000\n",
            "    num_env_steps_trained: 288000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-07-43\n",
            "  done: false\n",
            "  episode_len_mean: 104.57\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 104.57\n",
            "  episode_reward_min: 17.0\n",
            "  episodes_this_iter: 90\n",
            "  episodes_total: 3415\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.72529602050781\n",
            "          policy_loss: 584.95654296875\n",
            "          var_gnorm: 26.547040939331055\n",
            "          vf_explained_var: 0.09087622165679932\n",
            "          vf_loss: 8512.1904296875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 288000\n",
            "    num_agent_steps_trained: 288000\n",
            "    num_env_steps_sampled: 288000\n",
            "    num_env_steps_trained: 288000\n",
            "  iterations_since_restore: 55\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 288000\n",
            "  num_agent_steps_trained: 288000\n",
            "  num_env_steps_sampled: 288000\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 288000\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.82666666666668\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13834539172020044\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10800728599035452\n",
            "    mean_inference_ms: 1.3119136008373544\n",
            "    mean_raw_obs_processing_ms: 0.24785162925500678\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 104.57\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 104.57\n",
            "    episode_reward_min: 17.0\n",
            "    episodes_this_iter: 90\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 50\n",
            "      - 49\n",
            "      - 57\n",
            "      - 121\n",
            "      - 200\n",
            "      - 79\n",
            "      - 67\n",
            "      - 33\n",
            "      - 161\n",
            "      - 44\n",
            "      - 154\n",
            "      - 116\n",
            "      - 131\n",
            "      - 177\n",
            "      - 119\n",
            "      - 158\n",
            "      - 191\n",
            "      - 125\n",
            "      - 146\n",
            "      - 131\n",
            "      - 150\n",
            "      - 139\n",
            "      - 54\n",
            "      - 23\n",
            "      - 163\n",
            "      - 17\n",
            "      - 124\n",
            "      - 25\n",
            "      - 200\n",
            "      - 90\n",
            "      - 199\n",
            "      - 200\n",
            "      - 60\n",
            "      - 137\n",
            "      - 200\n",
            "      - 54\n",
            "      - 168\n",
            "      - 36\n",
            "      - 152\n",
            "      - 40\n",
            "      - 50\n",
            "      - 21\n",
            "      - 31\n",
            "      - 25\n",
            "      - 79\n",
            "      - 34\n",
            "      - 106\n",
            "      - 101\n",
            "      - 169\n",
            "      - 65\n",
            "      - 200\n",
            "      - 71\n",
            "      - 88\n",
            "      - 52\n",
            "      - 52\n",
            "      - 200\n",
            "      - 122\n",
            "      - 139\n",
            "      - 89\n",
            "      - 20\n",
            "      - 120\n",
            "      - 25\n",
            "      - 26\n",
            "      - 125\n",
            "      - 200\n",
            "      - 122\n",
            "      - 97\n",
            "      - 40\n",
            "      - 75\n",
            "      - 137\n",
            "      - 200\n",
            "      - 74\n",
            "      - 152\n",
            "      - 27\n",
            "      - 60\n",
            "      - 94\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 33\n",
            "      - 198\n",
            "      - 36\n",
            "      - 140\n",
            "      - 101\n",
            "      - 66\n",
            "      - 137\n",
            "      - 200\n",
            "      - 125\n",
            "      - 45\n",
            "      - 119\n",
            "      - 95\n",
            "      - 135\n",
            "      - 116\n",
            "      - 91\n",
            "      - 18\n",
            "      - 134\n",
            "      - 90\n",
            "      - 76\n",
            "      - 37\n",
            "      - 184\n",
            "      episode_reward:\n",
            "      - 50.0\n",
            "      - 49.0\n",
            "      - 57.0\n",
            "      - 121.0\n",
            "      - 200.0\n",
            "      - 79.0\n",
            "      - 67.0\n",
            "      - 33.0\n",
            "      - 161.0\n",
            "      - 44.0\n",
            "      - 154.0\n",
            "      - 116.0\n",
            "      - 131.0\n",
            "      - 177.0\n",
            "      - 119.0\n",
            "      - 158.0\n",
            "      - 191.0\n",
            "      - 125.0\n",
            "      - 146.0\n",
            "      - 131.0\n",
            "      - 150.0\n",
            "      - 139.0\n",
            "      - 54.0\n",
            "      - 23.0\n",
            "      - 163.0\n",
            "      - 17.0\n",
            "      - 124.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 60.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 54.0\n",
            "      - 168.0\n",
            "      - 36.0\n",
            "      - 152.0\n",
            "      - 40.0\n",
            "      - 50.0\n",
            "      - 21.0\n",
            "      - 31.0\n",
            "      - 25.0\n",
            "      - 79.0\n",
            "      - 34.0\n",
            "      - 106.0\n",
            "      - 101.0\n",
            "      - 169.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 71.0\n",
            "      - 88.0\n",
            "      - 52.0\n",
            "      - 52.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 139.0\n",
            "      - 89.0\n",
            "      - 20.0\n",
            "      - 120.0\n",
            "      - 25.0\n",
            "      - 26.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 97.0\n",
            "      - 40.0\n",
            "      - 75.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 74.0\n",
            "      - 152.0\n",
            "      - 27.0\n",
            "      - 60.0\n",
            "      - 94.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 198.0\n",
            "      - 36.0\n",
            "      - 140.0\n",
            "      - 101.0\n",
            "      - 66.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 45.0\n",
            "      - 119.0\n",
            "      - 95.0\n",
            "      - 135.0\n",
            "      - 116.0\n",
            "      - 91.0\n",
            "      - 18.0\n",
            "      - 134.0\n",
            "      - 90.0\n",
            "      - 76.0\n",
            "      - 37.0\n",
            "      - 184.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13834539172020044\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10800728599035452\n",
            "      mean_inference_ms: 1.3119136008373544\n",
            "      mean_raw_obs_processing_ms: 0.24785162925500678\n",
            "  time_since_restore: 591.6274721622467\n",
            "  time_this_iter_s: 10.148335218429565\n",
            "  time_total_s: 591.6274721622467\n",
            "  timers:\n",
            "    learn_throughput: 30699.389\n",
            "    learn_time_ms: 6.515\n",
            "    load_throughput: 1026757.405\n",
            "    load_time_ms: 0.195\n",
            "    training_iteration_time_ms: 216.512\n",
            "    update_time_ms: 4.095\n",
            "  timestamp: 1656954463\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 288000\n",
            "  training_iteration: 55\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:48 (running for 00:10:18.14)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         591.627</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">  104.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            104.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:07:53 (running for 00:10:23.25)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         591.627</td><td style=\"text-align: right;\">288000</td><td style=\"text-align: right;\">  104.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  17</td><td style=\"text-align: right;\">            104.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 288800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 288800\n",
            "    num_agent_steps_trained: 288800\n",
            "    num_env_steps_sampled: 288800\n",
            "    num_env_steps_trained: 288800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-07-56\n",
            "  done: false\n",
            "  episode_len_mean: 106.45\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 106.45\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 7\n",
            "  episodes_total: 3422\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 126.35\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 126.35\n",
            "    episode_reward_min: 34.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 184\n",
            "      - 200\n",
            "      - 148\n",
            "      - 109\n",
            "      - 129\n",
            "      - 40\n",
            "      - 143\n",
            "      - 73\n",
            "      - 87\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 94\n",
            "      - 34\n",
            "      - 82\n",
            "      - 115\n",
            "      - 113\n",
            "      - 158\n",
            "      - 200\n",
            "      - 43\n",
            "      episode_reward:\n",
            "      - 184.0\n",
            "      - 200.0\n",
            "      - 148.0\n",
            "      - 109.0\n",
            "      - 129.0\n",
            "      - 40.0\n",
            "      - 143.0\n",
            "      - 73.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 94.0\n",
            "      - 34.0\n",
            "      - 82.0\n",
            "      - 115.0\n",
            "      - 113.0\n",
            "      - 158.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10038547670919118\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07676572935351349\n",
            "      mean_inference_ms: 0.9780733016325918\n",
            "      mean_raw_obs_processing_ms: 0.10754213797778575\n",
            "    timesteps_this_iter: 2527\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 125.86883544921875\n",
            "          policy_loss: 594.5381469726562\n",
            "          var_gnorm: 26.555994033813477\n",
            "          vf_explained_var: 0.13000833988189697\n",
            "          vf_loss: 7561.39697265625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 288800\n",
            "    num_agent_steps_trained: 288800\n",
            "    num_env_steps_sampled: 288800\n",
            "    num_env_steps_trained: 288800\n",
            "  iterations_since_restore: 56\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 288800\n",
            "  num_agent_steps_trained: 288800\n",
            "  num_env_steps_sampled: 288800\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 288800\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 74.90555555555557\n",
            "    ram_util_percent: 21.71111111111111\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13838184706939155\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10799148580418338\n",
            "    mean_inference_ms: 1.312121679354509\n",
            "    mean_raw_obs_processing_ms: 0.24778214352920158\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 106.45\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 106.45\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 7\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 33\n",
            "      - 161\n",
            "      - 44\n",
            "      - 154\n",
            "      - 116\n",
            "      - 131\n",
            "      - 177\n",
            "      - 119\n",
            "      - 158\n",
            "      - 191\n",
            "      - 125\n",
            "      - 146\n",
            "      - 131\n",
            "      - 150\n",
            "      - 139\n",
            "      - 54\n",
            "      - 23\n",
            "      - 163\n",
            "      - 17\n",
            "      - 124\n",
            "      - 25\n",
            "      - 200\n",
            "      - 90\n",
            "      - 199\n",
            "      - 200\n",
            "      - 60\n",
            "      - 137\n",
            "      - 200\n",
            "      - 54\n",
            "      - 168\n",
            "      - 36\n",
            "      - 152\n",
            "      - 40\n",
            "      - 50\n",
            "      - 21\n",
            "      - 31\n",
            "      - 25\n",
            "      - 79\n",
            "      - 34\n",
            "      - 106\n",
            "      - 101\n",
            "      - 169\n",
            "      - 65\n",
            "      - 200\n",
            "      - 71\n",
            "      - 88\n",
            "      - 52\n",
            "      - 52\n",
            "      - 200\n",
            "      - 122\n",
            "      - 139\n",
            "      - 89\n",
            "      - 20\n",
            "      - 120\n",
            "      - 25\n",
            "      - 26\n",
            "      - 125\n",
            "      - 200\n",
            "      - 122\n",
            "      - 97\n",
            "      - 40\n",
            "      - 75\n",
            "      - 137\n",
            "      - 200\n",
            "      - 74\n",
            "      - 152\n",
            "      - 27\n",
            "      - 60\n",
            "      - 94\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 33\n",
            "      - 198\n",
            "      - 36\n",
            "      - 140\n",
            "      - 101\n",
            "      - 66\n",
            "      - 137\n",
            "      - 200\n",
            "      - 125\n",
            "      - 45\n",
            "      - 119\n",
            "      - 95\n",
            "      - 135\n",
            "      - 116\n",
            "      - 91\n",
            "      - 18\n",
            "      - 134\n",
            "      - 90\n",
            "      - 76\n",
            "      - 37\n",
            "      - 184\n",
            "      - 131\n",
            "      - 61\n",
            "      - 14\n",
            "      - 130\n",
            "      - 111\n",
            "      - 164\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 33.0\n",
            "      - 161.0\n",
            "      - 44.0\n",
            "      - 154.0\n",
            "      - 116.0\n",
            "      - 131.0\n",
            "      - 177.0\n",
            "      - 119.0\n",
            "      - 158.0\n",
            "      - 191.0\n",
            "      - 125.0\n",
            "      - 146.0\n",
            "      - 131.0\n",
            "      - 150.0\n",
            "      - 139.0\n",
            "      - 54.0\n",
            "      - 23.0\n",
            "      - 163.0\n",
            "      - 17.0\n",
            "      - 124.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 199.0\n",
            "      - 200.0\n",
            "      - 60.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 54.0\n",
            "      - 168.0\n",
            "      - 36.0\n",
            "      - 152.0\n",
            "      - 40.0\n",
            "      - 50.0\n",
            "      - 21.0\n",
            "      - 31.0\n",
            "      - 25.0\n",
            "      - 79.0\n",
            "      - 34.0\n",
            "      - 106.0\n",
            "      - 101.0\n",
            "      - 169.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 71.0\n",
            "      - 88.0\n",
            "      - 52.0\n",
            "      - 52.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 139.0\n",
            "      - 89.0\n",
            "      - 20.0\n",
            "      - 120.0\n",
            "      - 25.0\n",
            "      - 26.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 97.0\n",
            "      - 40.0\n",
            "      - 75.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 74.0\n",
            "      - 152.0\n",
            "      - 27.0\n",
            "      - 60.0\n",
            "      - 94.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 198.0\n",
            "      - 36.0\n",
            "      - 140.0\n",
            "      - 101.0\n",
            "      - 66.0\n",
            "      - 137.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 45.0\n",
            "      - 119.0\n",
            "      - 95.0\n",
            "      - 135.0\n",
            "      - 116.0\n",
            "      - 91.0\n",
            "      - 18.0\n",
            "      - 134.0\n",
            "      - 90.0\n",
            "      - 76.0\n",
            "      - 37.0\n",
            "      - 184.0\n",
            "      - 131.0\n",
            "      - 61.0\n",
            "      - 14.0\n",
            "      - 130.0\n",
            "      - 111.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13838184706939155\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10799148580418338\n",
            "      mean_inference_ms: 1.312121679354509\n",
            "      mean_raw_obs_processing_ms: 0.24778214352920158\n",
            "  time_since_restore: 604.2669551372528\n",
            "  time_this_iter_s: 12.639482975006104\n",
            "  time_total_s: 604.2669551372528\n",
            "  timers:\n",
            "    learn_throughput: 29978.479\n",
            "    learn_time_ms: 6.671\n",
            "    load_throughput: 976668.762\n",
            "    load_time_ms: 0.205\n",
            "    training_iteration_time_ms: 242.839\n",
            "    update_time_ms: 5.814\n",
            "  timestamp: 1656954476\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 288800\n",
            "  training_iteration: 56\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:01 (running for 00:10:30.82)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         604.267</td><td style=\"text-align: right;\">288800</td><td style=\"text-align: right;\">  106.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            106.45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:06 (running for 00:10:35.94)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         604.267</td><td style=\"text-align: right;\">288800</td><td style=\"text-align: right;\">  106.45</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            106.45</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 298400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 298400\n",
            "    num_agent_steps_trained: 298400\n",
            "    num_env_steps_sampled: 298400\n",
            "    num_env_steps_trained: 298400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-08-06\n",
            "  done: false\n",
            "  episode_len_mean: 106.58\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 106.58\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 92\n",
            "  episodes_total: 3514\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 126.92277526855469\n",
            "          policy_loss: 220.57931518554688\n",
            "          var_gnorm: 26.67081642150879\n",
            "          vf_explained_var: 0.07040625810623169\n",
            "          vf_loss: 12449.8916015625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 298400\n",
            "    num_agent_steps_trained: 298400\n",
            "    num_env_steps_sampled: 298400\n",
            "    num_env_steps_trained: 298400\n",
            "  iterations_since_restore: 57\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 298400\n",
            "  num_agent_steps_trained: 298400\n",
            "  num_env_steps_sampled: 298400\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 298400\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.6214285714286\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1384448825734416\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10798951787257056\n",
            "    mean_inference_ms: 1.312439454565396\n",
            "    mean_raw_obs_processing_ms: 0.2475113814655078\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 106.58\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 106.58\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 92\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 184\n",
            "      - 131\n",
            "      - 61\n",
            "      - 14\n",
            "      - 130\n",
            "      - 111\n",
            "      - 164\n",
            "      - 200\n",
            "      - 139\n",
            "      - 200\n",
            "      - 37\n",
            "      - 27\n",
            "      - 59\n",
            "      - 200\n",
            "      - 24\n",
            "      - 126\n",
            "      - 67\n",
            "      - 66\n",
            "      - 171\n",
            "      - 155\n",
            "      - 200\n",
            "      - 39\n",
            "      - 130\n",
            "      - 23\n",
            "      - 62\n",
            "      - 92\n",
            "      - 53\n",
            "      - 53\n",
            "      - 112\n",
            "      - 101\n",
            "      - 113\n",
            "      - 25\n",
            "      - 200\n",
            "      - 84\n",
            "      - 26\n",
            "      - 174\n",
            "      - 164\n",
            "      - 23\n",
            "      - 133\n",
            "      - 35\n",
            "      - 145\n",
            "      - 200\n",
            "      - 57\n",
            "      - 37\n",
            "      - 35\n",
            "      - 175\n",
            "      - 82\n",
            "      - 45\n",
            "      - 33\n",
            "      - 23\n",
            "      - 137\n",
            "      - 98\n",
            "      - 149\n",
            "      - 142\n",
            "      - 164\n",
            "      - 56\n",
            "      - 37\n",
            "      - 78\n",
            "      - 124\n",
            "      - 124\n",
            "      - 27\n",
            "      - 82\n",
            "      - 158\n",
            "      - 100\n",
            "      - 76\n",
            "      - 83\n",
            "      - 193\n",
            "      - 194\n",
            "      - 140\n",
            "      - 152\n",
            "      - 186\n",
            "      - 88\n",
            "      - 113\n",
            "      - 85\n",
            "      - 200\n",
            "      - 166\n",
            "      - 53\n",
            "      - 128\n",
            "      - 31\n",
            "      - 108\n",
            "      - 78\n",
            "      - 42\n",
            "      - 133\n",
            "      - 158\n",
            "      - 132\n",
            "      - 25\n",
            "      - 94\n",
            "      - 106\n",
            "      - 77\n",
            "      - 87\n",
            "      - 85\n",
            "      - 156\n",
            "      - 180\n",
            "      - 97\n",
            "      - 164\n",
            "      - 200\n",
            "      - 99\n",
            "      - 32\n",
            "      - 164\n",
            "      - 137\n",
            "      episode_reward:\n",
            "      - 184.0\n",
            "      - 131.0\n",
            "      - 61.0\n",
            "      - 14.0\n",
            "      - 130.0\n",
            "      - 111.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 37.0\n",
            "      - 27.0\n",
            "      - 59.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 126.0\n",
            "      - 67.0\n",
            "      - 66.0\n",
            "      - 171.0\n",
            "      - 155.0\n",
            "      - 200.0\n",
            "      - 39.0\n",
            "      - 130.0\n",
            "      - 23.0\n",
            "      - 62.0\n",
            "      - 92.0\n",
            "      - 53.0\n",
            "      - 53.0\n",
            "      - 112.0\n",
            "      - 101.0\n",
            "      - 113.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 84.0\n",
            "      - 26.0\n",
            "      - 174.0\n",
            "      - 164.0\n",
            "      - 23.0\n",
            "      - 133.0\n",
            "      - 35.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 57.0\n",
            "      - 37.0\n",
            "      - 35.0\n",
            "      - 175.0\n",
            "      - 82.0\n",
            "      - 45.0\n",
            "      - 33.0\n",
            "      - 23.0\n",
            "      - 137.0\n",
            "      - 98.0\n",
            "      - 149.0\n",
            "      - 142.0\n",
            "      - 164.0\n",
            "      - 56.0\n",
            "      - 37.0\n",
            "      - 78.0\n",
            "      - 124.0\n",
            "      - 124.0\n",
            "      - 27.0\n",
            "      - 82.0\n",
            "      - 158.0\n",
            "      - 100.0\n",
            "      - 76.0\n",
            "      - 83.0\n",
            "      - 193.0\n",
            "      - 194.0\n",
            "      - 140.0\n",
            "      - 152.0\n",
            "      - 186.0\n",
            "      - 88.0\n",
            "      - 113.0\n",
            "      - 85.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 53.0\n",
            "      - 128.0\n",
            "      - 31.0\n",
            "      - 108.0\n",
            "      - 78.0\n",
            "      - 42.0\n",
            "      - 133.0\n",
            "      - 158.0\n",
            "      - 132.0\n",
            "      - 25.0\n",
            "      - 94.0\n",
            "      - 106.0\n",
            "      - 77.0\n",
            "      - 87.0\n",
            "      - 85.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 97.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 32.0\n",
            "      - 164.0\n",
            "      - 137.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1384448825734416\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10798951787257056\n",
            "      mean_inference_ms: 1.312439454565396\n",
            "      mean_raw_obs_processing_ms: 0.2475113814655078\n",
            "  time_since_restore: 614.4634735584259\n",
            "  time_this_iter_s: 10.196518421173096\n",
            "  time_total_s: 614.4634735584259\n",
            "  timers:\n",
            "    learn_throughput: 33060.638\n",
            "    learn_time_ms: 6.049\n",
            "    load_throughput: 1048182.931\n",
            "    load_time_ms: 0.191\n",
            "    training_iteration_time_ms: 214.11\n",
            "    update_time_ms: 4.411\n",
            "  timestamp: 1656954486\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 298400\n",
            "  training_iteration: 57\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:11 (running for 00:10:41.04)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         614.463</td><td style=\"text-align: right;\">298400</td><td style=\"text-align: right;\">  106.58</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            106.58</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:16 (running for 00:10:46.14)<br>Memory usage on this node: 2.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         614.463</td><td style=\"text-align: right;\">298400</td><td style=\"text-align: right;\">  106.58</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            106.58</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 299200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 299200\n",
            "    num_agent_steps_trained: 299200\n",
            "    num_env_steps_sampled: 299200\n",
            "    num_env_steps_trained: 299200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-08-17\n",
            "  done: false\n",
            "  episode_len_mean: 104.32\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 104.32\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 9\n",
            "  episodes_total: 3523\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 96.4\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 96.4\n",
            "    episode_reward_min: 20.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 155\n",
            "      - 125\n",
            "      - 24\n",
            "      - 127\n",
            "      - 125\n",
            "      - 98\n",
            "      - 169\n",
            "      - 95\n",
            "      - 200\n",
            "      - 113\n",
            "      - 62\n",
            "      - 54\n",
            "      - 88\n",
            "      - 41\n",
            "      - 43\n",
            "      - 99\n",
            "      - 125\n",
            "      - 20\n",
            "      - 24\n",
            "      - 141\n",
            "      episode_reward:\n",
            "      - 155.0\n",
            "      - 125.0\n",
            "      - 24.0\n",
            "      - 127.0\n",
            "      - 125.0\n",
            "      - 98.0\n",
            "      - 169.0\n",
            "      - 95.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 62.0\n",
            "      - 54.0\n",
            "      - 88.0\n",
            "      - 41.0\n",
            "      - 43.0\n",
            "      - 99.0\n",
            "      - 125.0\n",
            "      - 20.0\n",
            "      - 24.0\n",
            "      - 141.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10029744187901195\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07668394079286606\n",
            "      mean_inference_ms: 0.9769191610700446\n",
            "      mean_raw_obs_processing_ms: 0.10743849334154768\n",
            "    timesteps_this_iter: 1928\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 119.56871795654297\n",
            "          policy_loss: -401.2492370605469\n",
            "          var_gnorm: 26.6790771484375\n",
            "          vf_explained_var: 0.2057594656944275\n",
            "          vf_loss: 15428.0625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 299200\n",
            "    num_agent_steps_trained: 299200\n",
            "    num_env_steps_sampled: 299200\n",
            "    num_env_steps_trained: 299200\n",
            "  iterations_since_restore: 58\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 299200\n",
            "  num_agent_steps_trained: 299200\n",
            "  num_env_steps_sampled: 299200\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 299200\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.925\n",
            "    ram_util_percent: 21.7\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13842860832591705\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10799287230293952\n",
            "    mean_inference_ms: 1.3122804721803862\n",
            "    mean_raw_obs_processing_ms: 0.24751414582642298\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 104.32\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 104.32\n",
            "    episode_reward_min: 23.0\n",
            "    episodes_this_iter: 9\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 37\n",
            "      - 27\n",
            "      - 59\n",
            "      - 200\n",
            "      - 24\n",
            "      - 126\n",
            "      - 67\n",
            "      - 66\n",
            "      - 171\n",
            "      - 155\n",
            "      - 200\n",
            "      - 39\n",
            "      - 130\n",
            "      - 23\n",
            "      - 62\n",
            "      - 92\n",
            "      - 53\n",
            "      - 53\n",
            "      - 112\n",
            "      - 101\n",
            "      - 113\n",
            "      - 25\n",
            "      - 200\n",
            "      - 84\n",
            "      - 26\n",
            "      - 174\n",
            "      - 164\n",
            "      - 23\n",
            "      - 133\n",
            "      - 35\n",
            "      - 145\n",
            "      - 200\n",
            "      - 57\n",
            "      - 37\n",
            "      - 35\n",
            "      - 175\n",
            "      - 82\n",
            "      - 45\n",
            "      - 33\n",
            "      - 23\n",
            "      - 137\n",
            "      - 98\n",
            "      - 149\n",
            "      - 142\n",
            "      - 164\n",
            "      - 56\n",
            "      - 37\n",
            "      - 78\n",
            "      - 124\n",
            "      - 124\n",
            "      - 27\n",
            "      - 82\n",
            "      - 158\n",
            "      - 100\n",
            "      - 76\n",
            "      - 83\n",
            "      - 193\n",
            "      - 194\n",
            "      - 140\n",
            "      - 152\n",
            "      - 186\n",
            "      - 88\n",
            "      - 113\n",
            "      - 85\n",
            "      - 200\n",
            "      - 166\n",
            "      - 53\n",
            "      - 128\n",
            "      - 31\n",
            "      - 108\n",
            "      - 78\n",
            "      - 42\n",
            "      - 133\n",
            "      - 158\n",
            "      - 132\n",
            "      - 25\n",
            "      - 94\n",
            "      - 106\n",
            "      - 77\n",
            "      - 87\n",
            "      - 85\n",
            "      - 156\n",
            "      - 180\n",
            "      - 97\n",
            "      - 164\n",
            "      - 200\n",
            "      - 99\n",
            "      - 32\n",
            "      - 164\n",
            "      - 137\n",
            "      - 159\n",
            "      - 28\n",
            "      - 93\n",
            "      - 138\n",
            "      - 48\n",
            "      - 97\n",
            "      - 121\n",
            "      - 168\n",
            "      - 56\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 37.0\n",
            "      - 27.0\n",
            "      - 59.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 126.0\n",
            "      - 67.0\n",
            "      - 66.0\n",
            "      - 171.0\n",
            "      - 155.0\n",
            "      - 200.0\n",
            "      - 39.0\n",
            "      - 130.0\n",
            "      - 23.0\n",
            "      - 62.0\n",
            "      - 92.0\n",
            "      - 53.0\n",
            "      - 53.0\n",
            "      - 112.0\n",
            "      - 101.0\n",
            "      - 113.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 84.0\n",
            "      - 26.0\n",
            "      - 174.0\n",
            "      - 164.0\n",
            "      - 23.0\n",
            "      - 133.0\n",
            "      - 35.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 57.0\n",
            "      - 37.0\n",
            "      - 35.0\n",
            "      - 175.0\n",
            "      - 82.0\n",
            "      - 45.0\n",
            "      - 33.0\n",
            "      - 23.0\n",
            "      - 137.0\n",
            "      - 98.0\n",
            "      - 149.0\n",
            "      - 142.0\n",
            "      - 164.0\n",
            "      - 56.0\n",
            "      - 37.0\n",
            "      - 78.0\n",
            "      - 124.0\n",
            "      - 124.0\n",
            "      - 27.0\n",
            "      - 82.0\n",
            "      - 158.0\n",
            "      - 100.0\n",
            "      - 76.0\n",
            "      - 83.0\n",
            "      - 193.0\n",
            "      - 194.0\n",
            "      - 140.0\n",
            "      - 152.0\n",
            "      - 186.0\n",
            "      - 88.0\n",
            "      - 113.0\n",
            "      - 85.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 53.0\n",
            "      - 128.0\n",
            "      - 31.0\n",
            "      - 108.0\n",
            "      - 78.0\n",
            "      - 42.0\n",
            "      - 133.0\n",
            "      - 158.0\n",
            "      - 132.0\n",
            "      - 25.0\n",
            "      - 94.0\n",
            "      - 106.0\n",
            "      - 77.0\n",
            "      - 87.0\n",
            "      - 85.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 97.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 32.0\n",
            "      - 164.0\n",
            "      - 137.0\n",
            "      - 159.0\n",
            "      - 28.0\n",
            "      - 93.0\n",
            "      - 138.0\n",
            "      - 48.0\n",
            "      - 97.0\n",
            "      - 121.0\n",
            "      - 168.0\n",
            "      - 56.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13842860832591705\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10799287230293952\n",
            "      mean_inference_ms: 1.3122804721803862\n",
            "      mean_raw_obs_processing_ms: 0.24751414582642298\n",
            "  time_since_restore: 625.6926736831665\n",
            "  time_this_iter_s: 11.2292001247406\n",
            "  time_total_s: 625.6926736831665\n",
            "  timers:\n",
            "    learn_throughput: 31390.262\n",
            "    learn_time_ms: 6.371\n",
            "    load_throughput: 1016677.736\n",
            "    load_time_ms: 0.197\n",
            "    training_iteration_time_ms: 218.251\n",
            "    update_time_ms: 4.418\n",
            "  timestamp: 1656954497\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 299200\n",
            "  training_iteration: 58\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:22 (running for 00:10:52.31)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         625.693</td><td style=\"text-align: right;\">299200</td><td style=\"text-align: right;\">  104.32</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            104.32</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:27 (running for 00:10:57.43)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         625.693</td><td style=\"text-align: right;\">299200</td><td style=\"text-align: right;\">  104.32</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            104.32</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 308800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 308800\n",
            "    num_agent_steps_trained: 308800\n",
            "    num_env_steps_sampled: 308800\n",
            "    num_env_steps_trained: 308800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-08-28\n",
            "  done: false\n",
            "  episode_len_mean: 110.61\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 110.61\n",
            "  episode_reward_min: 24.0\n",
            "  episodes_this_iter: 85\n",
            "  episodes_total: 3608\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 126.92694091796875\n",
            "          policy_loss: 672.43310546875\n",
            "          var_gnorm: 26.772180557250977\n",
            "          vf_explained_var: 0.20266127586364746\n",
            "          vf_loss: 6885.30078125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 308800\n",
            "    num_agent_steps_trained: 308800\n",
            "    num_env_steps_sampled: 308800\n",
            "    num_env_steps_trained: 308800\n",
            "  iterations_since_restore: 59\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 308800\n",
            "  num_agent_steps_trained: 308800\n",
            "  num_env_steps_sampled: 308800\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 308800\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 92.89333333333335\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13832554726889906\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1080105081253823\n",
            "    mean_inference_ms: 1.3111844819228722\n",
            "    mean_raw_obs_processing_ms: 0.2473245589244788\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 110.61\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 110.61\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 85\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 164\n",
            "      - 200\n",
            "      - 99\n",
            "      - 32\n",
            "      - 164\n",
            "      - 137\n",
            "      - 159\n",
            "      - 28\n",
            "      - 93\n",
            "      - 138\n",
            "      - 48\n",
            "      - 97\n",
            "      - 121\n",
            "      - 168\n",
            "      - 56\n",
            "      - 131\n",
            "      - 200\n",
            "      - 46\n",
            "      - 200\n",
            "      - 167\n",
            "      - 57\n",
            "      - 56\n",
            "      - 41\n",
            "      - 41\n",
            "      - 134\n",
            "      - 30\n",
            "      - 79\n",
            "      - 109\n",
            "      - 200\n",
            "      - 49\n",
            "      - 144\n",
            "      - 200\n",
            "      - 68\n",
            "      - 162\n",
            "      - 46\n",
            "      - 158\n",
            "      - 193\n",
            "      - 46\n",
            "      - 138\n",
            "      - 101\n",
            "      - 56\n",
            "      - 149\n",
            "      - 91\n",
            "      - 45\n",
            "      - 101\n",
            "      - 130\n",
            "      - 78\n",
            "      - 47\n",
            "      - 68\n",
            "      - 140\n",
            "      - 164\n",
            "      - 77\n",
            "      - 180\n",
            "      - 42\n",
            "      - 149\n",
            "      - 200\n",
            "      - 134\n",
            "      - 80\n",
            "      - 89\n",
            "      - 133\n",
            "      - 110\n",
            "      - 111\n",
            "      - 105\n",
            "      - 36\n",
            "      - 78\n",
            "      - 150\n",
            "      - 146\n",
            "      - 127\n",
            "      - 200\n",
            "      - 200\n",
            "      - 65\n",
            "      - 132\n",
            "      - 113\n",
            "      - 178\n",
            "      - 152\n",
            "      - 38\n",
            "      - 42\n",
            "      - 39\n",
            "      - 30\n",
            "      - 34\n",
            "      - 36\n",
            "      - 105\n",
            "      - 196\n",
            "      - 136\n",
            "      - 83\n",
            "      - 171\n",
            "      - 47\n",
            "      - 63\n",
            "      - 125\n",
            "      - 43\n",
            "      - 200\n",
            "      - 111\n",
            "      - 103\n",
            "      - 51\n",
            "      - 171\n",
            "      - 24\n",
            "      - 157\n",
            "      - 200\n",
            "      - 168\n",
            "      - 132\n",
            "      episode_reward:\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 99.0\n",
            "      - 32.0\n",
            "      - 164.0\n",
            "      - 137.0\n",
            "      - 159.0\n",
            "      - 28.0\n",
            "      - 93.0\n",
            "      - 138.0\n",
            "      - 48.0\n",
            "      - 97.0\n",
            "      - 121.0\n",
            "      - 168.0\n",
            "      - 56.0\n",
            "      - 131.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 57.0\n",
            "      - 56.0\n",
            "      - 41.0\n",
            "      - 41.0\n",
            "      - 134.0\n",
            "      - 30.0\n",
            "      - 79.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 49.0\n",
            "      - 144.0\n",
            "      - 200.0\n",
            "      - 68.0\n",
            "      - 162.0\n",
            "      - 46.0\n",
            "      - 158.0\n",
            "      - 193.0\n",
            "      - 46.0\n",
            "      - 138.0\n",
            "      - 101.0\n",
            "      - 56.0\n",
            "      - 149.0\n",
            "      - 91.0\n",
            "      - 45.0\n",
            "      - 101.0\n",
            "      - 130.0\n",
            "      - 78.0\n",
            "      - 47.0\n",
            "      - 68.0\n",
            "      - 140.0\n",
            "      - 164.0\n",
            "      - 77.0\n",
            "      - 180.0\n",
            "      - 42.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 134.0\n",
            "      - 80.0\n",
            "      - 89.0\n",
            "      - 133.0\n",
            "      - 110.0\n",
            "      - 111.0\n",
            "      - 105.0\n",
            "      - 36.0\n",
            "      - 78.0\n",
            "      - 150.0\n",
            "      - 146.0\n",
            "      - 127.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 65.0\n",
            "      - 132.0\n",
            "      - 113.0\n",
            "      - 178.0\n",
            "      - 152.0\n",
            "      - 38.0\n",
            "      - 42.0\n",
            "      - 39.0\n",
            "      - 30.0\n",
            "      - 34.0\n",
            "      - 36.0\n",
            "      - 105.0\n",
            "      - 196.0\n",
            "      - 136.0\n",
            "      - 83.0\n",
            "      - 171.0\n",
            "      - 47.0\n",
            "      - 63.0\n",
            "      - 125.0\n",
            "      - 43.0\n",
            "      - 200.0\n",
            "      - 111.0\n",
            "      - 103.0\n",
            "      - 51.0\n",
            "      - 171.0\n",
            "      - 24.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 132.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13832554726889906\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1080105081253823\n",
            "      mean_inference_ms: 1.3111844819228722\n",
            "      mean_raw_obs_processing_ms: 0.2473245589244788\n",
            "  time_since_restore: 635.8951508998871\n",
            "  time_this_iter_s: 10.202477216720581\n",
            "  time_total_s: 635.8951508998871\n",
            "  timers:\n",
            "    learn_throughput: 33269.908\n",
            "    learn_time_ms: 6.011\n",
            "    load_throughput: 971578.411\n",
            "    load_time_ms: 0.206\n",
            "    training_iteration_time_ms: 211.838\n",
            "    update_time_ms: 4.377\n",
            "  timestamp: 1656954508\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 308800\n",
            "  training_iteration: 59\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:33 (running for 00:11:02.55)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         635.895</td><td style=\"text-align: right;\">308800</td><td style=\"text-align: right;\">  110.61</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            110.61</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:38 (running for 00:11:07.63)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         635.895</td><td style=\"text-align: right;\">308800</td><td style=\"text-align: right;\">  110.61</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            110.61</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 309600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 309600\n",
            "    num_agent_steps_trained: 309600\n",
            "    num_env_steps_sampled: 309600\n",
            "    num_env_steps_trained: 309600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-08-40\n",
            "  done: false\n",
            "  episode_len_mean: 108.02\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 108.02\n",
            "  episode_reward_min: 24.0\n",
            "  episodes_this_iter: 10\n",
            "  episodes_total: 3618\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 115.75\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 115.75\n",
            "    episode_reward_min: 22.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 37\n",
            "      - 200\n",
            "      - 139\n",
            "      - 124\n",
            "      - 115\n",
            "      - 45\n",
            "      - 155\n",
            "      - 155\n",
            "      - 200\n",
            "      - 46\n",
            "      - 165\n",
            "      - 43\n",
            "      - 179\n",
            "      - 126\n",
            "      - 22\n",
            "      - 143\n",
            "      - 173\n",
            "      - 179\n",
            "      - 33\n",
            "      - 36\n",
            "      episode_reward:\n",
            "      - 37.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 124.0\n",
            "      - 115.0\n",
            "      - 45.0\n",
            "      - 155.0\n",
            "      - 155.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 165.0\n",
            "      - 43.0\n",
            "      - 179.0\n",
            "      - 126.0\n",
            "      - 22.0\n",
            "      - 143.0\n",
            "      - 173.0\n",
            "      - 179.0\n",
            "      - 33.0\n",
            "      - 36.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10017863143927411\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0765947304044113\n",
            "      mean_inference_ms: 0.9757477743703513\n",
            "      mean_raw_obs_processing_ms: 0.10728476791395791\n",
            "    timesteps_this_iter: 2315\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 122.4731216430664\n",
            "          policy_loss: 166.98248291015625\n",
            "          var_gnorm: 26.781129837036133\n",
            "          vf_explained_var: 0.10281127691268921\n",
            "          vf_loss: 12225.7490234375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 309600\n",
            "    num_agent_steps_trained: 309600\n",
            "    num_env_steps_sampled: 309600\n",
            "    num_env_steps_trained: 309600\n",
            "  iterations_since_restore: 60\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 309600\n",
            "  num_agent_steps_trained: 309600\n",
            "  num_env_steps_sampled: 309600\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 309600\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.70555555555556\n",
            "    ram_util_percent: 21.7\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1383215025429725\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10800658208358194\n",
            "    mean_inference_ms: 1.3110950161890838\n",
            "    mean_raw_obs_processing_ms: 0.24728518069119632\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 108.02\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 108.02\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 10\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 48\n",
            "      - 97\n",
            "      - 121\n",
            "      - 168\n",
            "      - 56\n",
            "      - 131\n",
            "      - 200\n",
            "      - 46\n",
            "      - 200\n",
            "      - 167\n",
            "      - 57\n",
            "      - 56\n",
            "      - 41\n",
            "      - 41\n",
            "      - 134\n",
            "      - 30\n",
            "      - 79\n",
            "      - 109\n",
            "      - 200\n",
            "      - 49\n",
            "      - 144\n",
            "      - 200\n",
            "      - 68\n",
            "      - 162\n",
            "      - 46\n",
            "      - 158\n",
            "      - 193\n",
            "      - 46\n",
            "      - 138\n",
            "      - 101\n",
            "      - 56\n",
            "      - 149\n",
            "      - 91\n",
            "      - 45\n",
            "      - 101\n",
            "      - 130\n",
            "      - 78\n",
            "      - 47\n",
            "      - 68\n",
            "      - 140\n",
            "      - 164\n",
            "      - 77\n",
            "      - 180\n",
            "      - 42\n",
            "      - 149\n",
            "      - 200\n",
            "      - 134\n",
            "      - 80\n",
            "      - 89\n",
            "      - 133\n",
            "      - 110\n",
            "      - 111\n",
            "      - 105\n",
            "      - 36\n",
            "      - 78\n",
            "      - 150\n",
            "      - 146\n",
            "      - 127\n",
            "      - 200\n",
            "      - 200\n",
            "      - 65\n",
            "      - 132\n",
            "      - 113\n",
            "      - 178\n",
            "      - 152\n",
            "      - 38\n",
            "      - 42\n",
            "      - 39\n",
            "      - 30\n",
            "      - 34\n",
            "      - 36\n",
            "      - 105\n",
            "      - 196\n",
            "      - 136\n",
            "      - 83\n",
            "      - 171\n",
            "      - 47\n",
            "      - 63\n",
            "      - 125\n",
            "      - 43\n",
            "      - 200\n",
            "      - 111\n",
            "      - 103\n",
            "      - 51\n",
            "      - 171\n",
            "      - 24\n",
            "      - 157\n",
            "      - 200\n",
            "      - 168\n",
            "      - 132\n",
            "      - 172\n",
            "      - 58\n",
            "      - 82\n",
            "      - 101\n",
            "      - 200\n",
            "      - 68\n",
            "      - 30\n",
            "      - 28\n",
            "      - 119\n",
            "      - 97\n",
            "      episode_reward:\n",
            "      - 48.0\n",
            "      - 97.0\n",
            "      - 121.0\n",
            "      - 168.0\n",
            "      - 56.0\n",
            "      - 131.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 57.0\n",
            "      - 56.0\n",
            "      - 41.0\n",
            "      - 41.0\n",
            "      - 134.0\n",
            "      - 30.0\n",
            "      - 79.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 49.0\n",
            "      - 144.0\n",
            "      - 200.0\n",
            "      - 68.0\n",
            "      - 162.0\n",
            "      - 46.0\n",
            "      - 158.0\n",
            "      - 193.0\n",
            "      - 46.0\n",
            "      - 138.0\n",
            "      - 101.0\n",
            "      - 56.0\n",
            "      - 149.0\n",
            "      - 91.0\n",
            "      - 45.0\n",
            "      - 101.0\n",
            "      - 130.0\n",
            "      - 78.0\n",
            "      - 47.0\n",
            "      - 68.0\n",
            "      - 140.0\n",
            "      - 164.0\n",
            "      - 77.0\n",
            "      - 180.0\n",
            "      - 42.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 134.0\n",
            "      - 80.0\n",
            "      - 89.0\n",
            "      - 133.0\n",
            "      - 110.0\n",
            "      - 111.0\n",
            "      - 105.0\n",
            "      - 36.0\n",
            "      - 78.0\n",
            "      - 150.0\n",
            "      - 146.0\n",
            "      - 127.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 65.0\n",
            "      - 132.0\n",
            "      - 113.0\n",
            "      - 178.0\n",
            "      - 152.0\n",
            "      - 38.0\n",
            "      - 42.0\n",
            "      - 39.0\n",
            "      - 30.0\n",
            "      - 34.0\n",
            "      - 36.0\n",
            "      - 105.0\n",
            "      - 196.0\n",
            "      - 136.0\n",
            "      - 83.0\n",
            "      - 171.0\n",
            "      - 47.0\n",
            "      - 63.0\n",
            "      - 125.0\n",
            "      - 43.0\n",
            "      - 200.0\n",
            "      - 111.0\n",
            "      - 103.0\n",
            "      - 51.0\n",
            "      - 171.0\n",
            "      - 24.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 132.0\n",
            "      - 172.0\n",
            "      - 58.0\n",
            "      - 82.0\n",
            "      - 101.0\n",
            "      - 200.0\n",
            "      - 68.0\n",
            "      - 30.0\n",
            "      - 28.0\n",
            "      - 119.0\n",
            "      - 97.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1383215025429725\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10800658208358194\n",
            "      mean_inference_ms: 1.3110950161890838\n",
            "      mean_raw_obs_processing_ms: 0.24728518069119632\n",
            "  time_since_restore: 648.0974245071411\n",
            "  time_this_iter_s: 12.202273607254028\n",
            "  time_total_s: 648.0974245071411\n",
            "  timers:\n",
            "    learn_throughput: 31021.352\n",
            "    learn_time_ms: 6.447\n",
            "    load_throughput: 1027386.16\n",
            "    load_time_ms: 0.195\n",
            "    training_iteration_time_ms: 212.222\n",
            "    update_time_ms: 3.719\n",
            "  timestamp: 1656954520\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 309600\n",
            "  training_iteration: 60\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:45 (running for 00:11:14.78)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         648.097</td><td style=\"text-align: right;\">309600</td><td style=\"text-align: right;\">  108.02</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            108.02</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:50 (running for 00:11:19.91)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         648.097</td><td style=\"text-align: right;\">309600</td><td style=\"text-align: right;\">  108.02</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            108.02</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 319400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 319400\n",
            "    num_agent_steps_trained: 319400\n",
            "    num_env_steps_sampled: 319400\n",
            "    num_env_steps_trained: 319400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-08-50\n",
            "  done: false\n",
            "  episode_len_mean: 110.23\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 110.23\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 88\n",
            "  episodes_total: 3706\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.80256652832031\n",
            "          policy_loss: 479.828857421875\n",
            "          var_gnorm: 26.89008331298828\n",
            "          vf_explained_var: 0.5856907367706299\n",
            "          vf_loss: 5728.04931640625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 319400\n",
            "    num_agent_steps_trained: 319400\n",
            "    num_env_steps_sampled: 319400\n",
            "    num_env_steps_trained: 319400\n",
            "  iterations_since_restore: 61\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 319400\n",
            "  num_agent_steps_trained: 319400\n",
            "  num_env_steps_sampled: 319400\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 319400\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.28571428571429\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1381779910880644\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1078915424813579\n",
            "    mean_inference_ms: 1.3096562338553286\n",
            "    mean_raw_obs_processing_ms: 0.247038889585492\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 110.23\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 110.23\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 88\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 168\n",
            "      - 132\n",
            "      - 172\n",
            "      - 58\n",
            "      - 82\n",
            "      - 101\n",
            "      - 200\n",
            "      - 68\n",
            "      - 30\n",
            "      - 28\n",
            "      - 119\n",
            "      - 97\n",
            "      - 136\n",
            "      - 77\n",
            "      - 171\n",
            "      - 62\n",
            "      - 44\n",
            "      - 22\n",
            "      - 156\n",
            "      - 44\n",
            "      - 147\n",
            "      - 111\n",
            "      - 200\n",
            "      - 127\n",
            "      - 200\n",
            "      - 128\n",
            "      - 182\n",
            "      - 139\n",
            "      - 31\n",
            "      - 124\n",
            "      - 62\n",
            "      - 101\n",
            "      - 132\n",
            "      - 108\n",
            "      - 190\n",
            "      - 89\n",
            "      - 70\n",
            "      - 52\n",
            "      - 25\n",
            "      - 200\n",
            "      - 186\n",
            "      - 197\n",
            "      - 54\n",
            "      - 170\n",
            "      - 29\n",
            "      - 72\n",
            "      - 24\n",
            "      - 20\n",
            "      - 23\n",
            "      - 200\n",
            "      - 124\n",
            "      - 147\n",
            "      - 130\n",
            "      - 25\n",
            "      - 145\n",
            "      - 179\n",
            "      - 174\n",
            "      - 144\n",
            "      - 143\n",
            "      - 92\n",
            "      - 157\n",
            "      - 16\n",
            "      - 148\n",
            "      - 73\n",
            "      - 126\n",
            "      - 200\n",
            "      - 47\n",
            "      - 143\n",
            "      - 131\n",
            "      - 37\n",
            "      - 42\n",
            "      - 13\n",
            "      - 63\n",
            "      - 187\n",
            "      - 29\n",
            "      - 181\n",
            "      - 163\n",
            "      - 112\n",
            "      - 157\n",
            "      - 148\n",
            "      - 170\n",
            "      - 147\n",
            "      - 200\n",
            "      - 200\n",
            "      - 152\n",
            "      - 99\n",
            "      - 200\n",
            "      - 97\n",
            "      - 17\n",
            "      - 54\n",
            "      - 115\n",
            "      - 116\n",
            "      - 64\n",
            "      - 84\n",
            "      - 41\n",
            "      - 12\n",
            "      - 133\n",
            "      - 38\n",
            "      - 109\n",
            "      - 139\n",
            "      episode_reward:\n",
            "      - 168.0\n",
            "      - 132.0\n",
            "      - 172.0\n",
            "      - 58.0\n",
            "      - 82.0\n",
            "      - 101.0\n",
            "      - 200.0\n",
            "      - 68.0\n",
            "      - 30.0\n",
            "      - 28.0\n",
            "      - 119.0\n",
            "      - 97.0\n",
            "      - 136.0\n",
            "      - 77.0\n",
            "      - 171.0\n",
            "      - 62.0\n",
            "      - 44.0\n",
            "      - 22.0\n",
            "      - 156.0\n",
            "      - 44.0\n",
            "      - 147.0\n",
            "      - 111.0\n",
            "      - 200.0\n",
            "      - 127.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 182.0\n",
            "      - 139.0\n",
            "      - 31.0\n",
            "      - 124.0\n",
            "      - 62.0\n",
            "      - 101.0\n",
            "      - 132.0\n",
            "      - 108.0\n",
            "      - 190.0\n",
            "      - 89.0\n",
            "      - 70.0\n",
            "      - 52.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 197.0\n",
            "      - 54.0\n",
            "      - 170.0\n",
            "      - 29.0\n",
            "      - 72.0\n",
            "      - 24.0\n",
            "      - 20.0\n",
            "      - 23.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 147.0\n",
            "      - 130.0\n",
            "      - 25.0\n",
            "      - 145.0\n",
            "      - 179.0\n",
            "      - 174.0\n",
            "      - 144.0\n",
            "      - 143.0\n",
            "      - 92.0\n",
            "      - 157.0\n",
            "      - 16.0\n",
            "      - 148.0\n",
            "      - 73.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 47.0\n",
            "      - 143.0\n",
            "      - 131.0\n",
            "      - 37.0\n",
            "      - 42.0\n",
            "      - 13.0\n",
            "      - 63.0\n",
            "      - 187.0\n",
            "      - 29.0\n",
            "      - 181.0\n",
            "      - 163.0\n",
            "      - 112.0\n",
            "      - 157.0\n",
            "      - 148.0\n",
            "      - 170.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 97.0\n",
            "      - 17.0\n",
            "      - 54.0\n",
            "      - 115.0\n",
            "      - 116.0\n",
            "      - 64.0\n",
            "      - 84.0\n",
            "      - 41.0\n",
            "      - 12.0\n",
            "      - 133.0\n",
            "      - 38.0\n",
            "      - 109.0\n",
            "      - 139.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1381779910880644\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1078915424813579\n",
            "      mean_inference_ms: 1.3096562338553286\n",
            "      mean_raw_obs_processing_ms: 0.247038889585492\n",
            "  time_since_restore: 658.3476574420929\n",
            "  time_this_iter_s: 10.250232934951782\n",
            "  time_total_s: 658.3476574420929\n",
            "  timers:\n",
            "    learn_throughput: 32542.364\n",
            "    learn_time_ms: 6.146\n",
            "    load_throughput: 1079614.929\n",
            "    load_time_ms: 0.185\n",
            "    training_iteration_time_ms: 213.484\n",
            "    update_time_ms: 3.578\n",
            "  timestamp: 1656954530\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 319400\n",
            "  training_iteration: 61\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:08:55 (running for 00:11:25.07)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         658.348</td><td style=\"text-align: right;\">319400</td><td style=\"text-align: right;\">  110.23</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            110.23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:00 (running for 00:11:30.18)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         658.348</td><td style=\"text-align: right;\">319400</td><td style=\"text-align: right;\">  110.23</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            110.23</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 320200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 320200\n",
            "    num_agent_steps_trained: 320200\n",
            "    num_env_steps_sampled: 320200\n",
            "    num_env_steps_trained: 320200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-09-02\n",
            "  done: false\n",
            "  episode_len_mean: 111.08\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 111.08\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 6\n",
            "  episodes_total: 3712\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 124.95\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 124.95\n",
            "    episode_reward_min: 29.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 133\n",
            "      - 53\n",
            "      - 128\n",
            "      - 130\n",
            "      - 106\n",
            "      - 150\n",
            "      - 108\n",
            "      - 35\n",
            "      - 162\n",
            "      - 200\n",
            "      - 200\n",
            "      - 169\n",
            "      - 176\n",
            "      - 78\n",
            "      - 160\n",
            "      - 29\n",
            "      - 97\n",
            "      - 173\n",
            "      - 30\n",
            "      - 182\n",
            "      episode_reward:\n",
            "      - 133.0\n",
            "      - 53.0\n",
            "      - 128.0\n",
            "      - 130.0\n",
            "      - 106.0\n",
            "      - 150.0\n",
            "      - 108.0\n",
            "      - 35.0\n",
            "      - 162.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 176.0\n",
            "      - 78.0\n",
            "      - 160.0\n",
            "      - 29.0\n",
            "      - 97.0\n",
            "      - 173.0\n",
            "      - 30.0\n",
            "      - 182.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10008031647247059\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07649925263849494\n",
            "      mean_inference_ms: 0.9746587719766275\n",
            "      mean_raw_obs_processing_ms: 0.10712129052880257\n",
            "    timesteps_this_iter: 2499\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 118.59765625\n",
            "          policy_loss: 386.7151184082031\n",
            "          var_gnorm: 26.898906707763672\n",
            "          vf_explained_var: 0.28831952810287476\n",
            "          vf_loss: 11188.23828125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 320200\n",
            "    num_agent_steps_trained: 320200\n",
            "    num_env_steps_sampled: 320200\n",
            "    num_env_steps_trained: 320200\n",
            "  iterations_since_restore: 62\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 320200\n",
            "  num_agent_steps_trained: 320200\n",
            "  num_env_steps_sampled: 320200\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 320200\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.68888888888888\n",
            "    ram_util_percent: 21.7\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13815685136376443\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10789169011622231\n",
            "    mean_inference_ms: 1.3095046089076448\n",
            "    mean_raw_obs_processing_ms: 0.24703890159067765\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 111.08\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 111.08\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 6\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 68\n",
            "      - 30\n",
            "      - 28\n",
            "      - 119\n",
            "      - 97\n",
            "      - 136\n",
            "      - 77\n",
            "      - 171\n",
            "      - 62\n",
            "      - 44\n",
            "      - 22\n",
            "      - 156\n",
            "      - 44\n",
            "      - 147\n",
            "      - 111\n",
            "      - 200\n",
            "      - 127\n",
            "      - 200\n",
            "      - 128\n",
            "      - 182\n",
            "      - 139\n",
            "      - 31\n",
            "      - 124\n",
            "      - 62\n",
            "      - 101\n",
            "      - 132\n",
            "      - 108\n",
            "      - 190\n",
            "      - 89\n",
            "      - 70\n",
            "      - 52\n",
            "      - 25\n",
            "      - 200\n",
            "      - 186\n",
            "      - 197\n",
            "      - 54\n",
            "      - 170\n",
            "      - 29\n",
            "      - 72\n",
            "      - 24\n",
            "      - 20\n",
            "      - 23\n",
            "      - 200\n",
            "      - 124\n",
            "      - 147\n",
            "      - 130\n",
            "      - 25\n",
            "      - 145\n",
            "      - 179\n",
            "      - 174\n",
            "      - 144\n",
            "      - 143\n",
            "      - 92\n",
            "      - 157\n",
            "      - 16\n",
            "      - 148\n",
            "      - 73\n",
            "      - 126\n",
            "      - 200\n",
            "      - 47\n",
            "      - 143\n",
            "      - 131\n",
            "      - 37\n",
            "      - 42\n",
            "      - 13\n",
            "      - 63\n",
            "      - 187\n",
            "      - 29\n",
            "      - 181\n",
            "      - 163\n",
            "      - 112\n",
            "      - 157\n",
            "      - 148\n",
            "      - 170\n",
            "      - 147\n",
            "      - 200\n",
            "      - 200\n",
            "      - 152\n",
            "      - 99\n",
            "      - 200\n",
            "      - 97\n",
            "      - 17\n",
            "      - 54\n",
            "      - 115\n",
            "      - 116\n",
            "      - 64\n",
            "      - 84\n",
            "      - 41\n",
            "      - 12\n",
            "      - 133\n",
            "      - 38\n",
            "      - 109\n",
            "      - 139\n",
            "      - 139\n",
            "      - 168\n",
            "      - 125\n",
            "      - 165\n",
            "      - 167\n",
            "      - 34\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 68.0\n",
            "      - 30.0\n",
            "      - 28.0\n",
            "      - 119.0\n",
            "      - 97.0\n",
            "      - 136.0\n",
            "      - 77.0\n",
            "      - 171.0\n",
            "      - 62.0\n",
            "      - 44.0\n",
            "      - 22.0\n",
            "      - 156.0\n",
            "      - 44.0\n",
            "      - 147.0\n",
            "      - 111.0\n",
            "      - 200.0\n",
            "      - 127.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 182.0\n",
            "      - 139.0\n",
            "      - 31.0\n",
            "      - 124.0\n",
            "      - 62.0\n",
            "      - 101.0\n",
            "      - 132.0\n",
            "      - 108.0\n",
            "      - 190.0\n",
            "      - 89.0\n",
            "      - 70.0\n",
            "      - 52.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 197.0\n",
            "      - 54.0\n",
            "      - 170.0\n",
            "      - 29.0\n",
            "      - 72.0\n",
            "      - 24.0\n",
            "      - 20.0\n",
            "      - 23.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 147.0\n",
            "      - 130.0\n",
            "      - 25.0\n",
            "      - 145.0\n",
            "      - 179.0\n",
            "      - 174.0\n",
            "      - 144.0\n",
            "      - 143.0\n",
            "      - 92.0\n",
            "      - 157.0\n",
            "      - 16.0\n",
            "      - 148.0\n",
            "      - 73.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 47.0\n",
            "      - 143.0\n",
            "      - 131.0\n",
            "      - 37.0\n",
            "      - 42.0\n",
            "      - 13.0\n",
            "      - 63.0\n",
            "      - 187.0\n",
            "      - 29.0\n",
            "      - 181.0\n",
            "      - 163.0\n",
            "      - 112.0\n",
            "      - 157.0\n",
            "      - 148.0\n",
            "      - 170.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 97.0\n",
            "      - 17.0\n",
            "      - 54.0\n",
            "      - 115.0\n",
            "      - 116.0\n",
            "      - 64.0\n",
            "      - 84.0\n",
            "      - 41.0\n",
            "      - 12.0\n",
            "      - 133.0\n",
            "      - 38.0\n",
            "      - 109.0\n",
            "      - 139.0\n",
            "      - 139.0\n",
            "      - 168.0\n",
            "      - 125.0\n",
            "      - 165.0\n",
            "      - 167.0\n",
            "      - 34.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13815685136376443\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10789169011622231\n",
            "      mean_inference_ms: 1.3095046089076448\n",
            "      mean_raw_obs_processing_ms: 0.24703890159067765\n",
            "  time_since_restore: 670.626204252243\n",
            "  time_this_iter_s: 12.278546810150146\n",
            "  time_total_s: 670.626204252243\n",
            "  timers:\n",
            "    learn_throughput: 31315.616\n",
            "    learn_time_ms: 6.387\n",
            "    load_throughput: 1094404.175\n",
            "    load_time_ms: 0.183\n",
            "    training_iteration_time_ms: 216.364\n",
            "    update_time_ms: 3.984\n",
            "  timestamp: 1656954542\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 320200\n",
            "  training_iteration: 62\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:07 (running for 00:11:37.39)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         670.626</td><td style=\"text-align: right;\">320200</td><td style=\"text-align: right;\">  111.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            111.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:13 (running for 00:11:42.50)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         670.626</td><td style=\"text-align: right;\">320200</td><td style=\"text-align: right;\">  111.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            111.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 329800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 329800\n",
            "    num_agent_steps_trained: 329800\n",
            "    num_env_steps_sampled: 329800\n",
            "    num_env_steps_trained: 329800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-09-12\n",
            "  done: false\n",
            "  episode_len_mean: 110.62\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 110.62\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 86\n",
            "  episodes_total: 3798\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 121.15316772460938\n",
            "          policy_loss: 167.5242919921875\n",
            "          var_gnorm: 26.9937686920166\n",
            "          vf_explained_var: 0.37292414903640747\n",
            "          vf_loss: 11246.943359375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 329800\n",
            "    num_agent_steps_trained: 329800\n",
            "    num_env_steps_sampled: 329800\n",
            "    num_env_steps_trained: 329800\n",
            "  iterations_since_restore: 63\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 329800\n",
            "  num_agent_steps_trained: 329800\n",
            "  num_env_steps_sampled: 329800\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 329800\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.85714285714288\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13800606555392161\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10788521934499165\n",
            "    mean_inference_ms: 1.308118280917852\n",
            "    mean_raw_obs_processing_ms: 0.24685789212243592\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 110.62\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 110.62\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 86\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 64\n",
            "      - 84\n",
            "      - 41\n",
            "      - 12\n",
            "      - 133\n",
            "      - 38\n",
            "      - 109\n",
            "      - 139\n",
            "      - 139\n",
            "      - 168\n",
            "      - 125\n",
            "      - 165\n",
            "      - 167\n",
            "      - 34\n",
            "      - 179\n",
            "      - 99\n",
            "      - 192\n",
            "      - 190\n",
            "      - 79\n",
            "      - 12\n",
            "      - 151\n",
            "      - 110\n",
            "      - 28\n",
            "      - 167\n",
            "      - 37\n",
            "      - 127\n",
            "      - 95\n",
            "      - 120\n",
            "      - 200\n",
            "      - 168\n",
            "      - 148\n",
            "      - 26\n",
            "      - 147\n",
            "      - 200\n",
            "      - 175\n",
            "      - 130\n",
            "      - 156\n",
            "      - 174\n",
            "      - 200\n",
            "      - 36\n",
            "      - 43\n",
            "      - 92\n",
            "      - 94\n",
            "      - 133\n",
            "      - 197\n",
            "      - 25\n",
            "      - 92\n",
            "      - 200\n",
            "      - 25\n",
            "      - 163\n",
            "      - 31\n",
            "      - 53\n",
            "      - 199\n",
            "      - 59\n",
            "      - 84\n",
            "      - 126\n",
            "      - 40\n",
            "      - 116\n",
            "      - 81\n",
            "      - 57\n",
            "      - 95\n",
            "      - 162\n",
            "      - 76\n",
            "      - 69\n",
            "      - 68\n",
            "      - 19\n",
            "      - 200\n",
            "      - 172\n",
            "      - 198\n",
            "      - 67\n",
            "      - 138\n",
            "      - 191\n",
            "      - 46\n",
            "      - 176\n",
            "      - 170\n",
            "      - 70\n",
            "      - 167\n",
            "      - 123\n",
            "      - 138\n",
            "      - 23\n",
            "      - 99\n",
            "      - 200\n",
            "      - 130\n",
            "      - 108\n",
            "      - 20\n",
            "      - 67\n",
            "      - 43\n",
            "      - 85\n",
            "      - 101\n",
            "      - 72\n",
            "      - 123\n",
            "      - 77\n",
            "      - 43\n",
            "      - 83\n",
            "      - 165\n",
            "      - 200\n",
            "      - 200\n",
            "      - 50\n",
            "      - 119\n",
            "      - 35\n",
            "      episode_reward:\n",
            "      - 64.0\n",
            "      - 84.0\n",
            "      - 41.0\n",
            "      - 12.0\n",
            "      - 133.0\n",
            "      - 38.0\n",
            "      - 109.0\n",
            "      - 139.0\n",
            "      - 139.0\n",
            "      - 168.0\n",
            "      - 125.0\n",
            "      - 165.0\n",
            "      - 167.0\n",
            "      - 34.0\n",
            "      - 179.0\n",
            "      - 99.0\n",
            "      - 192.0\n",
            "      - 190.0\n",
            "      - 79.0\n",
            "      - 12.0\n",
            "      - 151.0\n",
            "      - 110.0\n",
            "      - 28.0\n",
            "      - 167.0\n",
            "      - 37.0\n",
            "      - 127.0\n",
            "      - 95.0\n",
            "      - 120.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 148.0\n",
            "      - 26.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 130.0\n",
            "      - 156.0\n",
            "      - 174.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 43.0\n",
            "      - 92.0\n",
            "      - 94.0\n",
            "      - 133.0\n",
            "      - 197.0\n",
            "      - 25.0\n",
            "      - 92.0\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "      - 163.0\n",
            "      - 31.0\n",
            "      - 53.0\n",
            "      - 199.0\n",
            "      - 59.0\n",
            "      - 84.0\n",
            "      - 126.0\n",
            "      - 40.0\n",
            "      - 116.0\n",
            "      - 81.0\n",
            "      - 57.0\n",
            "      - 95.0\n",
            "      - 162.0\n",
            "      - 76.0\n",
            "      - 69.0\n",
            "      - 68.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 198.0\n",
            "      - 67.0\n",
            "      - 138.0\n",
            "      - 191.0\n",
            "      - 46.0\n",
            "      - 176.0\n",
            "      - 170.0\n",
            "      - 70.0\n",
            "      - 167.0\n",
            "      - 123.0\n",
            "      - 138.0\n",
            "      - 23.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 130.0\n",
            "      - 108.0\n",
            "      - 20.0\n",
            "      - 67.0\n",
            "      - 43.0\n",
            "      - 85.0\n",
            "      - 101.0\n",
            "      - 72.0\n",
            "      - 123.0\n",
            "      - 77.0\n",
            "      - 43.0\n",
            "      - 83.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 119.0\n",
            "      - 35.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13800606555392161\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10788521934499165\n",
            "      mean_inference_ms: 1.308118280917852\n",
            "      mean_raw_obs_processing_ms: 0.24685789212243592\n",
            "  time_since_restore: 680.714512348175\n",
            "  time_this_iter_s: 10.088308095932007\n",
            "  time_total_s: 680.714512348175\n",
            "  timers:\n",
            "    learn_throughput: 31481.796\n",
            "    learn_time_ms: 6.353\n",
            "    load_throughput: 1048444.944\n",
            "    load_time_ms: 0.191\n",
            "    training_iteration_time_ms: 210.841\n",
            "    update_time_ms: 3.482\n",
            "  timestamp: 1656954552\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 329800\n",
            "  training_iteration: 63\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:18 (running for 00:11:47.54)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         680.715</td><td style=\"text-align: right;\">329800</td><td style=\"text-align: right;\">  110.62</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            110.62</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:23 (running for 00:11:52.63)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         680.715</td><td style=\"text-align: right;\">329800</td><td style=\"text-align: right;\">  110.62</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            110.62</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 330600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 330600\n",
            "    num_agent_steps_trained: 330600\n",
            "    num_env_steps_sampled: 330600\n",
            "    num_env_steps_trained: 330600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-09-26\n",
            "  done: false\n",
            "  episode_len_mean: 112.36\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 112.36\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 8\n",
            "  episodes_total: 3806\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 153.25\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 153.25\n",
            "    episode_reward_min: 46.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 160\n",
            "      - 171\n",
            "      - 141\n",
            "      - 177\n",
            "      - 59\n",
            "      - 170\n",
            "      - 136\n",
            "      - 194\n",
            "      - 168\n",
            "      - 200\n",
            "      - 200\n",
            "      - 185\n",
            "      - 150\n",
            "      - 121\n",
            "      - 46\n",
            "      - 120\n",
            "      - 200\n",
            "      - 149\n",
            "      - 122\n",
            "      - 196\n",
            "      episode_reward:\n",
            "      - 160.0\n",
            "      - 171.0\n",
            "      - 141.0\n",
            "      - 177.0\n",
            "      - 59.0\n",
            "      - 170.0\n",
            "      - 136.0\n",
            "      - 194.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 185.0\n",
            "      - 150.0\n",
            "      - 121.0\n",
            "      - 46.0\n",
            "      - 120.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 122.0\n",
            "      - 196.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10008381586527482\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07648196361887535\n",
            "      mean_inference_ms: 0.9741985320139044\n",
            "      mean_raw_obs_processing_ms: 0.10705743348754493\n",
            "    timesteps_this_iter: 3065\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 120.74469757080078\n",
            "          policy_loss: 272.5669860839844\n",
            "          var_gnorm: 26.999692916870117\n",
            "          vf_explained_var: 0.04051262140274048\n",
            "          vf_loss: 12162.42578125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 330600\n",
            "    num_agent_steps_trained: 330600\n",
            "    num_env_steps_sampled: 330600\n",
            "    num_env_steps_trained: 330600\n",
            "  iterations_since_restore: 64\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 330600\n",
            "  num_agent_steps_trained: 330600\n",
            "  num_env_steps_sampled: 330600\n",
            "  num_env_steps_sampled_this_iter: 800\n",
            "  num_env_steps_trained: 330600\n",
            "  num_env_steps_trained_this_iter: 800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.94736842105262\n",
            "    ram_util_percent: 21.7\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13803753802310487\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10785001523355615\n",
            "    mean_inference_ms: 1.3082637912699318\n",
            "    mean_raw_obs_processing_ms: 0.24677482189143646\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 112.36\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 112.36\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 8\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 139\n",
            "      - 168\n",
            "      - 125\n",
            "      - 165\n",
            "      - 167\n",
            "      - 34\n",
            "      - 179\n",
            "      - 99\n",
            "      - 192\n",
            "      - 190\n",
            "      - 79\n",
            "      - 12\n",
            "      - 151\n",
            "      - 110\n",
            "      - 28\n",
            "      - 167\n",
            "      - 37\n",
            "      - 127\n",
            "      - 95\n",
            "      - 120\n",
            "      - 200\n",
            "      - 168\n",
            "      - 148\n",
            "      - 26\n",
            "      - 147\n",
            "      - 200\n",
            "      - 175\n",
            "      - 130\n",
            "      - 156\n",
            "      - 174\n",
            "      - 200\n",
            "      - 36\n",
            "      - 43\n",
            "      - 92\n",
            "      - 94\n",
            "      - 133\n",
            "      - 197\n",
            "      - 25\n",
            "      - 92\n",
            "      - 200\n",
            "      - 25\n",
            "      - 163\n",
            "      - 31\n",
            "      - 53\n",
            "      - 199\n",
            "      - 59\n",
            "      - 84\n",
            "      - 126\n",
            "      - 40\n",
            "      - 116\n",
            "      - 81\n",
            "      - 57\n",
            "      - 95\n",
            "      - 162\n",
            "      - 76\n",
            "      - 69\n",
            "      - 68\n",
            "      - 19\n",
            "      - 200\n",
            "      - 172\n",
            "      - 198\n",
            "      - 67\n",
            "      - 138\n",
            "      - 191\n",
            "      - 46\n",
            "      - 176\n",
            "      - 170\n",
            "      - 70\n",
            "      - 167\n",
            "      - 123\n",
            "      - 138\n",
            "      - 23\n",
            "      - 99\n",
            "      - 200\n",
            "      - 130\n",
            "      - 108\n",
            "      - 20\n",
            "      - 67\n",
            "      - 43\n",
            "      - 85\n",
            "      - 101\n",
            "      - 72\n",
            "      - 123\n",
            "      - 77\n",
            "      - 43\n",
            "      - 83\n",
            "      - 165\n",
            "      - 200\n",
            "      - 200\n",
            "      - 50\n",
            "      - 119\n",
            "      - 35\n",
            "      - 102\n",
            "      - 39\n",
            "      - 144\n",
            "      - 74\n",
            "      - 141\n",
            "      - 141\n",
            "      - 132\n",
            "      - 21\n",
            "      episode_reward:\n",
            "      - 139.0\n",
            "      - 168.0\n",
            "      - 125.0\n",
            "      - 165.0\n",
            "      - 167.0\n",
            "      - 34.0\n",
            "      - 179.0\n",
            "      - 99.0\n",
            "      - 192.0\n",
            "      - 190.0\n",
            "      - 79.0\n",
            "      - 12.0\n",
            "      - 151.0\n",
            "      - 110.0\n",
            "      - 28.0\n",
            "      - 167.0\n",
            "      - 37.0\n",
            "      - 127.0\n",
            "      - 95.0\n",
            "      - 120.0\n",
            "      - 200.0\n",
            "      - 168.0\n",
            "      - 148.0\n",
            "      - 26.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 130.0\n",
            "      - 156.0\n",
            "      - 174.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 43.0\n",
            "      - 92.0\n",
            "      - 94.0\n",
            "      - 133.0\n",
            "      - 197.0\n",
            "      - 25.0\n",
            "      - 92.0\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "      - 163.0\n",
            "      - 31.0\n",
            "      - 53.0\n",
            "      - 199.0\n",
            "      - 59.0\n",
            "      - 84.0\n",
            "      - 126.0\n",
            "      - 40.0\n",
            "      - 116.0\n",
            "      - 81.0\n",
            "      - 57.0\n",
            "      - 95.0\n",
            "      - 162.0\n",
            "      - 76.0\n",
            "      - 69.0\n",
            "      - 68.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 198.0\n",
            "      - 67.0\n",
            "      - 138.0\n",
            "      - 191.0\n",
            "      - 46.0\n",
            "      - 176.0\n",
            "      - 170.0\n",
            "      - 70.0\n",
            "      - 167.0\n",
            "      - 123.0\n",
            "      - 138.0\n",
            "      - 23.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 130.0\n",
            "      - 108.0\n",
            "      - 20.0\n",
            "      - 67.0\n",
            "      - 43.0\n",
            "      - 85.0\n",
            "      - 101.0\n",
            "      - 72.0\n",
            "      - 123.0\n",
            "      - 77.0\n",
            "      - 43.0\n",
            "      - 83.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 119.0\n",
            "      - 35.0\n",
            "      - 102.0\n",
            "      - 39.0\n",
            "      - 144.0\n",
            "      - 74.0\n",
            "      - 141.0\n",
            "      - 141.0\n",
            "      - 132.0\n",
            "      - 21.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13803753802310487\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10785001523355615\n",
            "      mean_inference_ms: 1.3082637912699318\n",
            "      mean_raw_obs_processing_ms: 0.24677482189143646\n",
            "  time_since_restore: 694.1059494018555\n",
            "  time_this_iter_s: 13.39143705368042\n",
            "  time_total_s: 694.1059494018555\n",
            "  timers:\n",
            "    learn_throughput: 33510.332\n",
            "    learn_time_ms: 5.968\n",
            "    load_throughput: 1069157.278\n",
            "    load_time_ms: 0.187\n",
            "    training_iteration_time_ms: 211.136\n",
            "    update_time_ms: 3.427\n",
            "  timestamp: 1656954566\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 330600\n",
            "  training_iteration: 64\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:31 (running for 00:12:00.97)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         694.106</td><td style=\"text-align: right;\">330600</td><td style=\"text-align: right;\">  112.36</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            112.36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:36 (running for 00:12:06.12)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         694.106</td><td style=\"text-align: right;\">330600</td><td style=\"text-align: right;\">  112.36</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            112.36</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 339400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 339400\n",
            "    num_agent_steps_trained: 339400\n",
            "    num_env_steps_sampled: 339400\n",
            "    num_env_steps_trained: 339400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-09-36\n",
            "  done: false\n",
            "  episode_len_mean: 113.07\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 113.07\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 74\n",
            "  episodes_total: 3880\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 123.87812805175781\n",
            "          policy_loss: 208.45567321777344\n",
            "          var_gnorm: 27.092275619506836\n",
            "          vf_explained_var: 0.22208130359649658\n",
            "          vf_loss: 10705.375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 339400\n",
            "    num_agent_steps_trained: 339400\n",
            "    num_env_steps_sampled: 339400\n",
            "    num_env_steps_trained: 339400\n",
            "  iterations_since_restore: 65\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 339400\n",
            "  num_agent_steps_trained: 339400\n",
            "  num_env_steps_sampled: 339400\n",
            "  num_env_steps_sampled_this_iter: 8800\n",
            "  num_env_steps_trained: 339400\n",
            "  num_env_steps_trained_this_iter: 8800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.82666666666667\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1380527981724995\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10800561864190646\n",
            "    mean_inference_ms: 1.30925042382839\n",
            "    mean_raw_obs_processing_ms: 0.24698312322895216\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 113.07\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 113.07\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 74\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 130\n",
            "      - 108\n",
            "      - 20\n",
            "      - 67\n",
            "      - 43\n",
            "      - 85\n",
            "      - 101\n",
            "      - 72\n",
            "      - 123\n",
            "      - 77\n",
            "      - 43\n",
            "      - 83\n",
            "      - 165\n",
            "      - 200\n",
            "      - 200\n",
            "      - 50\n",
            "      - 119\n",
            "      - 35\n",
            "      - 102\n",
            "      - 39\n",
            "      - 144\n",
            "      - 74\n",
            "      - 141\n",
            "      - 141\n",
            "      - 132\n",
            "      - 21\n",
            "      - 145\n",
            "      - 189\n",
            "      - 176\n",
            "      - 187\n",
            "      - 78\n",
            "      - 45\n",
            "      - 68\n",
            "      - 200\n",
            "      - 43\n",
            "      - 175\n",
            "      - 165\n",
            "      - 200\n",
            "      - 145\n",
            "      - 182\n",
            "      - 80\n",
            "      - 200\n",
            "      - 47\n",
            "      - 155\n",
            "      - 127\n",
            "      - 62\n",
            "      - 188\n",
            "      - 97\n",
            "      - 119\n",
            "      - 21\n",
            "      - 116\n",
            "      - 145\n",
            "      - 32\n",
            "      - 160\n",
            "      - 105\n",
            "      - 128\n",
            "      - 145\n",
            "      - 53\n",
            "      - 122\n",
            "      - 127\n",
            "      - 157\n",
            "      - 194\n",
            "      - 146\n",
            "      - 185\n",
            "      - 194\n",
            "      - 55\n",
            "      - 61\n",
            "      - 118\n",
            "      - 153\n",
            "      - 165\n",
            "      - 125\n",
            "      - 200\n",
            "      - 183\n",
            "      - 35\n",
            "      - 92\n",
            "      - 22\n",
            "      - 48\n",
            "      - 159\n",
            "      - 163\n",
            "      - 199\n",
            "      - 31\n",
            "      - 71\n",
            "      - 160\n",
            "      - 62\n",
            "      - 91\n",
            "      - 46\n",
            "      - 184\n",
            "      - 200\n",
            "      - 55\n",
            "      - 65\n",
            "      - 200\n",
            "      - 122\n",
            "      - 16\n",
            "      - 62\n",
            "      - 31\n",
            "      - 135\n",
            "      - 111\n",
            "      - 28\n",
            "      - 117\n",
            "      - 124\n",
            "      episode_reward:\n",
            "      - 130.0\n",
            "      - 108.0\n",
            "      - 20.0\n",
            "      - 67.0\n",
            "      - 43.0\n",
            "      - 85.0\n",
            "      - 101.0\n",
            "      - 72.0\n",
            "      - 123.0\n",
            "      - 77.0\n",
            "      - 43.0\n",
            "      - 83.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 119.0\n",
            "      - 35.0\n",
            "      - 102.0\n",
            "      - 39.0\n",
            "      - 144.0\n",
            "      - 74.0\n",
            "      - 141.0\n",
            "      - 141.0\n",
            "      - 132.0\n",
            "      - 21.0\n",
            "      - 145.0\n",
            "      - 189.0\n",
            "      - 176.0\n",
            "      - 187.0\n",
            "      - 78.0\n",
            "      - 45.0\n",
            "      - 68.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 175.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 145.0\n",
            "      - 182.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 47.0\n",
            "      - 155.0\n",
            "      - 127.0\n",
            "      - 62.0\n",
            "      - 188.0\n",
            "      - 97.0\n",
            "      - 119.0\n",
            "      - 21.0\n",
            "      - 116.0\n",
            "      - 145.0\n",
            "      - 32.0\n",
            "      - 160.0\n",
            "      - 105.0\n",
            "      - 128.0\n",
            "      - 145.0\n",
            "      - 53.0\n",
            "      - 122.0\n",
            "      - 127.0\n",
            "      - 157.0\n",
            "      - 194.0\n",
            "      - 146.0\n",
            "      - 185.0\n",
            "      - 194.0\n",
            "      - 55.0\n",
            "      - 61.0\n",
            "      - 118.0\n",
            "      - 153.0\n",
            "      - 165.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 183.0\n",
            "      - 35.0\n",
            "      - 92.0\n",
            "      - 22.0\n",
            "      - 48.0\n",
            "      - 159.0\n",
            "      - 163.0\n",
            "      - 199.0\n",
            "      - 31.0\n",
            "      - 71.0\n",
            "      - 160.0\n",
            "      - 62.0\n",
            "      - 91.0\n",
            "      - 46.0\n",
            "      - 184.0\n",
            "      - 200.0\n",
            "      - 55.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 16.0\n",
            "      - 62.0\n",
            "      - 31.0\n",
            "      - 135.0\n",
            "      - 111.0\n",
            "      - 28.0\n",
            "      - 117.0\n",
            "      - 124.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1380527981724995\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10800561864190646\n",
            "      mean_inference_ms: 1.30925042382839\n",
            "      mean_raw_obs_processing_ms: 0.24698312322895216\n",
            "  time_since_restore: 704.1939239501953\n",
            "  time_this_iter_s: 10.087974548339844\n",
            "  time_total_s: 704.1939239501953\n",
            "  timers:\n",
            "    learn_throughput: 22697.064\n",
            "    learn_time_ms: 8.812\n",
            "    load_throughput: 588591.636\n",
            "    load_time_ms: 0.34\n",
            "    training_iteration_time_ms: 277.427\n",
            "    update_time_ms: 7.308\n",
            "  timestamp: 1656954576\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 339400\n",
            "  training_iteration: 65\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:41 (running for 00:12:11.22)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         704.194</td><td style=\"text-align: right;\">339400</td><td style=\"text-align: right;\">  113.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            113.07</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:46 (running for 00:12:16.31)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         704.194</td><td style=\"text-align: right;\">339400</td><td style=\"text-align: right;\">  113.07</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            113.07</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 340000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 340000\n",
            "    num_agent_steps_trained: 340000\n",
            "    num_env_steps_sampled: 340000\n",
            "    num_env_steps_trained: 340000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-09-47\n",
            "  done: false\n",
            "  episode_len_mean: 115.02\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 115.02\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 6\n",
            "  episodes_total: 3886\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 122.8\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 122.8\n",
            "    episode_reward_min: 11.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 160\n",
            "      - 39\n",
            "      - 34\n",
            "      - 170\n",
            "      - 139\n",
            "      - 200\n",
            "      - 200\n",
            "      - 26\n",
            "      - 165\n",
            "      - 148\n",
            "      - 143\n",
            "      - 122\n",
            "      - 55\n",
            "      - 55\n",
            "      - 11\n",
            "      - 170\n",
            "      - 151\n",
            "      - 117\n",
            "      - 200\n",
            "      - 151\n",
            "      episode_reward:\n",
            "      - 160.0\n",
            "      - 39.0\n",
            "      - 34.0\n",
            "      - 170.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 26.0\n",
            "      - 165.0\n",
            "      - 148.0\n",
            "      - 143.0\n",
            "      - 122.0\n",
            "      - 55.0\n",
            "      - 55.0\n",
            "      - 11.0\n",
            "      - 170.0\n",
            "      - 151.0\n",
            "      - 117.0\n",
            "      - 200.0\n",
            "      - 151.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10015606531580608\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0765972503537034\n",
            "      mean_inference_ms: 0.9755658380634668\n",
            "      mean_raw_obs_processing_ms: 0.1071507031086137\n",
            "    timesteps_this_iter: 2456\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 114.4752426147461\n",
            "          policy_loss: -11.387313842773438\n",
            "          var_gnorm: 27.09874153137207\n",
            "          vf_explained_var: 0.2255169153213501\n",
            "          vf_loss: 13356.5703125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 340000\n",
            "    num_agent_steps_trained: 340000\n",
            "    num_env_steps_sampled: 340000\n",
            "    num_env_steps_trained: 340000\n",
            "  iterations_since_restore: 66\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 340000\n",
            "  num_agent_steps_trained: 340000\n",
            "  num_env_steps_sampled: 340000\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 340000\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 72.2\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13810646116939748\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1079810257815081\n",
            "    mean_inference_ms: 1.3096888375948337\n",
            "    mean_raw_obs_processing_ms: 0.24692444691976398\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 115.02\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 115.02\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 6\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 101\n",
            "      - 72\n",
            "      - 123\n",
            "      - 77\n",
            "      - 43\n",
            "      - 83\n",
            "      - 165\n",
            "      - 200\n",
            "      - 200\n",
            "      - 50\n",
            "      - 119\n",
            "      - 35\n",
            "      - 102\n",
            "      - 39\n",
            "      - 144\n",
            "      - 74\n",
            "      - 141\n",
            "      - 141\n",
            "      - 132\n",
            "      - 21\n",
            "      - 145\n",
            "      - 189\n",
            "      - 176\n",
            "      - 187\n",
            "      - 78\n",
            "      - 45\n",
            "      - 68\n",
            "      - 200\n",
            "      - 43\n",
            "      - 175\n",
            "      - 165\n",
            "      - 200\n",
            "      - 145\n",
            "      - 182\n",
            "      - 80\n",
            "      - 200\n",
            "      - 47\n",
            "      - 155\n",
            "      - 127\n",
            "      - 62\n",
            "      - 188\n",
            "      - 97\n",
            "      - 119\n",
            "      - 21\n",
            "      - 116\n",
            "      - 145\n",
            "      - 32\n",
            "      - 160\n",
            "      - 105\n",
            "      - 128\n",
            "      - 145\n",
            "      - 53\n",
            "      - 122\n",
            "      - 127\n",
            "      - 157\n",
            "      - 194\n",
            "      - 146\n",
            "      - 185\n",
            "      - 194\n",
            "      - 55\n",
            "      - 61\n",
            "      - 118\n",
            "      - 153\n",
            "      - 165\n",
            "      - 125\n",
            "      - 200\n",
            "      - 183\n",
            "      - 35\n",
            "      - 92\n",
            "      - 22\n",
            "      - 48\n",
            "      - 159\n",
            "      - 163\n",
            "      - 199\n",
            "      - 31\n",
            "      - 71\n",
            "      - 160\n",
            "      - 62\n",
            "      - 91\n",
            "      - 46\n",
            "      - 184\n",
            "      - 200\n",
            "      - 55\n",
            "      - 65\n",
            "      - 200\n",
            "      - 122\n",
            "      - 16\n",
            "      - 62\n",
            "      - 31\n",
            "      - 135\n",
            "      - 111\n",
            "      - 28\n",
            "      - 117\n",
            "      - 124\n",
            "      - 141\n",
            "      - 179\n",
            "      - 29\n",
            "      - 31\n",
            "      - 107\n",
            "      - 161\n",
            "      episode_reward:\n",
            "      - 101.0\n",
            "      - 72.0\n",
            "      - 123.0\n",
            "      - 77.0\n",
            "      - 43.0\n",
            "      - 83.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 119.0\n",
            "      - 35.0\n",
            "      - 102.0\n",
            "      - 39.0\n",
            "      - 144.0\n",
            "      - 74.0\n",
            "      - 141.0\n",
            "      - 141.0\n",
            "      - 132.0\n",
            "      - 21.0\n",
            "      - 145.0\n",
            "      - 189.0\n",
            "      - 176.0\n",
            "      - 187.0\n",
            "      - 78.0\n",
            "      - 45.0\n",
            "      - 68.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 175.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 145.0\n",
            "      - 182.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 47.0\n",
            "      - 155.0\n",
            "      - 127.0\n",
            "      - 62.0\n",
            "      - 188.0\n",
            "      - 97.0\n",
            "      - 119.0\n",
            "      - 21.0\n",
            "      - 116.0\n",
            "      - 145.0\n",
            "      - 32.0\n",
            "      - 160.0\n",
            "      - 105.0\n",
            "      - 128.0\n",
            "      - 145.0\n",
            "      - 53.0\n",
            "      - 122.0\n",
            "      - 127.0\n",
            "      - 157.0\n",
            "      - 194.0\n",
            "      - 146.0\n",
            "      - 185.0\n",
            "      - 194.0\n",
            "      - 55.0\n",
            "      - 61.0\n",
            "      - 118.0\n",
            "      - 153.0\n",
            "      - 165.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 183.0\n",
            "      - 35.0\n",
            "      - 92.0\n",
            "      - 22.0\n",
            "      - 48.0\n",
            "      - 159.0\n",
            "      - 163.0\n",
            "      - 199.0\n",
            "      - 31.0\n",
            "      - 71.0\n",
            "      - 160.0\n",
            "      - 62.0\n",
            "      - 91.0\n",
            "      - 46.0\n",
            "      - 184.0\n",
            "      - 200.0\n",
            "      - 55.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 16.0\n",
            "      - 62.0\n",
            "      - 31.0\n",
            "      - 135.0\n",
            "      - 111.0\n",
            "      - 28.0\n",
            "      - 117.0\n",
            "      - 124.0\n",
            "      - 141.0\n",
            "      - 179.0\n",
            "      - 29.0\n",
            "      - 31.0\n",
            "      - 107.0\n",
            "      - 161.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13810646116939748\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1079810257815081\n",
            "      mean_inference_ms: 1.3096888375948337\n",
            "      mean_raw_obs_processing_ms: 0.24692444691976398\n",
            "  time_since_restore: 714.6953735351562\n",
            "  time_this_iter_s: 10.501449584960938\n",
            "  time_total_s: 714.6953735351562\n",
            "  timers:\n",
            "    learn_throughput: 22843.424\n",
            "    learn_time_ms: 8.755\n",
            "    load_throughput: 556569.002\n",
            "    load_time_ms: 0.359\n",
            "    training_iteration_time_ms: 305.227\n",
            "    update_time_ms: 8.003\n",
            "  timestamp: 1656954587\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 340000\n",
            "  training_iteration: 66\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:52 (running for 00:12:21.73)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         714.695</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  115.02</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            115.02</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:09:57 (running for 00:12:26.81)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         714.695</td><td style=\"text-align: right;\">340000</td><td style=\"text-align: right;\">  115.02</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            115.02</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 349200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 349200\n",
            "    num_agent_steps_trained: 349200\n",
            "    num_env_steps_sampled: 349200\n",
            "    num_env_steps_trained: 349200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-09-57\n",
            "  done: false\n",
            "  episode_len_mean: 114.82\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 114.82\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 77\n",
            "  episodes_total: 3963\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.48328399658203\n",
            "          policy_loss: 41.47021484375\n",
            "          var_gnorm: 27.181978225708008\n",
            "          vf_explained_var: 0.3223479390144348\n",
            "          vf_loss: 12307.228515625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 349200\n",
            "    num_agent_steps_trained: 349200\n",
            "    num_env_steps_sampled: 349200\n",
            "    num_env_steps_trained: 349200\n",
            "  iterations_since_restore: 67\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 349200\n",
            "  num_agent_steps_trained: 349200\n",
            "  num_env_steps_sampled: 349200\n",
            "  num_env_steps_sampled_this_iter: 9200\n",
            "  num_env_steps_trained: 349200\n",
            "  num_env_steps_trained_this_iter: 9200\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.30714285714285\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.138237953856323\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10812544758979055\n",
            "    mean_inference_ms: 1.3111495815874392\n",
            "    mean_raw_obs_processing_ms: 0.24706537035878762\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 114.82\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 114.82\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 77\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 62\n",
            "      - 91\n",
            "      - 46\n",
            "      - 184\n",
            "      - 200\n",
            "      - 55\n",
            "      - 65\n",
            "      - 200\n",
            "      - 122\n",
            "      - 16\n",
            "      - 62\n",
            "      - 31\n",
            "      - 135\n",
            "      - 111\n",
            "      - 28\n",
            "      - 117\n",
            "      - 124\n",
            "      - 141\n",
            "      - 179\n",
            "      - 29\n",
            "      - 31\n",
            "      - 107\n",
            "      - 161\n",
            "      - 45\n",
            "      - 88\n",
            "      - 120\n",
            "      - 200\n",
            "      - 61\n",
            "      - 126\n",
            "      - 60\n",
            "      - 55\n",
            "      - 150\n",
            "      - 174\n",
            "      - 71\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 133\n",
            "      - 160\n",
            "      - 108\n",
            "      - 142\n",
            "      - 67\n",
            "      - 192\n",
            "      - 125\n",
            "      - 141\n",
            "      - 100\n",
            "      - 34\n",
            "      - 103\n",
            "      - 114\n",
            "      - 78\n",
            "      - 193\n",
            "      - 32\n",
            "      - 36\n",
            "      - 34\n",
            "      - 73\n",
            "      - 134\n",
            "      - 200\n",
            "      - 116\n",
            "      - 132\n",
            "      - 66\n",
            "      - 37\n",
            "      - 143\n",
            "      - 130\n",
            "      - 103\n",
            "      - 148\n",
            "      - 143\n",
            "      - 197\n",
            "      - 128\n",
            "      - 200\n",
            "      - 147\n",
            "      - 66\n",
            "      - 126\n",
            "      - 200\n",
            "      - 33\n",
            "      - 163\n",
            "      - 46\n",
            "      - 133\n",
            "      - 184\n",
            "      - 154\n",
            "      - 200\n",
            "      - 53\n",
            "      - 21\n",
            "      - 164\n",
            "      - 132\n",
            "      - 17\n",
            "      - 200\n",
            "      - 31\n",
            "      - 200\n",
            "      - 200\n",
            "      - 16\n",
            "      - 44\n",
            "      - 147\n",
            "      - 200\n",
            "      - 147\n",
            "      - 62\n",
            "      - 22\n",
            "      - 143\n",
            "      - 187\n",
            "      - 150\n",
            "      - 105\n",
            "      episode_reward:\n",
            "      - 62.0\n",
            "      - 91.0\n",
            "      - 46.0\n",
            "      - 184.0\n",
            "      - 200.0\n",
            "      - 55.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 16.0\n",
            "      - 62.0\n",
            "      - 31.0\n",
            "      - 135.0\n",
            "      - 111.0\n",
            "      - 28.0\n",
            "      - 117.0\n",
            "      - 124.0\n",
            "      - 141.0\n",
            "      - 179.0\n",
            "      - 29.0\n",
            "      - 31.0\n",
            "      - 107.0\n",
            "      - 161.0\n",
            "      - 45.0\n",
            "      - 88.0\n",
            "      - 120.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 126.0\n",
            "      - 60.0\n",
            "      - 55.0\n",
            "      - 150.0\n",
            "      - 174.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 133.0\n",
            "      - 160.0\n",
            "      - 108.0\n",
            "      - 142.0\n",
            "      - 67.0\n",
            "      - 192.0\n",
            "      - 125.0\n",
            "      - 141.0\n",
            "      - 100.0\n",
            "      - 34.0\n",
            "      - 103.0\n",
            "      - 114.0\n",
            "      - 78.0\n",
            "      - 193.0\n",
            "      - 32.0\n",
            "      - 36.0\n",
            "      - 34.0\n",
            "      - 73.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 132.0\n",
            "      - 66.0\n",
            "      - 37.0\n",
            "      - 143.0\n",
            "      - 130.0\n",
            "      - 103.0\n",
            "      - 148.0\n",
            "      - 143.0\n",
            "      - 197.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 66.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 163.0\n",
            "      - 46.0\n",
            "      - 133.0\n",
            "      - 184.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 21.0\n",
            "      - 164.0\n",
            "      - 132.0\n",
            "      - 17.0\n",
            "      - 200.0\n",
            "      - 31.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 16.0\n",
            "      - 44.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 62.0\n",
            "      - 22.0\n",
            "      - 143.0\n",
            "      - 187.0\n",
            "      - 150.0\n",
            "      - 105.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.138237953856323\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10812544758979055\n",
            "      mean_inference_ms: 1.3111495815874392\n",
            "      mean_raw_obs_processing_ms: 0.24706537035878762\n",
            "  time_since_restore: 724.742107629776\n",
            "  time_this_iter_s: 10.046734094619751\n",
            "  time_total_s: 724.742107629776\n",
            "  timers:\n",
            "    learn_throughput: 31883.967\n",
            "    learn_time_ms: 6.273\n",
            "    load_throughput: 1003182.014\n",
            "    load_time_ms: 0.199\n",
            "    training_iteration_time_ms: 211.707\n",
            "    update_time_ms: 3.772\n",
            "  timestamp: 1656954597\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 349200\n",
            "  training_iteration: 67\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:02 (running for 00:12:31.86)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         724.742</td><td style=\"text-align: right;\">349200</td><td style=\"text-align: right;\">  114.82</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            114.82</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:07 (running for 00:12:36.96)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         724.742</td><td style=\"text-align: right;\">349200</td><td style=\"text-align: right;\">  114.82</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            114.82</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 349800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 349800\n",
            "    num_agent_steps_trained: 349800\n",
            "    num_env_steps_sampled: 349800\n",
            "    num_env_steps_trained: 349800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-10-07\n",
            "  done: false\n",
            "  episode_len_mean: 112.63\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 112.63\n",
            "  episode_reward_min: 16.0\n",
            "  episodes_this_iter: 7\n",
            "  episodes_total: 3970\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 112.45\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 112.45\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 170\n",
            "      - 150\n",
            "      - 53\n",
            "      - 133\n",
            "      - 93\n",
            "      - 13\n",
            "      - 91\n",
            "      - 23\n",
            "      - 195\n",
            "      - 188\n",
            "      - 148\n",
            "      - 200\n",
            "      - 137\n",
            "      - 40\n",
            "      - 22\n",
            "      - 154\n",
            "      - 122\n",
            "      - 200\n",
            "      - 31\n",
            "      - 86\n",
            "      episode_reward:\n",
            "      - 170.0\n",
            "      - 150.0\n",
            "      - 53.0\n",
            "      - 133.0\n",
            "      - 93.0\n",
            "      - 13.0\n",
            "      - 91.0\n",
            "      - 23.0\n",
            "      - 195.0\n",
            "      - 188.0\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 137.0\n",
            "      - 40.0\n",
            "      - 22.0\n",
            "      - 154.0\n",
            "      - 122.0\n",
            "      - 200.0\n",
            "      - 31.0\n",
            "      - 86.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10028613171113095\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07665600794401188\n",
            "      mean_inference_ms: 0.9765394013889116\n",
            "      mean_raw_obs_processing_ms: 0.10722527666936084\n",
            "    timesteps_this_iter: 2249\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 119.98504638671875\n",
            "          policy_loss: -149.4425048828125\n",
            "          var_gnorm: 27.186250686645508\n",
            "          vf_explained_var: 0.014907658100128174\n",
            "          vf_loss: 19178.19140625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 349800\n",
            "    num_agent_steps_trained: 349800\n",
            "    num_env_steps_sampled: 349800\n",
            "    num_env_steps_trained: 349800\n",
            "  iterations_since_restore: 68\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 349800\n",
            "  num_agent_steps_trained: 349800\n",
            "  num_env_steps_sampled: 349800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 349800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 66.16\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13830806505505994\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10808106393398365\n",
            "    mean_inference_ms: 1.3116295635345312\n",
            "    mean_raw_obs_processing_ms: 0.24698704838901048\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 112.63\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 112.63\n",
            "    episode_reward_min: 16.0\n",
            "    episodes_this_iter: 7\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 122\n",
            "      - 16\n",
            "      - 62\n",
            "      - 31\n",
            "      - 135\n",
            "      - 111\n",
            "      - 28\n",
            "      - 117\n",
            "      - 124\n",
            "      - 141\n",
            "      - 179\n",
            "      - 29\n",
            "      - 31\n",
            "      - 107\n",
            "      - 161\n",
            "      - 45\n",
            "      - 88\n",
            "      - 120\n",
            "      - 200\n",
            "      - 61\n",
            "      - 126\n",
            "      - 60\n",
            "      - 55\n",
            "      - 150\n",
            "      - 174\n",
            "      - 71\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 133\n",
            "      - 160\n",
            "      - 108\n",
            "      - 142\n",
            "      - 67\n",
            "      - 192\n",
            "      - 125\n",
            "      - 141\n",
            "      - 100\n",
            "      - 34\n",
            "      - 103\n",
            "      - 114\n",
            "      - 78\n",
            "      - 193\n",
            "      - 32\n",
            "      - 36\n",
            "      - 34\n",
            "      - 73\n",
            "      - 134\n",
            "      - 200\n",
            "      - 116\n",
            "      - 132\n",
            "      - 66\n",
            "      - 37\n",
            "      - 143\n",
            "      - 130\n",
            "      - 103\n",
            "      - 148\n",
            "      - 143\n",
            "      - 197\n",
            "      - 128\n",
            "      - 200\n",
            "      - 147\n",
            "      - 66\n",
            "      - 126\n",
            "      - 200\n",
            "      - 33\n",
            "      - 163\n",
            "      - 46\n",
            "      - 133\n",
            "      - 184\n",
            "      - 154\n",
            "      - 200\n",
            "      - 53\n",
            "      - 21\n",
            "      - 164\n",
            "      - 132\n",
            "      - 17\n",
            "      - 200\n",
            "      - 31\n",
            "      - 200\n",
            "      - 200\n",
            "      - 16\n",
            "      - 44\n",
            "      - 147\n",
            "      - 200\n",
            "      - 147\n",
            "      - 62\n",
            "      - 22\n",
            "      - 143\n",
            "      - 187\n",
            "      - 150\n",
            "      - 105\n",
            "      - 68\n",
            "      - 58\n",
            "      - 63\n",
            "      - 21\n",
            "      - 51\n",
            "      - 41\n",
            "      - 182\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 16.0\n",
            "      - 62.0\n",
            "      - 31.0\n",
            "      - 135.0\n",
            "      - 111.0\n",
            "      - 28.0\n",
            "      - 117.0\n",
            "      - 124.0\n",
            "      - 141.0\n",
            "      - 179.0\n",
            "      - 29.0\n",
            "      - 31.0\n",
            "      - 107.0\n",
            "      - 161.0\n",
            "      - 45.0\n",
            "      - 88.0\n",
            "      - 120.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 126.0\n",
            "      - 60.0\n",
            "      - 55.0\n",
            "      - 150.0\n",
            "      - 174.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 133.0\n",
            "      - 160.0\n",
            "      - 108.0\n",
            "      - 142.0\n",
            "      - 67.0\n",
            "      - 192.0\n",
            "      - 125.0\n",
            "      - 141.0\n",
            "      - 100.0\n",
            "      - 34.0\n",
            "      - 103.0\n",
            "      - 114.0\n",
            "      - 78.0\n",
            "      - 193.0\n",
            "      - 32.0\n",
            "      - 36.0\n",
            "      - 34.0\n",
            "      - 73.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 116.0\n",
            "      - 132.0\n",
            "      - 66.0\n",
            "      - 37.0\n",
            "      - 143.0\n",
            "      - 130.0\n",
            "      - 103.0\n",
            "      - 148.0\n",
            "      - 143.0\n",
            "      - 197.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 66.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 163.0\n",
            "      - 46.0\n",
            "      - 133.0\n",
            "      - 184.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 21.0\n",
            "      - 164.0\n",
            "      - 132.0\n",
            "      - 17.0\n",
            "      - 200.0\n",
            "      - 31.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 16.0\n",
            "      - 44.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 62.0\n",
            "      - 22.0\n",
            "      - 143.0\n",
            "      - 187.0\n",
            "      - 150.0\n",
            "      - 105.0\n",
            "      - 68.0\n",
            "      - 58.0\n",
            "      - 63.0\n",
            "      - 21.0\n",
            "      - 51.0\n",
            "      - 41.0\n",
            "      - 182.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13830806505505994\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10808106393398365\n",
            "      mean_inference_ms: 1.3116295635345312\n",
            "      mean_raw_obs_processing_ms: 0.24698704838901048\n",
            "  time_since_restore: 734.8710956573486\n",
            "  time_this_iter_s: 10.128988027572632\n",
            "  time_total_s: 734.8710956573486\n",
            "  timers:\n",
            "    learn_throughput: 30599.053\n",
            "    learn_time_ms: 6.536\n",
            "    load_throughput: 983885.527\n",
            "    load_time_ms: 0.203\n",
            "    training_iteration_time_ms: 214.507\n",
            "    update_time_ms: 3.706\n",
            "  timestamp: 1656954607\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 349800\n",
            "  training_iteration: 68\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:12 (running for 00:12:42.04)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         734.871</td><td style=\"text-align: right;\">349800</td><td style=\"text-align: right;\">  112.63</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            112.63</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:17 (running for 00:12:47.18)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         734.871</td><td style=\"text-align: right;\">349800</td><td style=\"text-align: right;\">  112.63</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  16</td><td style=\"text-align: right;\">            112.63</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 359600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 359600\n",
            "    num_agent_steps_trained: 359600\n",
            "    num_env_steps_sampled: 359600\n",
            "    num_env_steps_trained: 359600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-10-17\n",
            "  done: false\n",
            "  episode_len_mean: 115.25\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 115.25\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 82\n",
            "  episodes_total: 4052\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 122.98880004882812\n",
            "          policy_loss: 376.4736022949219\n",
            "          var_gnorm: 27.27185821533203\n",
            "          vf_explained_var: 0.04136466979980469\n",
            "          vf_loss: 10140.234375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 359600\n",
            "    num_agent_steps_trained: 359600\n",
            "    num_env_steps_sampled: 359600\n",
            "    num_env_steps_trained: 359600\n",
            "  iterations_since_restore: 69\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 359600\n",
            "  num_agent_steps_trained: 359600\n",
            "  num_env_steps_sampled: 359600\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 359600\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.12666666666668\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13824519710077596\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10803427339953559\n",
            "    mean_inference_ms: 1.3106400619359295\n",
            "    mean_raw_obs_processing_ms: 0.24686830029984277\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 115.25\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 115.25\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 82\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 16\n",
            "      - 44\n",
            "      - 147\n",
            "      - 200\n",
            "      - 147\n",
            "      - 62\n",
            "      - 22\n",
            "      - 143\n",
            "      - 187\n",
            "      - 150\n",
            "      - 105\n",
            "      - 68\n",
            "      - 58\n",
            "      - 63\n",
            "      - 21\n",
            "      - 51\n",
            "      - 41\n",
            "      - 182\n",
            "      - 153\n",
            "      - 200\n",
            "      - 129\n",
            "      - 122\n",
            "      - 148\n",
            "      - 200\n",
            "      - 52\n",
            "      - 124\n",
            "      - 95\n",
            "      - 66\n",
            "      - 52\n",
            "      - 141\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 200\n",
            "      - 27\n",
            "      - 99\n",
            "      - 128\n",
            "      - 161\n",
            "      - 200\n",
            "      - 144\n",
            "      - 83\n",
            "      - 70\n",
            "      - 200\n",
            "      - 13\n",
            "      - 183\n",
            "      - 142\n",
            "      - 85\n",
            "      - 130\n",
            "      - 25\n",
            "      - 105\n",
            "      - 58\n",
            "      - 29\n",
            "      - 21\n",
            "      - 42\n",
            "      - 50\n",
            "      - 139\n",
            "      - 142\n",
            "      - 178\n",
            "      - 69\n",
            "      - 66\n",
            "      - 37\n",
            "      - 179\n",
            "      - 200\n",
            "      - 43\n",
            "      - 124\n",
            "      - 200\n",
            "      - 195\n",
            "      - 195\n",
            "      - 200\n",
            "      - 154\n",
            "      - 25\n",
            "      - 26\n",
            "      - 67\n",
            "      - 200\n",
            "      - 157\n",
            "      - 72\n",
            "      - 85\n",
            "      - 14\n",
            "      - 94\n",
            "      - 82\n",
            "      - 77\n",
            "      - 134\n",
            "      - 154\n",
            "      - 37\n",
            "      - 200\n",
            "      - 189\n",
            "      - 21\n",
            "      - 133\n",
            "      - 86\n",
            "      - 155\n",
            "      - 146\n",
            "      - 113\n",
            "      - 135\n",
            "      - 200\n",
            "      - 154\n",
            "      - 151\n",
            "      - 80\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 16.0\n",
            "      - 44.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 62.0\n",
            "      - 22.0\n",
            "      - 143.0\n",
            "      - 187.0\n",
            "      - 150.0\n",
            "      - 105.0\n",
            "      - 68.0\n",
            "      - 58.0\n",
            "      - 63.0\n",
            "      - 21.0\n",
            "      - 51.0\n",
            "      - 41.0\n",
            "      - 182.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 129.0\n",
            "      - 122.0\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 124.0\n",
            "      - 95.0\n",
            "      - 66.0\n",
            "      - 52.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 99.0\n",
            "      - 128.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 144.0\n",
            "      - 83.0\n",
            "      - 70.0\n",
            "      - 200.0\n",
            "      - 13.0\n",
            "      - 183.0\n",
            "      - 142.0\n",
            "      - 85.0\n",
            "      - 130.0\n",
            "      - 25.0\n",
            "      - 105.0\n",
            "      - 58.0\n",
            "      - 29.0\n",
            "      - 21.0\n",
            "      - 42.0\n",
            "      - 50.0\n",
            "      - 139.0\n",
            "      - 142.0\n",
            "      - 178.0\n",
            "      - 69.0\n",
            "      - 66.0\n",
            "      - 37.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 124.0\n",
            "      - 200.0\n",
            "      - 195.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 25.0\n",
            "      - 26.0\n",
            "      - 67.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 72.0\n",
            "      - 85.0\n",
            "      - 14.0\n",
            "      - 94.0\n",
            "      - 82.0\n",
            "      - 77.0\n",
            "      - 134.0\n",
            "      - 154.0\n",
            "      - 37.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 21.0\n",
            "      - 133.0\n",
            "      - 86.0\n",
            "      - 155.0\n",
            "      - 146.0\n",
            "      - 113.0\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 151.0\n",
            "      - 80.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13824519710077596\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10803427339953559\n",
            "      mean_inference_ms: 1.3106400619359295\n",
            "      mean_raw_obs_processing_ms: 0.24686830029984277\n",
            "  time_since_restore: 745.090913772583\n",
            "  time_this_iter_s: 10.219818115234375\n",
            "  time_total_s: 745.090913772583\n",
            "  timers:\n",
            "    learn_throughput: 28312.44\n",
            "    learn_time_ms: 7.064\n",
            "    load_throughput: 1029908.901\n",
            "    load_time_ms: 0.194\n",
            "    training_iteration_time_ms: 212.972\n",
            "    update_time_ms: 3.608\n",
            "  timestamp: 1656954617\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 359600\n",
            "  training_iteration: 69\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:22 (running for 00:12:52.29)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         745.091</td><td style=\"text-align: right;\">359600</td><td style=\"text-align: right;\">  115.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            115.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:27 (running for 00:12:57.38)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         745.091</td><td style=\"text-align: right;\">359600</td><td style=\"text-align: right;\">  115.25</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            115.25</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 360200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 360200\n",
            "    num_agent_steps_trained: 360200\n",
            "    num_env_steps_sampled: 360200\n",
            "    num_env_steps_trained: 360200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-10-28\n",
            "  done: false\n",
            "  episode_len_mean: 116.61\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 116.61\n",
            "  episode_reward_min: 13.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 4056\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 118.7\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 118.7\n",
            "    episode_reward_min: 21.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 76\n",
            "      - 192\n",
            "      - 88\n",
            "      - 107\n",
            "      - 188\n",
            "      - 69\n",
            "      - 21\n",
            "      - 149\n",
            "      - 134\n",
            "      - 113\n",
            "      - 51\n",
            "      - 143\n",
            "      - 36\n",
            "      - 200\n",
            "      - 200\n",
            "      - 43\n",
            "      - 132\n",
            "      - 110\n",
            "      - 142\n",
            "      - 180\n",
            "      episode_reward:\n",
            "      - 76.0\n",
            "      - 192.0\n",
            "      - 88.0\n",
            "      - 107.0\n",
            "      - 188.0\n",
            "      - 69.0\n",
            "      - 21.0\n",
            "      - 149.0\n",
            "      - 134.0\n",
            "      - 113.0\n",
            "      - 51.0\n",
            "      - 143.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 132.0\n",
            "      - 110.0\n",
            "      - 142.0\n",
            "      - 180.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10026334863778737\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07659886889227926\n",
            "      mean_inference_ms: 0.9758316171872103\n",
            "      mean_raw_obs_processing_ms: 0.10712438819741207\n",
            "    timesteps_this_iter: 2374\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.3582992553711\n",
            "          policy_loss: 583.459716796875\n",
            "          var_gnorm: 27.277978897094727\n",
            "          vf_explained_var: 0.5147615671157837\n",
            "          vf_loss: 3964.221435546875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 360200\n",
            "    num_agent_steps_trained: 360200\n",
            "    num_env_steps_sampled: 360200\n",
            "    num_env_steps_trained: 360200\n",
            "  iterations_since_restore: 70\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 360200\n",
            "  num_agent_steps_trained: 360200\n",
            "  num_env_steps_sampled: 360200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 360200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.02666666666667\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13826964357878052\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1079979268563658\n",
            "    mean_inference_ms: 1.310776331846411\n",
            "    mean_raw_obs_processing_ms: 0.24681381550138162\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 116.61\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 116.61\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 147\n",
            "      - 62\n",
            "      - 22\n",
            "      - 143\n",
            "      - 187\n",
            "      - 150\n",
            "      - 105\n",
            "      - 68\n",
            "      - 58\n",
            "      - 63\n",
            "      - 21\n",
            "      - 51\n",
            "      - 41\n",
            "      - 182\n",
            "      - 153\n",
            "      - 200\n",
            "      - 129\n",
            "      - 122\n",
            "      - 148\n",
            "      - 200\n",
            "      - 52\n",
            "      - 124\n",
            "      - 95\n",
            "      - 66\n",
            "      - 52\n",
            "      - 141\n",
            "      - 200\n",
            "      - 76\n",
            "      - 200\n",
            "      - 200\n",
            "      - 27\n",
            "      - 99\n",
            "      - 128\n",
            "      - 161\n",
            "      - 200\n",
            "      - 144\n",
            "      - 83\n",
            "      - 70\n",
            "      - 200\n",
            "      - 13\n",
            "      - 183\n",
            "      - 142\n",
            "      - 85\n",
            "      - 130\n",
            "      - 25\n",
            "      - 105\n",
            "      - 58\n",
            "      - 29\n",
            "      - 21\n",
            "      - 42\n",
            "      - 50\n",
            "      - 139\n",
            "      - 142\n",
            "      - 178\n",
            "      - 69\n",
            "      - 66\n",
            "      - 37\n",
            "      - 179\n",
            "      - 200\n",
            "      - 43\n",
            "      - 124\n",
            "      - 200\n",
            "      - 195\n",
            "      - 195\n",
            "      - 200\n",
            "      - 154\n",
            "      - 25\n",
            "      - 26\n",
            "      - 67\n",
            "      - 200\n",
            "      - 157\n",
            "      - 72\n",
            "      - 85\n",
            "      - 14\n",
            "      - 94\n",
            "      - 82\n",
            "      - 77\n",
            "      - 134\n",
            "      - 154\n",
            "      - 37\n",
            "      - 200\n",
            "      - 189\n",
            "      - 21\n",
            "      - 133\n",
            "      - 86\n",
            "      - 155\n",
            "      - 146\n",
            "      - 113\n",
            "      - 135\n",
            "      - 200\n",
            "      - 154\n",
            "      - 151\n",
            "      - 80\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 84\n",
            "      - 84\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 147.0\n",
            "      - 62.0\n",
            "      - 22.0\n",
            "      - 143.0\n",
            "      - 187.0\n",
            "      - 150.0\n",
            "      - 105.0\n",
            "      - 68.0\n",
            "      - 58.0\n",
            "      - 63.0\n",
            "      - 21.0\n",
            "      - 51.0\n",
            "      - 41.0\n",
            "      - 182.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 129.0\n",
            "      - 122.0\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 124.0\n",
            "      - 95.0\n",
            "      - 66.0\n",
            "      - 52.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 76.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 99.0\n",
            "      - 128.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 144.0\n",
            "      - 83.0\n",
            "      - 70.0\n",
            "      - 200.0\n",
            "      - 13.0\n",
            "      - 183.0\n",
            "      - 142.0\n",
            "      - 85.0\n",
            "      - 130.0\n",
            "      - 25.0\n",
            "      - 105.0\n",
            "      - 58.0\n",
            "      - 29.0\n",
            "      - 21.0\n",
            "      - 42.0\n",
            "      - 50.0\n",
            "      - 139.0\n",
            "      - 142.0\n",
            "      - 178.0\n",
            "      - 69.0\n",
            "      - 66.0\n",
            "      - 37.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 124.0\n",
            "      - 200.0\n",
            "      - 195.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 25.0\n",
            "      - 26.0\n",
            "      - 67.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 72.0\n",
            "      - 85.0\n",
            "      - 14.0\n",
            "      - 94.0\n",
            "      - 82.0\n",
            "      - 77.0\n",
            "      - 134.0\n",
            "      - 154.0\n",
            "      - 37.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 21.0\n",
            "      - 133.0\n",
            "      - 86.0\n",
            "      - 155.0\n",
            "      - 146.0\n",
            "      - 113.0\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 151.0\n",
            "      - 80.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 84.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13826964357878052\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1079979268563658\n",
            "      mean_inference_ms: 1.310776331846411\n",
            "      mean_raw_obs_processing_ms: 0.24681381550138162\n",
            "  time_since_restore: 755.5368711948395\n",
            "  time_this_iter_s: 10.44595742225647\n",
            "  time_total_s: 755.5368711948395\n",
            "  timers:\n",
            "    learn_throughput: 28388.421\n",
            "    learn_time_ms: 7.045\n",
            "    load_throughput: 1081006.186\n",
            "    load_time_ms: 0.185\n",
            "    training_iteration_time_ms: 212.844\n",
            "    update_time_ms: 3.214\n",
            "  timestamp: 1656954628\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 360200\n",
            "  training_iteration: 70\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:33 (running for 00:13:02.77)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         755.537</td><td style=\"text-align: right;\">360200</td><td style=\"text-align: right;\">  116.61</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            116.61</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:38 (running for 00:13:07.93)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         755.537</td><td style=\"text-align: right;\">360200</td><td style=\"text-align: right;\">  116.61</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  13</td><td style=\"text-align: right;\">            116.61</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 369600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 369600\n",
            "    num_agent_steps_trained: 369600\n",
            "    num_env_steps_sampled: 369600\n",
            "    num_env_steps_trained: 369600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-10-38\n",
            "  done: false\n",
            "  episode_len_mean: 131.39\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 131.39\n",
            "  episode_reward_min: 14.0\n",
            "  episodes_this_iter: 71\n",
            "  episodes_total: 4127\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 126.2467041015625\n",
            "          policy_loss: 31.060314178466797\n",
            "          var_gnorm: 27.371673583984375\n",
            "          vf_explained_var: 0.015121996402740479\n",
            "          vf_loss: 18238.859375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 369600\n",
            "    num_agent_steps_trained: 369600\n",
            "    num_env_steps_sampled: 369600\n",
            "    num_env_steps_trained: 369600\n",
            "  iterations_since_restore: 71\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 369600\n",
            "  num_agent_steps_trained: 369600\n",
            "  num_env_steps_sampled: 369600\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 369600\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 95.40714285714286\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13814424320241434\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10813683288967785\n",
            "    mean_inference_ms: 1.3096040107373565\n",
            "    mean_raw_obs_processing_ms: 0.2469818485138668\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 131.39\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 131.39\n",
            "    episode_reward_min: 14.0\n",
            "    episodes_this_iter: 71\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 72\n",
            "      - 85\n",
            "      - 14\n",
            "      - 94\n",
            "      - 82\n",
            "      - 77\n",
            "      - 134\n",
            "      - 154\n",
            "      - 37\n",
            "      - 200\n",
            "      - 189\n",
            "      - 21\n",
            "      - 133\n",
            "      - 86\n",
            "      - 155\n",
            "      - 146\n",
            "      - 113\n",
            "      - 135\n",
            "      - 200\n",
            "      - 154\n",
            "      - 151\n",
            "      - 80\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 84\n",
            "      - 84\n",
            "      - 200\n",
            "      - 138\n",
            "      - 175\n",
            "      - 145\n",
            "      - 21\n",
            "      - 113\n",
            "      - 129\n",
            "      - 200\n",
            "      - 166\n",
            "      - 32\n",
            "      - 169\n",
            "      - 163\n",
            "      - 75\n",
            "      - 114\n",
            "      - 184\n",
            "      - 183\n",
            "      - 172\n",
            "      - 189\n",
            "      - 136\n",
            "      - 121\n",
            "      - 193\n",
            "      - 82\n",
            "      - 81\n",
            "      - 167\n",
            "      - 152\n",
            "      - 116\n",
            "      - 142\n",
            "      - 188\n",
            "      - 157\n",
            "      - 200\n",
            "      - 153\n",
            "      - 200\n",
            "      - 61\n",
            "      - 131\n",
            "      - 97\n",
            "      - 179\n",
            "      - 141\n",
            "      - 137\n",
            "      - 36\n",
            "      - 200\n",
            "      - 200\n",
            "      - 37\n",
            "      - 109\n",
            "      - 84\n",
            "      - 80\n",
            "      - 168\n",
            "      - 200\n",
            "      - 200\n",
            "      - 152\n",
            "      - 200\n",
            "      - 188\n",
            "      - 105\n",
            "      - 28\n",
            "      - 34\n",
            "      - 87\n",
            "      - 200\n",
            "      - 180\n",
            "      - 140\n",
            "      - 178\n",
            "      - 76\n",
            "      - 66\n",
            "      - 102\n",
            "      - 89\n",
            "      - 200\n",
            "      - 181\n",
            "      - 23\n",
            "      - 164\n",
            "      - 41\n",
            "      - 46\n",
            "      - 160\n",
            "      - 171\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 72.0\n",
            "      - 85.0\n",
            "      - 14.0\n",
            "      - 94.0\n",
            "      - 82.0\n",
            "      - 77.0\n",
            "      - 134.0\n",
            "      - 154.0\n",
            "      - 37.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 21.0\n",
            "      - 133.0\n",
            "      - 86.0\n",
            "      - 155.0\n",
            "      - 146.0\n",
            "      - 113.0\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 151.0\n",
            "      - 80.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 84.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 175.0\n",
            "      - 145.0\n",
            "      - 21.0\n",
            "      - 113.0\n",
            "      - 129.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 32.0\n",
            "      - 169.0\n",
            "      - 163.0\n",
            "      - 75.0\n",
            "      - 114.0\n",
            "      - 184.0\n",
            "      - 183.0\n",
            "      - 172.0\n",
            "      - 189.0\n",
            "      - 136.0\n",
            "      - 121.0\n",
            "      - 193.0\n",
            "      - 82.0\n",
            "      - 81.0\n",
            "      - 167.0\n",
            "      - 152.0\n",
            "      - 116.0\n",
            "      - 142.0\n",
            "      - 188.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 131.0\n",
            "      - 97.0\n",
            "      - 179.0\n",
            "      - 141.0\n",
            "      - 137.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 37.0\n",
            "      - 109.0\n",
            "      - 84.0\n",
            "      - 80.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 105.0\n",
            "      - 28.0\n",
            "      - 34.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 140.0\n",
            "      - 178.0\n",
            "      - 76.0\n",
            "      - 66.0\n",
            "      - 102.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 23.0\n",
            "      - 164.0\n",
            "      - 41.0\n",
            "      - 46.0\n",
            "      - 160.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13814424320241434\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10813683288967785\n",
            "      mean_inference_ms: 1.3096040107373565\n",
            "      mean_raw_obs_processing_ms: 0.2469818485138668\n",
            "  time_since_restore: 765.6933829784393\n",
            "  time_this_iter_s: 10.156511783599854\n",
            "  time_total_s: 765.6933829784393\n",
            "  timers:\n",
            "    learn_throughput: 31498.109\n",
            "    learn_time_ms: 6.35\n",
            "    load_throughput: 908940.08\n",
            "    load_time_ms: 0.22\n",
            "    training_iteration_time_ms: 218.728\n",
            "    update_time_ms: 4.492\n",
            "  timestamp: 1656954638\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 369600\n",
            "  training_iteration: 71\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:43 (running for 00:13:12.98)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         765.693</td><td style=\"text-align: right;\">369600</td><td style=\"text-align: right;\">  131.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:48 (running for 00:13:18.07)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         765.693</td><td style=\"text-align: right;\">369600</td><td style=\"text-align: right;\">  131.39</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  14</td><td style=\"text-align: right;\">            131.39</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 370200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 370200\n",
            "    num_agent_steps_trained: 370200\n",
            "    num_env_steps_sampled: 370200\n",
            "    num_env_steps_trained: 370200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-10-49\n",
            "  done: false\n",
            "  episode_len_mean: 133.38\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 133.38\n",
            "  episode_reward_min: 21.0\n",
            "  episodes_this_iter: 6\n",
            "  episodes_total: 4133\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 130.1\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 130.1\n",
            "    episode_reward_min: 20.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 123\n",
            "      - 198\n",
            "      - 84\n",
            "      - 158\n",
            "      - 128\n",
            "      - 108\n",
            "      - 71\n",
            "      - 200\n",
            "      - 74\n",
            "      - 46\n",
            "      - 136\n",
            "      - 200\n",
            "      - 91\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 21\n",
            "      - 20\n",
            "      - 200\n",
            "      - 144\n",
            "      episode_reward:\n",
            "      - 123.0\n",
            "      - 198.0\n",
            "      - 84.0\n",
            "      - 158.0\n",
            "      - 128.0\n",
            "      - 108.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 74.0\n",
            "      - 46.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 91.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 21.0\n",
            "      - 20.0\n",
            "      - 200.0\n",
            "      - 144.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10020911175916543\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07653448045968478\n",
            "      mean_inference_ms: 0.975073063140993\n",
            "      mean_raw_obs_processing_ms: 0.10700207179131649\n",
            "    timesteps_this_iter: 2602\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 113.01292419433594\n",
            "          policy_loss: -128.1894073486328\n",
            "          var_gnorm: 27.377595901489258\n",
            "          vf_explained_var: 0.06377947330474854\n",
            "          vf_loss: 17062.958984375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 370200\n",
            "    num_agent_steps_trained: 370200\n",
            "    num_env_steps_sampled: 370200\n",
            "    num_env_steps_trained: 370200\n",
            "  iterations_since_restore: 72\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 370200\n",
            "  num_agent_steps_trained: 370200\n",
            "  num_env_steps_sampled: 370200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 370200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.79333333333334\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13817216913157904\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10810384116367523\n",
            "    mean_inference_ms: 1.3097699350435006\n",
            "    mean_raw_obs_processing_ms: 0.24693938634944546\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 133.38\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 133.38\n",
            "    episode_reward_min: 21.0\n",
            "    episodes_this_iter: 6\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 134\n",
            "      - 154\n",
            "      - 37\n",
            "      - 200\n",
            "      - 189\n",
            "      - 21\n",
            "      - 133\n",
            "      - 86\n",
            "      - 155\n",
            "      - 146\n",
            "      - 113\n",
            "      - 135\n",
            "      - 200\n",
            "      - 154\n",
            "      - 151\n",
            "      - 80\n",
            "      - 157\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 84\n",
            "      - 84\n",
            "      - 200\n",
            "      - 138\n",
            "      - 175\n",
            "      - 145\n",
            "      - 21\n",
            "      - 113\n",
            "      - 129\n",
            "      - 200\n",
            "      - 166\n",
            "      - 32\n",
            "      - 169\n",
            "      - 163\n",
            "      - 75\n",
            "      - 114\n",
            "      - 184\n",
            "      - 183\n",
            "      - 172\n",
            "      - 189\n",
            "      - 136\n",
            "      - 121\n",
            "      - 193\n",
            "      - 82\n",
            "      - 81\n",
            "      - 167\n",
            "      - 152\n",
            "      - 116\n",
            "      - 142\n",
            "      - 188\n",
            "      - 157\n",
            "      - 200\n",
            "      - 153\n",
            "      - 200\n",
            "      - 61\n",
            "      - 131\n",
            "      - 97\n",
            "      - 179\n",
            "      - 141\n",
            "      - 137\n",
            "      - 36\n",
            "      - 200\n",
            "      - 200\n",
            "      - 37\n",
            "      - 109\n",
            "      - 84\n",
            "      - 80\n",
            "      - 168\n",
            "      - 200\n",
            "      - 200\n",
            "      - 152\n",
            "      - 200\n",
            "      - 188\n",
            "      - 105\n",
            "      - 28\n",
            "      - 34\n",
            "      - 87\n",
            "      - 200\n",
            "      - 180\n",
            "      - 140\n",
            "      - 178\n",
            "      - 76\n",
            "      - 66\n",
            "      - 102\n",
            "      - 89\n",
            "      - 200\n",
            "      - 181\n",
            "      - 23\n",
            "      - 164\n",
            "      - 41\n",
            "      - 46\n",
            "      - 160\n",
            "      - 171\n",
            "      - 200\n",
            "      - 93\n",
            "      - 200\n",
            "      - 56\n",
            "      - 187\n",
            "      - 48\n",
            "      - 39\n",
            "      episode_reward:\n",
            "      - 134.0\n",
            "      - 154.0\n",
            "      - 37.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 21.0\n",
            "      - 133.0\n",
            "      - 86.0\n",
            "      - 155.0\n",
            "      - 146.0\n",
            "      - 113.0\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 151.0\n",
            "      - 80.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 84.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 175.0\n",
            "      - 145.0\n",
            "      - 21.0\n",
            "      - 113.0\n",
            "      - 129.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 32.0\n",
            "      - 169.0\n",
            "      - 163.0\n",
            "      - 75.0\n",
            "      - 114.0\n",
            "      - 184.0\n",
            "      - 183.0\n",
            "      - 172.0\n",
            "      - 189.0\n",
            "      - 136.0\n",
            "      - 121.0\n",
            "      - 193.0\n",
            "      - 82.0\n",
            "      - 81.0\n",
            "      - 167.0\n",
            "      - 152.0\n",
            "      - 116.0\n",
            "      - 142.0\n",
            "      - 188.0\n",
            "      - 157.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 131.0\n",
            "      - 97.0\n",
            "      - 179.0\n",
            "      - 141.0\n",
            "      - 137.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 37.0\n",
            "      - 109.0\n",
            "      - 84.0\n",
            "      - 80.0\n",
            "      - 168.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 105.0\n",
            "      - 28.0\n",
            "      - 34.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 140.0\n",
            "      - 178.0\n",
            "      - 76.0\n",
            "      - 66.0\n",
            "      - 102.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 23.0\n",
            "      - 164.0\n",
            "      - 41.0\n",
            "      - 46.0\n",
            "      - 160.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 187.0\n",
            "      - 48.0\n",
            "      - 39.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13817216913157904\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10810384116367523\n",
            "      mean_inference_ms: 1.3097699350435006\n",
            "      mean_raw_obs_processing_ms: 0.24693938634944546\n",
            "  time_since_restore: 776.3584620952606\n",
            "  time_this_iter_s: 10.665079116821289\n",
            "  time_total_s: 776.3584620952606\n",
            "  timers:\n",
            "    learn_throughput: 31226.322\n",
            "    learn_time_ms: 6.405\n",
            "    load_throughput: 901709.986\n",
            "    load_time_ms: 0.222\n",
            "    training_iteration_time_ms: 217.304\n",
            "    update_time_ms: 4.601\n",
            "  timestamp: 1656954649\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 370200\n",
            "  training_iteration: 72\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:54 (running for 00:13:23.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         776.358</td><td style=\"text-align: right;\">370200</td><td style=\"text-align: right;\">  133.38</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            133.38</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:10:59 (running for 00:13:28.76)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         776.358</td><td style=\"text-align: right;\">370200</td><td style=\"text-align: right;\">  133.38</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            133.38</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 380000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 380000\n",
            "    num_agent_steps_trained: 380000\n",
            "    num_env_steps_sampled: 380000\n",
            "    num_env_steps_trained: 380000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-10-59\n",
            "  done: false\n",
            "  episode_len_mean: 120.75\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 120.75\n",
            "  episode_reward_min: 21.0\n",
            "  episodes_this_iter: 80\n",
            "  episodes_total: 4213\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.00000762939453\n",
            "          model: {}\n",
            "          policy_entropy: 125.42897033691406\n",
            "          policy_loss: 298.3131103515625\n",
            "          var_gnorm: 27.45111083984375\n",
            "          vf_explained_var: -0.001389622688293457\n",
            "          vf_loss: 11733.634765625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 380000\n",
            "    num_agent_steps_trained: 380000\n",
            "    num_env_steps_sampled: 380000\n",
            "    num_env_steps_trained: 380000\n",
            "  iterations_since_restore: 73\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 380000\n",
            "  num_agent_steps_trained: 380000\n",
            "  num_env_steps_sampled: 380000\n",
            "  num_env_steps_sampled_this_iter: 9800\n",
            "  num_env_steps_trained: 380000\n",
            "  num_env_steps_trained_this_iter: 9800\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.79333333333334\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13811443925022374\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10789621178867867\n",
            "    mean_inference_ms: 1.3085147302782847\n",
            "    mean_raw_obs_processing_ms: 0.24654276322652124\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 120.75\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 120.75\n",
            "    episode_reward_min: 21.0\n",
            "    episodes_this_iter: 80\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 178\n",
            "      - 76\n",
            "      - 66\n",
            "      - 102\n",
            "      - 89\n",
            "      - 200\n",
            "      - 181\n",
            "      - 23\n",
            "      - 164\n",
            "      - 41\n",
            "      - 46\n",
            "      - 160\n",
            "      - 171\n",
            "      - 200\n",
            "      - 93\n",
            "      - 200\n",
            "      - 56\n",
            "      - 187\n",
            "      - 48\n",
            "      - 39\n",
            "      - 35\n",
            "      - 200\n",
            "      - 200\n",
            "      - 56\n",
            "      - 39\n",
            "      - 65\n",
            "      - 38\n",
            "      - 40\n",
            "      - 42\n",
            "      - 191\n",
            "      - 85\n",
            "      - 183\n",
            "      - 168\n",
            "      - 57\n",
            "      - 161\n",
            "      - 54\n",
            "      - 81\n",
            "      - 121\n",
            "      - 149\n",
            "      - 189\n",
            "      - 60\n",
            "      - 53\n",
            "      - 65\n",
            "      - 162\n",
            "      - 42\n",
            "      - 151\n",
            "      - 145\n",
            "      - 93\n",
            "      - 146\n",
            "      - 181\n",
            "      - 149\n",
            "      - 80\n",
            "      - 200\n",
            "      - 55\n",
            "      - 116\n",
            "      - 108\n",
            "      - 113\n",
            "      - 200\n",
            "      - 163\n",
            "      - 179\n",
            "      - 97\n",
            "      - 98\n",
            "      - 53\n",
            "      - 112\n",
            "      - 200\n",
            "      - 193\n",
            "      - 154\n",
            "      - 38\n",
            "      - 200\n",
            "      - 147\n",
            "      - 177\n",
            "      - 82\n",
            "      - 200\n",
            "      - 139\n",
            "      - 21\n",
            "      - 67\n",
            "      - 55\n",
            "      - 139\n",
            "      - 135\n",
            "      - 174\n",
            "      - 82\n",
            "      - 200\n",
            "      - 28\n",
            "      - 139\n",
            "      - 30\n",
            "      - 165\n",
            "      - 161\n",
            "      - 143\n",
            "      - 115\n",
            "      - 200\n",
            "      - 54\n",
            "      - 200\n",
            "      - 122\n",
            "      - 141\n",
            "      - 148\n",
            "      - 63\n",
            "      - 142\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 178.0\n",
            "      - 76.0\n",
            "      - 66.0\n",
            "      - 102.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 23.0\n",
            "      - 164.0\n",
            "      - 41.0\n",
            "      - 46.0\n",
            "      - 160.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 187.0\n",
            "      - 48.0\n",
            "      - 39.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 39.0\n",
            "      - 65.0\n",
            "      - 38.0\n",
            "      - 40.0\n",
            "      - 42.0\n",
            "      - 191.0\n",
            "      - 85.0\n",
            "      - 183.0\n",
            "      - 168.0\n",
            "      - 57.0\n",
            "      - 161.0\n",
            "      - 54.0\n",
            "      - 81.0\n",
            "      - 121.0\n",
            "      - 149.0\n",
            "      - 189.0\n",
            "      - 60.0\n",
            "      - 53.0\n",
            "      - 65.0\n",
            "      - 162.0\n",
            "      - 42.0\n",
            "      - 151.0\n",
            "      - 145.0\n",
            "      - 93.0\n",
            "      - 146.0\n",
            "      - 181.0\n",
            "      - 149.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 55.0\n",
            "      - 116.0\n",
            "      - 108.0\n",
            "      - 113.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 179.0\n",
            "      - 97.0\n",
            "      - 98.0\n",
            "      - 53.0\n",
            "      - 112.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 154.0\n",
            "      - 38.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 177.0\n",
            "      - 82.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 21.0\n",
            "      - 67.0\n",
            "      - 55.0\n",
            "      - 139.0\n",
            "      - 135.0\n",
            "      - 174.0\n",
            "      - 82.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 139.0\n",
            "      - 30.0\n",
            "      - 165.0\n",
            "      - 161.0\n",
            "      - 143.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 54.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 141.0\n",
            "      - 148.0\n",
            "      - 63.0\n",
            "      - 142.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13811443925022374\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10789621178867867\n",
            "      mean_inference_ms: 1.3085147302782847\n",
            "      mean_raw_obs_processing_ms: 0.24654276322652124\n",
            "  time_since_restore: 786.4099297523499\n",
            "  time_this_iter_s: 10.051467657089233\n",
            "  time_total_s: 786.4099297523499\n",
            "  timers:\n",
            "    learn_throughput: 30924.49\n",
            "    learn_time_ms: 6.467\n",
            "    load_throughput: 906092.893\n",
            "    load_time_ms: 0.221\n",
            "    training_iteration_time_ms: 205.247\n",
            "    update_time_ms: 3.675\n",
            "  timestamp: 1656954659\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 380000\n",
            "  training_iteration: 73\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:04 (running for 00:13:33.80)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">          786.41</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">  120.75</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            120.75</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:09 (running for 00:13:38.89)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">          786.41</td><td style=\"text-align: right;\">380000</td><td style=\"text-align: right;\">  120.75</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            120.75</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 380600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 380600\n",
            "    num_agent_steps_trained: 380600\n",
            "    num_env_steps_sampled: 380600\n",
            "    num_env_steps_trained: 380600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-11-09\n",
            "  done: false\n",
            "  episode_len_mean: 118.12\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 118.12\n",
            "  episode_reward_min: 21.0\n",
            "  episodes_this_iter: 7\n",
            "  episodes_total: 4220\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 136.9\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 136.9\n",
            "    episode_reward_min: 17.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 17\n",
            "      - 155\n",
            "      - 200\n",
            "      - 126\n",
            "      - 81\n",
            "      - 164\n",
            "      - 122\n",
            "      - 134\n",
            "      - 200\n",
            "      - 78\n",
            "      - 149\n",
            "      - 146\n",
            "      - 77\n",
            "      - 134\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 131\n",
            "      - 31\n",
            "      - 193\n",
            "      episode_reward:\n",
            "      - 17.0\n",
            "      - 155.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 81.0\n",
            "      - 164.0\n",
            "      - 122.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 78.0\n",
            "      - 149.0\n",
            "      - 146.0\n",
            "      - 77.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 31.0\n",
            "      - 193.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10019963418659043\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07651137245334957\n",
            "      mean_inference_ms: 0.9749513305575214\n",
            "      mean_raw_obs_processing_ms: 0.10692620842589091\n",
            "    timesteps_this_iter: 2738\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 115.82368469238281\n",
            "          policy_loss: -189.9019775390625\n",
            "          var_gnorm: 27.455488204956055\n",
            "          vf_explained_var: 0.272588312625885\n",
            "          vf_loss: 15956.1162109375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 380600\n",
            "    num_agent_steps_trained: 380600\n",
            "    num_env_steps_sampled: 380600\n",
            "    num_env_steps_trained: 380600\n",
            "  iterations_since_restore: 74\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 380600\n",
            "  num_agent_steps_trained: 380600\n",
            "  num_env_steps_sampled: 380600\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 380600\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 64.96666666666667\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13813661511077874\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10784299506191149\n",
            "    mean_inference_ms: 1.3086217717783353\n",
            "    mean_raw_obs_processing_ms: 0.2464619936449884\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 118.12\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 118.12\n",
            "    episode_reward_min: 21.0\n",
            "    episodes_this_iter: 7\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 23\n",
            "      - 164\n",
            "      - 41\n",
            "      - 46\n",
            "      - 160\n",
            "      - 171\n",
            "      - 200\n",
            "      - 93\n",
            "      - 200\n",
            "      - 56\n",
            "      - 187\n",
            "      - 48\n",
            "      - 39\n",
            "      - 35\n",
            "      - 200\n",
            "      - 200\n",
            "      - 56\n",
            "      - 39\n",
            "      - 65\n",
            "      - 38\n",
            "      - 40\n",
            "      - 42\n",
            "      - 191\n",
            "      - 85\n",
            "      - 183\n",
            "      - 168\n",
            "      - 57\n",
            "      - 161\n",
            "      - 54\n",
            "      - 81\n",
            "      - 121\n",
            "      - 149\n",
            "      - 189\n",
            "      - 60\n",
            "      - 53\n",
            "      - 65\n",
            "      - 162\n",
            "      - 42\n",
            "      - 151\n",
            "      - 145\n",
            "      - 93\n",
            "      - 146\n",
            "      - 181\n",
            "      - 149\n",
            "      - 80\n",
            "      - 200\n",
            "      - 55\n",
            "      - 116\n",
            "      - 108\n",
            "      - 113\n",
            "      - 200\n",
            "      - 163\n",
            "      - 179\n",
            "      - 97\n",
            "      - 98\n",
            "      - 53\n",
            "      - 112\n",
            "      - 200\n",
            "      - 193\n",
            "      - 154\n",
            "      - 38\n",
            "      - 200\n",
            "      - 147\n",
            "      - 177\n",
            "      - 82\n",
            "      - 200\n",
            "      - 139\n",
            "      - 21\n",
            "      - 67\n",
            "      - 55\n",
            "      - 139\n",
            "      - 135\n",
            "      - 174\n",
            "      - 82\n",
            "      - 200\n",
            "      - 28\n",
            "      - 139\n",
            "      - 30\n",
            "      - 165\n",
            "      - 161\n",
            "      - 143\n",
            "      - 115\n",
            "      - 200\n",
            "      - 54\n",
            "      - 200\n",
            "      - 122\n",
            "      - 141\n",
            "      - 148\n",
            "      - 63\n",
            "      - 142\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 35\n",
            "      - 141\n",
            "      - 55\n",
            "      - 41\n",
            "      - 31\n",
            "      - 151\n",
            "      episode_reward:\n",
            "      - 23.0\n",
            "      - 164.0\n",
            "      - 41.0\n",
            "      - 46.0\n",
            "      - 160.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 187.0\n",
            "      - 48.0\n",
            "      - 39.0\n",
            "      - 35.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 39.0\n",
            "      - 65.0\n",
            "      - 38.0\n",
            "      - 40.0\n",
            "      - 42.0\n",
            "      - 191.0\n",
            "      - 85.0\n",
            "      - 183.0\n",
            "      - 168.0\n",
            "      - 57.0\n",
            "      - 161.0\n",
            "      - 54.0\n",
            "      - 81.0\n",
            "      - 121.0\n",
            "      - 149.0\n",
            "      - 189.0\n",
            "      - 60.0\n",
            "      - 53.0\n",
            "      - 65.0\n",
            "      - 162.0\n",
            "      - 42.0\n",
            "      - 151.0\n",
            "      - 145.0\n",
            "      - 93.0\n",
            "      - 146.0\n",
            "      - 181.0\n",
            "      - 149.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 55.0\n",
            "      - 116.0\n",
            "      - 108.0\n",
            "      - 113.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 179.0\n",
            "      - 97.0\n",
            "      - 98.0\n",
            "      - 53.0\n",
            "      - 112.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 154.0\n",
            "      - 38.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 177.0\n",
            "      - 82.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 21.0\n",
            "      - 67.0\n",
            "      - 55.0\n",
            "      - 139.0\n",
            "      - 135.0\n",
            "      - 174.0\n",
            "      - 82.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 139.0\n",
            "      - 30.0\n",
            "      - 165.0\n",
            "      - 161.0\n",
            "      - 143.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 54.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 141.0\n",
            "      - 148.0\n",
            "      - 63.0\n",
            "      - 142.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 35.0\n",
            "      - 141.0\n",
            "      - 55.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 151.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13813661511077874\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10784299506191149\n",
            "      mean_inference_ms: 1.3086217717783353\n",
            "      mean_raw_obs_processing_ms: 0.2464619936449884\n",
            "  time_since_restore: 796.9445962905884\n",
            "  time_this_iter_s: 10.534666538238525\n",
            "  time_total_s: 796.9445962905884\n",
            "  timers:\n",
            "    learn_throughput: 31043.853\n",
            "    learn_time_ms: 6.442\n",
            "    load_throughput: 995680.475\n",
            "    load_time_ms: 0.201\n",
            "    training_iteration_time_ms: 210.995\n",
            "    update_time_ms: 3.627\n",
            "  timestamp: 1656954669\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 380600\n",
            "  training_iteration: 74\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:14 (running for 00:13:44.37)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         796.945</td><td style=\"text-align: right;\">380600</td><td style=\"text-align: right;\">  118.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            118.12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:20 (running for 00:13:49.51)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         796.945</td><td style=\"text-align: right;\">380600</td><td style=\"text-align: right;\">  118.12</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            118.12</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 390200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 390200\n",
            "    num_agent_steps_trained: 390200\n",
            "    num_env_steps_sampled: 390200\n",
            "    num_env_steps_trained: 390200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-11-20\n",
            "  done: false\n",
            "  episode_len_mean: 125.05\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 125.05\n",
            "  episode_reward_min: 21.0\n",
            "  episodes_this_iter: 75\n",
            "  episodes_total: 4295\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 120.20454406738281\n",
            "          policy_loss: 300.4010925292969\n",
            "          var_gnorm: 27.520225524902344\n",
            "          vf_explained_var: 0.12130194902420044\n",
            "          vf_loss: 11099.90625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 390200\n",
            "    num_agent_steps_trained: 390200\n",
            "    num_env_steps_sampled: 390200\n",
            "    num_env_steps_trained: 390200\n",
            "  iterations_since_restore: 75\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 390200\n",
            "  num_agent_steps_trained: 390200\n",
            "  num_env_steps_sampled: 390200\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 390200\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.37142857142858\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13807606341592563\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10789013198773256\n",
            "    mean_inference_ms: 1.3077198345873324\n",
            "    mean_raw_obs_processing_ms: 0.24635142847377595\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 125.05\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 125.05\n",
            "    episode_reward_min: 21.0\n",
            "    episodes_this_iter: 75\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 28\n",
            "      - 139\n",
            "      - 30\n",
            "      - 165\n",
            "      - 161\n",
            "      - 143\n",
            "      - 115\n",
            "      - 200\n",
            "      - 54\n",
            "      - 200\n",
            "      - 122\n",
            "      - 141\n",
            "      - 148\n",
            "      - 63\n",
            "      - 142\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 35\n",
            "      - 141\n",
            "      - 55\n",
            "      - 41\n",
            "      - 31\n",
            "      - 151\n",
            "      - 153\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 163\n",
            "      - 200\n",
            "      - 174\n",
            "      - 57\n",
            "      - 21\n",
            "      - 200\n",
            "      - 200\n",
            "      - 178\n",
            "      - 46\n",
            "      - 24\n",
            "      - 41\n",
            "      - 200\n",
            "      - 88\n",
            "      - 90\n",
            "      - 200\n",
            "      - 171\n",
            "      - 58\n",
            "      - 87\n",
            "      - 178\n",
            "      - 157\n",
            "      - 150\n",
            "      - 56\n",
            "      - 200\n",
            "      - 150\n",
            "      - 30\n",
            "      - 102\n",
            "      - 95\n",
            "      - 54\n",
            "      - 100\n",
            "      - 28\n",
            "      - 88\n",
            "      - 79\n",
            "      - 100\n",
            "      - 86\n",
            "      - 59\n",
            "      - 136\n",
            "      - 99\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 200\n",
            "      - 66\n",
            "      - 32\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 101\n",
            "      - 81\n",
            "      - 176\n",
            "      - 28\n",
            "      - 34\n",
            "      - 156\n",
            "      - 177\n",
            "      - 178\n",
            "      - 107\n",
            "      - 200\n",
            "      - 181\n",
            "      - 169\n",
            "      - 41\n",
            "      - 39\n",
            "      - 88\n",
            "      - 75\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 32\n",
            "      - 72\n",
            "      - 159\n",
            "      - 126\n",
            "      - 164\n",
            "      - 197\n",
            "      episode_reward:\n",
            "      - 28.0\n",
            "      - 139.0\n",
            "      - 30.0\n",
            "      - 165.0\n",
            "      - 161.0\n",
            "      - 143.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 54.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 141.0\n",
            "      - 148.0\n",
            "      - 63.0\n",
            "      - 142.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 35.0\n",
            "      - 141.0\n",
            "      - 55.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 151.0\n",
            "      - 153.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 174.0\n",
            "      - 57.0\n",
            "      - 21.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 178.0\n",
            "      - 46.0\n",
            "      - 24.0\n",
            "      - 41.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 90.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 58.0\n",
            "      - 87.0\n",
            "      - 178.0\n",
            "      - 157.0\n",
            "      - 150.0\n",
            "      - 56.0\n",
            "      - 200.0\n",
            "      - 150.0\n",
            "      - 30.0\n",
            "      - 102.0\n",
            "      - 95.0\n",
            "      - 54.0\n",
            "      - 100.0\n",
            "      - 28.0\n",
            "      - 88.0\n",
            "      - 79.0\n",
            "      - 100.0\n",
            "      - 86.0\n",
            "      - 59.0\n",
            "      - 136.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 32.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 101.0\n",
            "      - 81.0\n",
            "      - 176.0\n",
            "      - 28.0\n",
            "      - 34.0\n",
            "      - 156.0\n",
            "      - 177.0\n",
            "      - 178.0\n",
            "      - 107.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 169.0\n",
            "      - 41.0\n",
            "      - 39.0\n",
            "      - 88.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 32.0\n",
            "      - 72.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 164.0\n",
            "      - 197.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13807606341592563\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10789013198773256\n",
            "      mean_inference_ms: 1.3077198345873324\n",
            "      mean_raw_obs_processing_ms: 0.24635142847377595\n",
            "  time_since_restore: 807.1790211200714\n",
            "  time_this_iter_s: 10.234424829483032\n",
            "  time_total_s: 807.1790211200714\n",
            "  timers:\n",
            "    learn_throughput: 33857.251\n",
            "    learn_time_ms: 5.907\n",
            "    load_throughput: 1080866.899\n",
            "    load_time_ms: 0.185\n",
            "    training_iteration_time_ms: 215.647\n",
            "    update_time_ms: 3.547\n",
            "  timestamp: 1656954680\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 390200\n",
            "  training_iteration: 75\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:25 (running for 00:13:54.64)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         807.179</td><td style=\"text-align: right;\">390200</td><td style=\"text-align: right;\">  125.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            125.05</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:30 (running for 00:13:59.74)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         807.179</td><td style=\"text-align: right;\">390200</td><td style=\"text-align: right;\">  125.05</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            125.05</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 390800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 390800\n",
            "    num_agent_steps_trained: 390800\n",
            "    num_env_steps_sampled: 390800\n",
            "    num_env_steps_trained: 390800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-11-31\n",
            "  done: false\n",
            "  episode_len_mean: 124.85\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 124.85\n",
            "  episode_reward_min: 21.0\n",
            "  episodes_this_iter: 6\n",
            "  episodes_total: 4301\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 122.35\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 122.35\n",
            "    episode_reward_min: 13.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 179\n",
            "      - 122\n",
            "      - 52\n",
            "      - 93\n",
            "      - 200\n",
            "      - 200\n",
            "      - 104\n",
            "      - 113\n",
            "      - 193\n",
            "      - 71\n",
            "      - 161\n",
            "      - 13\n",
            "      - 177\n",
            "      - 200\n",
            "      - 198\n",
            "      - 19\n",
            "      - 34\n",
            "      - 57\n",
            "      - 101\n",
            "      - 160\n",
            "      episode_reward:\n",
            "      - 179.0\n",
            "      - 122.0\n",
            "      - 52.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 104.0\n",
            "      - 113.0\n",
            "      - 193.0\n",
            "      - 71.0\n",
            "      - 161.0\n",
            "      - 13.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 19.0\n",
            "      - 34.0\n",
            "      - 57.0\n",
            "      - 101.0\n",
            "      - 160.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10037099574755917\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07667370443378062\n",
            "      mean_inference_ms: 0.9773176463723414\n",
            "      mean_raw_obs_processing_ms: 0.10708155229267775\n",
            "    timesteps_this_iter: 2447\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 119.42053985595703\n",
            "          policy_loss: -125.04656982421875\n",
            "          var_gnorm: 27.522695541381836\n",
            "          vf_explained_var: 0.03601038455963135\n",
            "          vf_loss: 20188.814453125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 390800\n",
            "    num_agent_steps_trained: 390800\n",
            "    num_env_steps_sampled: 390800\n",
            "    num_env_steps_trained: 390800\n",
            "  iterations_since_restore: 76\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 390800\n",
            "  num_agent_steps_trained: 390800\n",
            "  num_env_steps_sampled: 390800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 390800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 76.52352941176471\n",
            "    ram_util_percent: 21.735294117647058\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1380811909472555\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10788156640167972\n",
            "    mean_inference_ms: 1.30778435554922\n",
            "    mean_raw_obs_processing_ms: 0.24632946608841547\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 124.85\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 124.85\n",
            "    episode_reward_min: 21.0\n",
            "    episodes_this_iter: 6\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 115\n",
            "      - 200\n",
            "      - 54\n",
            "      - 200\n",
            "      - 122\n",
            "      - 141\n",
            "      - 148\n",
            "      - 63\n",
            "      - 142\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 175\n",
            "      - 35\n",
            "      - 141\n",
            "      - 55\n",
            "      - 41\n",
            "      - 31\n",
            "      - 151\n",
            "      - 153\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 163\n",
            "      - 200\n",
            "      - 174\n",
            "      - 57\n",
            "      - 21\n",
            "      - 200\n",
            "      - 200\n",
            "      - 178\n",
            "      - 46\n",
            "      - 24\n",
            "      - 41\n",
            "      - 200\n",
            "      - 88\n",
            "      - 90\n",
            "      - 200\n",
            "      - 171\n",
            "      - 58\n",
            "      - 87\n",
            "      - 178\n",
            "      - 157\n",
            "      - 150\n",
            "      - 56\n",
            "      - 200\n",
            "      - 150\n",
            "      - 30\n",
            "      - 102\n",
            "      - 95\n",
            "      - 54\n",
            "      - 100\n",
            "      - 28\n",
            "      - 88\n",
            "      - 79\n",
            "      - 100\n",
            "      - 86\n",
            "      - 59\n",
            "      - 136\n",
            "      - 99\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 200\n",
            "      - 66\n",
            "      - 32\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 101\n",
            "      - 81\n",
            "      - 176\n",
            "      - 28\n",
            "      - 34\n",
            "      - 156\n",
            "      - 177\n",
            "      - 178\n",
            "      - 107\n",
            "      - 200\n",
            "      - 181\n",
            "      - 169\n",
            "      - 41\n",
            "      - 39\n",
            "      - 88\n",
            "      - 75\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 32\n",
            "      - 72\n",
            "      - 159\n",
            "      - 126\n",
            "      - 164\n",
            "      - 197\n",
            "      - 200\n",
            "      - 169\n",
            "      - 154\n",
            "      - 59\n",
            "      - 29\n",
            "      - 35\n",
            "      episode_reward:\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 54.0\n",
            "      - 200.0\n",
            "      - 122.0\n",
            "      - 141.0\n",
            "      - 148.0\n",
            "      - 63.0\n",
            "      - 142.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 175.0\n",
            "      - 35.0\n",
            "      - 141.0\n",
            "      - 55.0\n",
            "      - 41.0\n",
            "      - 31.0\n",
            "      - 151.0\n",
            "      - 153.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 174.0\n",
            "      - 57.0\n",
            "      - 21.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 178.0\n",
            "      - 46.0\n",
            "      - 24.0\n",
            "      - 41.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 90.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 58.0\n",
            "      - 87.0\n",
            "      - 178.0\n",
            "      - 157.0\n",
            "      - 150.0\n",
            "      - 56.0\n",
            "      - 200.0\n",
            "      - 150.0\n",
            "      - 30.0\n",
            "      - 102.0\n",
            "      - 95.0\n",
            "      - 54.0\n",
            "      - 100.0\n",
            "      - 28.0\n",
            "      - 88.0\n",
            "      - 79.0\n",
            "      - 100.0\n",
            "      - 86.0\n",
            "      - 59.0\n",
            "      - 136.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 32.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 101.0\n",
            "      - 81.0\n",
            "      - 176.0\n",
            "      - 28.0\n",
            "      - 34.0\n",
            "      - 156.0\n",
            "      - 177.0\n",
            "      - 178.0\n",
            "      - 107.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 169.0\n",
            "      - 41.0\n",
            "      - 39.0\n",
            "      - 88.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 32.0\n",
            "      - 72.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 164.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 154.0\n",
            "      - 59.0\n",
            "      - 29.0\n",
            "      - 35.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1380811909472555\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10788156640167972\n",
            "      mean_inference_ms: 1.30778435554922\n",
            "      mean_raw_obs_processing_ms: 0.24632946608841547\n",
            "  time_since_restore: 818.5827434062958\n",
            "  time_this_iter_s: 11.403722286224365\n",
            "  time_total_s: 818.5827434062958\n",
            "  timers:\n",
            "    learn_throughput: 27573.878\n",
            "    learn_time_ms: 7.253\n",
            "    load_throughput: 1012138.996\n",
            "    load_time_ms: 0.198\n",
            "    training_iteration_time_ms: 232.205\n",
            "    update_time_ms: 4.18\n",
            "  timestamp: 1656954691\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 390800\n",
            "  training_iteration: 76\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:36 (running for 00:14:06.07)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         818.583</td><td style=\"text-align: right;\">390800</td><td style=\"text-align: right;\">  124.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            124.85</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:41 (running for 00:14:11.14)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         818.583</td><td style=\"text-align: right;\">390800</td><td style=\"text-align: right;\">  124.85</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">            124.85</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 400400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 400400\n",
            "    num_agent_steps_trained: 400400\n",
            "    num_env_steps_sampled: 400400\n",
            "    num_env_steps_trained: 400400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-11-41\n",
            "  done: false\n",
            "  episode_len_mean: 136.09\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 136.09\n",
            "  episode_reward_min: 19.0\n",
            "  episodes_this_iter: 67\n",
            "  episodes_total: 4368\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 122.95112609863281\n",
            "          policy_loss: 624.5732421875\n",
            "          var_gnorm: 27.594661712646484\n",
            "          vf_explained_var: 0.10036522150039673\n",
            "          vf_loss: 3988.0263671875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 400400\n",
            "    num_agent_steps_trained: 400400\n",
            "    num_env_steps_sampled: 400400\n",
            "    num_env_steps_trained: 400400\n",
            "  iterations_since_restore: 77\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 400400\n",
            "  num_agent_steps_trained: 400400\n",
            "  num_env_steps_sampled: 400400\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 400400\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.75000000000001\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1379636565539581\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10797177591568972\n",
            "    mean_inference_ms: 1.3067299978124276\n",
            "    mean_raw_obs_processing_ms: 0.24625108668071724\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 136.09\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 136.09\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 67\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 158\n",
            "      - 101\n",
            "      - 81\n",
            "      - 176\n",
            "      - 28\n",
            "      - 34\n",
            "      - 156\n",
            "      - 177\n",
            "      - 178\n",
            "      - 107\n",
            "      - 200\n",
            "      - 181\n",
            "      - 169\n",
            "      - 41\n",
            "      - 39\n",
            "      - 88\n",
            "      - 75\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 32\n",
            "      - 72\n",
            "      - 159\n",
            "      - 126\n",
            "      - 164\n",
            "      - 197\n",
            "      - 200\n",
            "      - 169\n",
            "      - 154\n",
            "      - 59\n",
            "      - 29\n",
            "      - 35\n",
            "      - 158\n",
            "      - 83\n",
            "      - 178\n",
            "      - 200\n",
            "      - 64\n",
            "      - 179\n",
            "      - 193\n",
            "      - 166\n",
            "      - 27\n",
            "      - 200\n",
            "      - 170\n",
            "      - 173\n",
            "      - 139\n",
            "      - 200\n",
            "      - 19\n",
            "      - 187\n",
            "      - 197\n",
            "      - 101\n",
            "      - 190\n",
            "      - 95\n",
            "      - 151\n",
            "      - 166\n",
            "      - 39\n",
            "      - 109\n",
            "      - 200\n",
            "      - 171\n",
            "      - 99\n",
            "      - 200\n",
            "      - 200\n",
            "      - 109\n",
            "      - 159\n",
            "      - 91\n",
            "      - 91\n",
            "      - 52\n",
            "      - 179\n",
            "      - 171\n",
            "      - 200\n",
            "      - 125\n",
            "      - 121\n",
            "      - 20\n",
            "      - 133\n",
            "      - 31\n",
            "      - 172\n",
            "      - 200\n",
            "      - 20\n",
            "      - 200\n",
            "      - 200\n",
            "      - 86\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 140\n",
            "      - 71\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 36\n",
            "      - 162\n",
            "      - 180\n",
            "      - 157\n",
            "      - 151\n",
            "      - 200\n",
            "      - 43\n",
            "      - 63\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 101.0\n",
            "      - 81.0\n",
            "      - 176.0\n",
            "      - 28.0\n",
            "      - 34.0\n",
            "      - 156.0\n",
            "      - 177.0\n",
            "      - 178.0\n",
            "      - 107.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 169.0\n",
            "      - 41.0\n",
            "      - 39.0\n",
            "      - 88.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 32.0\n",
            "      - 72.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 164.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 154.0\n",
            "      - 59.0\n",
            "      - 29.0\n",
            "      - 35.0\n",
            "      - 158.0\n",
            "      - 83.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 64.0\n",
            "      - 179.0\n",
            "      - 193.0\n",
            "      - 166.0\n",
            "      - 27.0\n",
            "      - 200.0\n",
            "      - 170.0\n",
            "      - 173.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 19.0\n",
            "      - 187.0\n",
            "      - 197.0\n",
            "      - 101.0\n",
            "      - 190.0\n",
            "      - 95.0\n",
            "      - 151.0\n",
            "      - 166.0\n",
            "      - 39.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 159.0\n",
            "      - 91.0\n",
            "      - 91.0\n",
            "      - 52.0\n",
            "      - 179.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 121.0\n",
            "      - 20.0\n",
            "      - 133.0\n",
            "      - 31.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 20.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 162.0\n",
            "      - 180.0\n",
            "      - 157.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 63.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1379636565539581\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10797177591568972\n",
            "      mean_inference_ms: 1.3067299978124276\n",
            "      mean_raw_obs_processing_ms: 0.24625108668071724\n",
            "  time_since_restore: 828.6160995960236\n",
            "  time_this_iter_s: 10.033356189727783\n",
            "  time_total_s: 828.6160995960236\n",
            "  timers:\n",
            "    learn_throughput: 29329.977\n",
            "    learn_time_ms: 6.819\n",
            "    load_throughput: 1003662.12\n",
            "    load_time_ms: 0.199\n",
            "    training_iteration_time_ms: 208.485\n",
            "    update_time_ms: 4.88\n",
            "  timestamp: 1656954701\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 400400\n",
            "  training_iteration: 77\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:46 (running for 00:14:16.18)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         828.616</td><td style=\"text-align: right;\">400400</td><td style=\"text-align: right;\">  136.09</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            136.09</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:51 (running for 00:14:21.28)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         828.616</td><td style=\"text-align: right;\">400400</td><td style=\"text-align: right;\">  136.09</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            136.09</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 401000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 401000\n",
            "    num_agent_steps_trained: 401000\n",
            "    num_env_steps_sampled: 401000\n",
            "    num_env_steps_trained: 401000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-11-52\n",
            "  done: false\n",
            "  episode_len_mean: 138.26\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 138.26\n",
            "  episode_reward_min: 19.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 4372\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 115.55\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 115.55\n",
            "    episode_reward_min: 22.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 195\n",
            "      - 39\n",
            "      - 165\n",
            "      - 188\n",
            "      - 156\n",
            "      - 51\n",
            "      - 173\n",
            "      - 28\n",
            "      - 28\n",
            "      - 22\n",
            "      - 149\n",
            "      - 173\n",
            "      - 133\n",
            "      - 34\n",
            "      - 200\n",
            "      - 38\n",
            "      - 157\n",
            "      - 197\n",
            "      - 147\n",
            "      - 38\n",
            "      episode_reward:\n",
            "      - 195.0\n",
            "      - 39.0\n",
            "      - 165.0\n",
            "      - 188.0\n",
            "      - 156.0\n",
            "      - 51.0\n",
            "      - 173.0\n",
            "      - 28.0\n",
            "      - 28.0\n",
            "      - 22.0\n",
            "      - 149.0\n",
            "      - 173.0\n",
            "      - 133.0\n",
            "      - 34.0\n",
            "      - 200.0\n",
            "      - 38.0\n",
            "      - 157.0\n",
            "      - 197.0\n",
            "      - 147.0\n",
            "      - 38.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10036117053814879\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07665924839450608\n",
            "      mean_inference_ms: 0.9771970585508309\n",
            "      mean_raw_obs_processing_ms: 0.10703756420746316\n",
            "    timesteps_this_iter: 2311\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 125.33964538574219\n",
            "          policy_loss: 136.15655517578125\n",
            "          var_gnorm: 27.598234176635742\n",
            "          vf_explained_var: 0.5913078784942627\n",
            "          vf_loss: 8713.853515625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 401000\n",
            "    num_agent_steps_trained: 401000\n",
            "    num_env_steps_sampled: 401000\n",
            "    num_env_steps_trained: 401000\n",
            "  iterations_since_restore: 78\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 401000\n",
            "  num_agent_steps_trained: 401000\n",
            "  num_env_steps_sampled: 401000\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 401000\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.60666666666665\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13797977442772008\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10794917375041818\n",
            "    mean_inference_ms: 1.3068163838260467\n",
            "    mean_raw_obs_processing_ms: 0.24622146025679625\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 138.26\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 138.26\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 176\n",
            "      - 28\n",
            "      - 34\n",
            "      - 156\n",
            "      - 177\n",
            "      - 178\n",
            "      - 107\n",
            "      - 200\n",
            "      - 181\n",
            "      - 169\n",
            "      - 41\n",
            "      - 39\n",
            "      - 88\n",
            "      - 75\n",
            "      - 200\n",
            "      - 200\n",
            "      - 192\n",
            "      - 32\n",
            "      - 72\n",
            "      - 159\n",
            "      - 126\n",
            "      - 164\n",
            "      - 197\n",
            "      - 200\n",
            "      - 169\n",
            "      - 154\n",
            "      - 59\n",
            "      - 29\n",
            "      - 35\n",
            "      - 158\n",
            "      - 83\n",
            "      - 178\n",
            "      - 200\n",
            "      - 64\n",
            "      - 179\n",
            "      - 193\n",
            "      - 166\n",
            "      - 27\n",
            "      - 200\n",
            "      - 170\n",
            "      - 173\n",
            "      - 139\n",
            "      - 200\n",
            "      - 19\n",
            "      - 187\n",
            "      - 197\n",
            "      - 101\n",
            "      - 190\n",
            "      - 95\n",
            "      - 151\n",
            "      - 166\n",
            "      - 39\n",
            "      - 109\n",
            "      - 200\n",
            "      - 171\n",
            "      - 99\n",
            "      - 200\n",
            "      - 200\n",
            "      - 109\n",
            "      - 159\n",
            "      - 91\n",
            "      - 91\n",
            "      - 52\n",
            "      - 179\n",
            "      - 171\n",
            "      - 200\n",
            "      - 125\n",
            "      - 121\n",
            "      - 20\n",
            "      - 133\n",
            "      - 31\n",
            "      - 172\n",
            "      - 200\n",
            "      - 20\n",
            "      - 200\n",
            "      - 200\n",
            "      - 86\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 140\n",
            "      - 71\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 36\n",
            "      - 162\n",
            "      - 180\n",
            "      - 157\n",
            "      - 151\n",
            "      - 200\n",
            "      - 43\n",
            "      - 63\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      episode_reward:\n",
            "      - 176.0\n",
            "      - 28.0\n",
            "      - 34.0\n",
            "      - 156.0\n",
            "      - 177.0\n",
            "      - 178.0\n",
            "      - 107.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 169.0\n",
            "      - 41.0\n",
            "      - 39.0\n",
            "      - 88.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 192.0\n",
            "      - 32.0\n",
            "      - 72.0\n",
            "      - 159.0\n",
            "      - 126.0\n",
            "      - 164.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 169.0\n",
            "      - 154.0\n",
            "      - 59.0\n",
            "      - 29.0\n",
            "      - 35.0\n",
            "      - 158.0\n",
            "      - 83.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 64.0\n",
            "      - 179.0\n",
            "      - 193.0\n",
            "      - 166.0\n",
            "      - 27.0\n",
            "      - 200.0\n",
            "      - 170.0\n",
            "      - 173.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 19.0\n",
            "      - 187.0\n",
            "      - 197.0\n",
            "      - 101.0\n",
            "      - 190.0\n",
            "      - 95.0\n",
            "      - 151.0\n",
            "      - 166.0\n",
            "      - 39.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 159.0\n",
            "      - 91.0\n",
            "      - 91.0\n",
            "      - 52.0\n",
            "      - 179.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 121.0\n",
            "      - 20.0\n",
            "      - 133.0\n",
            "      - 31.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 20.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 162.0\n",
            "      - 180.0\n",
            "      - 157.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13797977442772008\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10794917375041818\n",
            "      mean_inference_ms: 1.3068163838260467\n",
            "      mean_raw_obs_processing_ms: 0.24622146025679625\n",
            "  time_since_restore: 838.9544608592987\n",
            "  time_this_iter_s: 10.338361263275146\n",
            "  time_total_s: 838.9544608592987\n",
            "  timers:\n",
            "    learn_throughput: 28235.155\n",
            "    learn_time_ms: 7.083\n",
            "    load_throughput: 962658.71\n",
            "    load_time_ms: 0.208\n",
            "    training_iteration_time_ms: 207.408\n",
            "    update_time_ms: 4.504\n",
            "  timestamp: 1656954712\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 401000\n",
            "  training_iteration: 78\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:11:57 (running for 00:14:26.56)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         838.954</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">  138.26</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            138.26</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:02 (running for 00:14:31.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         838.954</td><td style=\"text-align: right;\">401000</td><td style=\"text-align: right;\">  138.26</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            138.26</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 410600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 410600\n",
            "    num_agent_steps_trained: 410600\n",
            "    num_env_steps_sampled: 410600\n",
            "    num_env_steps_trained: 410600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-12-02\n",
            "  done: false\n",
            "  episode_len_mean: 139.73\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 139.73\n",
            "  episode_reward_min: 20.0\n",
            "  episodes_this_iter: 71\n",
            "  episodes_total: 4443\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 119.87767028808594\n",
            "          policy_loss: 347.0005187988281\n",
            "          var_gnorm: 27.665081024169922\n",
            "          vf_explained_var: 0.38640934228897095\n",
            "          vf_loss: 9134.0830078125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 410600\n",
            "    num_agent_steps_trained: 410600\n",
            "    num_env_steps_sampled: 410600\n",
            "    num_env_steps_trained: 410600\n",
            "  iterations_since_restore: 79\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 410600\n",
            "  num_agent_steps_trained: 410600\n",
            "  num_env_steps_sampled: 410600\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 410600\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.88571428571429\n",
            "    ram_util_percent: 21.757142857142856\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13795996680886977\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10789547459694035\n",
            "    mean_inference_ms: 1.3060771214849365\n",
            "    mean_raw_obs_processing_ms: 0.2460500942609326\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 139.73\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 139.73\n",
            "    episode_reward_min: 20.0\n",
            "    episodes_this_iter: 71\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 172\n",
            "      - 200\n",
            "      - 20\n",
            "      - 200\n",
            "      - 200\n",
            "      - 86\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 140\n",
            "      - 71\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 36\n",
            "      - 162\n",
            "      - 180\n",
            "      - 157\n",
            "      - 151\n",
            "      - 200\n",
            "      - 43\n",
            "      - 63\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      - 170\n",
            "      - 55\n",
            "      - 95\n",
            "      - 107\n",
            "      - 167\n",
            "      - 200\n",
            "      - 173\n",
            "      - 162\n",
            "      - 68\n",
            "      - 142\n",
            "      - 180\n",
            "      - 170\n",
            "      - 181\n",
            "      - 27\n",
            "      - 200\n",
            "      - 143\n",
            "      - 85\n",
            "      - 200\n",
            "      - 113\n",
            "      - 171\n",
            "      - 200\n",
            "      - 48\n",
            "      - 23\n",
            "      - 64\n",
            "      - 97\n",
            "      - 36\n",
            "      - 120\n",
            "      - 140\n",
            "      - 169\n",
            "      - 97\n",
            "      - 200\n",
            "      - 46\n",
            "      - 124\n",
            "      - 52\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 124\n",
            "      - 167\n",
            "      - 31\n",
            "      - 50\n",
            "      - 58\n",
            "      - 151\n",
            "      - 166\n",
            "      - 96\n",
            "      - 151\n",
            "      - 110\n",
            "      - 167\n",
            "      - 200\n",
            "      - 200\n",
            "      - 25\n",
            "      - 188\n",
            "      - 200\n",
            "      - 181\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 162\n",
            "      - 173\n",
            "      - 156\n",
            "      - 180\n",
            "      - 26\n",
            "      - 200\n",
            "      - 200\n",
            "      - 52\n",
            "      - 115\n",
            "      - 117\n",
            "      - 52\n",
            "      - 147\n",
            "      - 164\n",
            "      - 140\n",
            "      episode_reward:\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 20.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 162.0\n",
            "      - 180.0\n",
            "      - 157.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 170.0\n",
            "      - 55.0\n",
            "      - 95.0\n",
            "      - 107.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 162.0\n",
            "      - 68.0\n",
            "      - 142.0\n",
            "      - 180.0\n",
            "      - 170.0\n",
            "      - 181.0\n",
            "      - 27.0\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 85.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 48.0\n",
            "      - 23.0\n",
            "      - 64.0\n",
            "      - 97.0\n",
            "      - 36.0\n",
            "      - 120.0\n",
            "      - 140.0\n",
            "      - 169.0\n",
            "      - 97.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 124.0\n",
            "      - 52.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 167.0\n",
            "      - 31.0\n",
            "      - 50.0\n",
            "      - 58.0\n",
            "      - 151.0\n",
            "      - 166.0\n",
            "      - 96.0\n",
            "      - 151.0\n",
            "      - 110.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 173.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 26.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 115.0\n",
            "      - 117.0\n",
            "      - 52.0\n",
            "      - 147.0\n",
            "      - 164.0\n",
            "      - 140.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13795996680886977\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10789547459694035\n",
            "      mean_inference_ms: 1.3060771214849365\n",
            "      mean_raw_obs_processing_ms: 0.2460500942609326\n",
            "  time_since_restore: 849.0728094577789\n",
            "  time_this_iter_s: 10.118348598480225\n",
            "  time_total_s: 849.0728094577789\n",
            "  timers:\n",
            "    learn_throughput: 29472.595\n",
            "    learn_time_ms: 6.786\n",
            "    load_throughput: 766222.872\n",
            "    load_time_ms: 0.261\n",
            "    training_iteration_time_ms: 218.153\n",
            "    update_time_ms: 4.232\n",
            "  timestamp: 1656954722\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 410600\n",
            "  training_iteration: 79\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:07 (running for 00:14:37.30)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         849.073</td><td style=\"text-align: right;\">410600</td><td style=\"text-align: right;\">  139.73</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">            139.73</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:12 (running for 00:14:42.39)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         849.073</td><td style=\"text-align: right;\">410600</td><td style=\"text-align: right;\">  139.73</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">            139.73</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 411200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 411200\n",
            "    num_agent_steps_trained: 411200\n",
            "    num_env_steps_sampled: 411200\n",
            "    num_env_steps_trained: 411200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-12-14\n",
            "  done: false\n",
            "  episode_len_mean: 141.52\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 141.52\n",
            "  episode_reward_min: 23.0\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 4446\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 139.5\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 139.5\n",
            "    episode_reward_min: 23.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 126\n",
            "      - 114\n",
            "      - 82\n",
            "      - 200\n",
            "      - 33\n",
            "      - 179\n",
            "      - 178\n",
            "      - 102\n",
            "      - 164\n",
            "      - 49\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 23\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 140\n",
            "      - 42\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 114.0\n",
            "      - 82.0\n",
            "      - 200.0\n",
            "      - 33.0\n",
            "      - 179.0\n",
            "      - 178.0\n",
            "      - 102.0\n",
            "      - 164.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 23.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 140.0\n",
            "      - 42.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10033469042325979\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07664505313020062\n",
            "      mean_inference_ms: 0.9770382523625806\n",
            "      mean_raw_obs_processing_ms: 0.1086290435716513\n",
            "    timesteps_this_iter: 2790\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 124.53880310058594\n",
            "          policy_loss: 565.1888427734375\n",
            "          var_gnorm: 27.668861389160156\n",
            "          vf_explained_var: 0.07441067695617676\n",
            "          vf_loss: 3516.23779296875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 411200\n",
            "    num_agent_steps_trained: 411200\n",
            "    num_env_steps_sampled: 411200\n",
            "    num_env_steps_trained: 411200\n",
            "  iterations_since_restore: 80\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 411200\n",
            "  num_agent_steps_trained: 411200\n",
            "  num_env_steps_sampled: 411200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 411200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 67.5529411764706\n",
            "    ram_util_percent: 21.7\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1379674098015678\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1078824998307314\n",
            "    mean_inference_ms: 1.3061047496817952\n",
            "    mean_raw_obs_processing_ms: 0.24603633342868136\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 141.52\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 141.52\n",
            "    episode_reward_min: 23.0\n",
            "    episodes_this_iter: 3\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 200\n",
            "      - 86\n",
            "      - 180\n",
            "      - 200\n",
            "      - 200\n",
            "      - 182\n",
            "      - 200\n",
            "      - 200\n",
            "      - 140\n",
            "      - 71\n",
            "      - 200\n",
            "      - 53\n",
            "      - 200\n",
            "      - 36\n",
            "      - 162\n",
            "      - 180\n",
            "      - 157\n",
            "      - 151\n",
            "      - 200\n",
            "      - 43\n",
            "      - 63\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      - 170\n",
            "      - 55\n",
            "      - 95\n",
            "      - 107\n",
            "      - 167\n",
            "      - 200\n",
            "      - 173\n",
            "      - 162\n",
            "      - 68\n",
            "      - 142\n",
            "      - 180\n",
            "      - 170\n",
            "      - 181\n",
            "      - 27\n",
            "      - 200\n",
            "      - 143\n",
            "      - 85\n",
            "      - 200\n",
            "      - 113\n",
            "      - 171\n",
            "      - 200\n",
            "      - 48\n",
            "      - 23\n",
            "      - 64\n",
            "      - 97\n",
            "      - 36\n",
            "      - 120\n",
            "      - 140\n",
            "      - 169\n",
            "      - 97\n",
            "      - 200\n",
            "      - 46\n",
            "      - 124\n",
            "      - 52\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 124\n",
            "      - 167\n",
            "      - 31\n",
            "      - 50\n",
            "      - 58\n",
            "      - 151\n",
            "      - 166\n",
            "      - 96\n",
            "      - 151\n",
            "      - 110\n",
            "      - 167\n",
            "      - 200\n",
            "      - 200\n",
            "      - 25\n",
            "      - 188\n",
            "      - 200\n",
            "      - 181\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 162\n",
            "      - 173\n",
            "      - 156\n",
            "      - 180\n",
            "      - 26\n",
            "      - 200\n",
            "      - 200\n",
            "      - 52\n",
            "      - 115\n",
            "      - 117\n",
            "      - 52\n",
            "      - 147\n",
            "      - 164\n",
            "      - 140\n",
            "      - 200\n",
            "      - 200\n",
            "      - 171\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 86.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 53.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 162.0\n",
            "      - 180.0\n",
            "      - 157.0\n",
            "      - 151.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 170.0\n",
            "      - 55.0\n",
            "      - 95.0\n",
            "      - 107.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 162.0\n",
            "      - 68.0\n",
            "      - 142.0\n",
            "      - 180.0\n",
            "      - 170.0\n",
            "      - 181.0\n",
            "      - 27.0\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 85.0\n",
            "      - 200.0\n",
            "      - 113.0\n",
            "      - 171.0\n",
            "      - 200.0\n",
            "      - 48.0\n",
            "      - 23.0\n",
            "      - 64.0\n",
            "      - 97.0\n",
            "      - 36.0\n",
            "      - 120.0\n",
            "      - 140.0\n",
            "      - 169.0\n",
            "      - 97.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 124.0\n",
            "      - 52.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 167.0\n",
            "      - 31.0\n",
            "      - 50.0\n",
            "      - 58.0\n",
            "      - 151.0\n",
            "      - 166.0\n",
            "      - 96.0\n",
            "      - 151.0\n",
            "      - 110.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 173.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 26.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 115.0\n",
            "      - 117.0\n",
            "      - 52.0\n",
            "      - 147.0\n",
            "      - 164.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1379674098015678\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1078824998307314\n",
            "      mean_inference_ms: 1.3061047496817952\n",
            "      mean_raw_obs_processing_ms: 0.24603633342868136\n",
            "  time_since_restore: 861.2388532161713\n",
            "  time_this_iter_s: 12.166043758392334\n",
            "  time_total_s: 861.2388532161713\n",
            "  timers:\n",
            "    learn_throughput: 31265.544\n",
            "    learn_time_ms: 6.397\n",
            "    load_throughput: 839868.642\n",
            "    load_time_ms: 0.238\n",
            "    training_iteration_time_ms: 213.537\n",
            "    update_time_ms: 4.119\n",
            "  timestamp: 1656954734\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 411200\n",
            "  training_iteration: 80\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:19 (running for 00:14:48.93)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         861.239</td><td style=\"text-align: right;\">411200</td><td style=\"text-align: right;\">  141.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            141.52</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:24 (running for 00:14:54.02)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         861.239</td><td style=\"text-align: right;\">411200</td><td style=\"text-align: right;\">  141.52</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  23</td><td style=\"text-align: right;\">            141.52</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 420600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 420600\n",
            "    num_agent_steps_trained: 420600\n",
            "    num_env_steps_sampled: 420600\n",
            "    num_env_steps_trained: 420600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-12-24\n",
            "  done: false\n",
            "  episode_len_mean: 132.43\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 132.43\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 75\n",
            "  episodes_total: 4521\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 126.84980773925781\n",
            "          policy_loss: 308.902587890625\n",
            "          var_gnorm: 27.729310989379883\n",
            "          vf_explained_var: -0.03520023822784424\n",
            "          vf_loss: 10520.34375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 420600\n",
            "    num_agent_steps_trained: 420600\n",
            "    num_env_steps_sampled: 420600\n",
            "    num_env_steps_trained: 420600\n",
            "  iterations_since_restore: 81\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 420600\n",
            "  num_agent_steps_trained: 420600\n",
            "  num_env_steps_sampled: 420600\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 420600\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.78666666666668\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1379649052951799\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10793675901238803\n",
            "    mean_inference_ms: 1.3058312007578499\n",
            "    mean_raw_obs_processing_ms: 0.24604613540355064\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 132.43\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 132.43\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 75\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 25\n",
            "      - 188\n",
            "      - 200\n",
            "      - 181\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 162\n",
            "      - 173\n",
            "      - 156\n",
            "      - 180\n",
            "      - 26\n",
            "      - 200\n",
            "      - 200\n",
            "      - 52\n",
            "      - 115\n",
            "      - 117\n",
            "      - 52\n",
            "      - 147\n",
            "      - 164\n",
            "      - 140\n",
            "      - 200\n",
            "      - 200\n",
            "      - 171\n",
            "      - 154\n",
            "      - 156\n",
            "      - 200\n",
            "      - 58\n",
            "      - 175\n",
            "      - 158\n",
            "      - 147\n",
            "      - 34\n",
            "      - 167\n",
            "      - 26\n",
            "      - 72\n",
            "      - 185\n",
            "      - 39\n",
            "      - 29\n",
            "      - 148\n",
            "      - 187\n",
            "      - 200\n",
            "      - 154\n",
            "      - 117\n",
            "      - 143\n",
            "      - 148\n",
            "      - 33\n",
            "      - 106\n",
            "      - 200\n",
            "      - 80\n",
            "      - 143\n",
            "      - 176\n",
            "      - 200\n",
            "      - 161\n",
            "      - 200\n",
            "      - 28\n",
            "      - 166\n",
            "      - 160\n",
            "      - 100\n",
            "      - 97\n",
            "      - 114\n",
            "      - 178\n",
            "      - 103\n",
            "      - 55\n",
            "      - 183\n",
            "      - 167\n",
            "      - 137\n",
            "      - 30\n",
            "      - 60\n",
            "      - 200\n",
            "      - 34\n",
            "      - 200\n",
            "      - 200\n",
            "      - 18\n",
            "      - 136\n",
            "      - 136\n",
            "      - 124\n",
            "      - 23\n",
            "      - 33\n",
            "      - 141\n",
            "      - 200\n",
            "      - 69\n",
            "      - 158\n",
            "      - 51\n",
            "      - 186\n",
            "      - 126\n",
            "      - 191\n",
            "      - 12\n",
            "      - 124\n",
            "      - 188\n",
            "      - 139\n",
            "      - 156\n",
            "      - 26\n",
            "      - 174\n",
            "      - 29\n",
            "      - 116\n",
            "      - 200\n",
            "      - 84\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 181.0\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 173.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 26.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 115.0\n",
            "      - 117.0\n",
            "      - 52.0\n",
            "      - 147.0\n",
            "      - 164.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 154.0\n",
            "      - 156.0\n",
            "      - 200.0\n",
            "      - 58.0\n",
            "      - 175.0\n",
            "      - 158.0\n",
            "      - 147.0\n",
            "      - 34.0\n",
            "      - 167.0\n",
            "      - 26.0\n",
            "      - 72.0\n",
            "      - 185.0\n",
            "      - 39.0\n",
            "      - 29.0\n",
            "      - 148.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 117.0\n",
            "      - 143.0\n",
            "      - 148.0\n",
            "      - 33.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 80.0\n",
            "      - 143.0\n",
            "      - 176.0\n",
            "      - 200.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 166.0\n",
            "      - 160.0\n",
            "      - 100.0\n",
            "      - 97.0\n",
            "      - 114.0\n",
            "      - 178.0\n",
            "      - 103.0\n",
            "      - 55.0\n",
            "      - 183.0\n",
            "      - 167.0\n",
            "      - 137.0\n",
            "      - 30.0\n",
            "      - 60.0\n",
            "      - 200.0\n",
            "      - 34.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 18.0\n",
            "      - 136.0\n",
            "      - 136.0\n",
            "      - 124.0\n",
            "      - 23.0\n",
            "      - 33.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 158.0\n",
            "      - 51.0\n",
            "      - 186.0\n",
            "      - 126.0\n",
            "      - 191.0\n",
            "      - 12.0\n",
            "      - 124.0\n",
            "      - 188.0\n",
            "      - 139.0\n",
            "      - 156.0\n",
            "      - 26.0\n",
            "      - 174.0\n",
            "      - 29.0\n",
            "      - 116.0\n",
            "      - 200.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1379649052951799\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10793675901238803\n",
            "      mean_inference_ms: 1.3058312007578499\n",
            "      mean_raw_obs_processing_ms: 0.24604613540355064\n",
            "  time_since_restore: 871.4552757740021\n",
            "  time_this_iter_s: 10.21642255783081\n",
            "  time_total_s: 871.4552757740021\n",
            "  timers:\n",
            "    learn_throughput: 31113.053\n",
            "    learn_time_ms: 6.428\n",
            "    load_throughput: 1051731.194\n",
            "    load_time_ms: 0.19\n",
            "    training_iteration_time_ms: 217.484\n",
            "    update_time_ms: 4.005\n",
            "  timestamp: 1656954744\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 420600\n",
            "  training_iteration: 81\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:29 (running for 00:14:59.18)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         871.455</td><td style=\"text-align: right;\">420600</td><td style=\"text-align: right;\">  132.43</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            132.43</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:34 (running for 00:15:04.28)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         871.455</td><td style=\"text-align: right;\">420600</td><td style=\"text-align: right;\">  132.43</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            132.43</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 421200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 421200\n",
            "    num_agent_steps_trained: 421200\n",
            "    num_env_steps_sampled: 421200\n",
            "    num_env_steps_trained: 421200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-12-35\n",
            "  done: false\n",
            "  episode_len_mean: 131.55\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 131.55\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 5\n",
            "  episodes_total: 4526\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 126.4\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 126.4\n",
            "    episode_reward_min: 33.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 172\n",
            "      - 184\n",
            "      - 152\n",
            "      - 76\n",
            "      - 145\n",
            "      - 66\n",
            "      - 63\n",
            "      - 104\n",
            "      - 200\n",
            "      - 40\n",
            "      - 44\n",
            "      - 200\n",
            "      - 148\n",
            "      - 200\n",
            "      - 135\n",
            "      - 191\n",
            "      - 155\n",
            "      - 66\n",
            "      - 154\n",
            "      - 33\n",
            "      episode_reward:\n",
            "      - 172.0\n",
            "      - 184.0\n",
            "      - 152.0\n",
            "      - 76.0\n",
            "      - 145.0\n",
            "      - 66.0\n",
            "      - 63.0\n",
            "      - 104.0\n",
            "      - 200.0\n",
            "      - 40.0\n",
            "      - 44.0\n",
            "      - 200.0\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 135.0\n",
            "      - 191.0\n",
            "      - 155.0\n",
            "      - 66.0\n",
            "      - 154.0\n",
            "      - 33.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10037095423691493\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07665299102015809\n",
            "      mean_inference_ms: 0.9773093069172235\n",
            "      mean_raw_obs_processing_ms: 0.10858705708387002\n",
            "    timesteps_this_iter: 2528\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 117.73307037353516\n",
            "          policy_loss: 272.6463623046875\n",
            "          var_gnorm: 27.73366928100586\n",
            "          vf_explained_var: 0.09288263320922852\n",
            "          vf_loss: 12951.052734375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 421200\n",
            "    num_agent_steps_trained: 421200\n",
            "    num_env_steps_sampled: 421200\n",
            "    num_env_steps_trained: 421200\n",
            "  iterations_since_restore: 82\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 421200\n",
            "  num_agent_steps_trained: 421200\n",
            "  num_env_steps_sampled: 421200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 421200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.53333333333333\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13799446587504957\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10790449884113927\n",
            "    mean_inference_ms: 1.3060051288539294\n",
            "    mean_raw_obs_processing_ms: 0.24601407932979946\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 131.55\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 131.55\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 5\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 146\n",
            "      - 200\n",
            "      - 200\n",
            "      - 162\n",
            "      - 173\n",
            "      - 156\n",
            "      - 180\n",
            "      - 26\n",
            "      - 200\n",
            "      - 200\n",
            "      - 52\n",
            "      - 115\n",
            "      - 117\n",
            "      - 52\n",
            "      - 147\n",
            "      - 164\n",
            "      - 140\n",
            "      - 200\n",
            "      - 200\n",
            "      - 171\n",
            "      - 154\n",
            "      - 156\n",
            "      - 200\n",
            "      - 58\n",
            "      - 175\n",
            "      - 158\n",
            "      - 147\n",
            "      - 34\n",
            "      - 167\n",
            "      - 26\n",
            "      - 72\n",
            "      - 185\n",
            "      - 39\n",
            "      - 29\n",
            "      - 148\n",
            "      - 187\n",
            "      - 200\n",
            "      - 154\n",
            "      - 117\n",
            "      - 143\n",
            "      - 148\n",
            "      - 33\n",
            "      - 106\n",
            "      - 200\n",
            "      - 80\n",
            "      - 143\n",
            "      - 176\n",
            "      - 200\n",
            "      - 161\n",
            "      - 200\n",
            "      - 28\n",
            "      - 166\n",
            "      - 160\n",
            "      - 100\n",
            "      - 97\n",
            "      - 114\n",
            "      - 178\n",
            "      - 103\n",
            "      - 55\n",
            "      - 183\n",
            "      - 167\n",
            "      - 137\n",
            "      - 30\n",
            "      - 60\n",
            "      - 200\n",
            "      - 34\n",
            "      - 200\n",
            "      - 200\n",
            "      - 18\n",
            "      - 136\n",
            "      - 136\n",
            "      - 124\n",
            "      - 23\n",
            "      - 33\n",
            "      - 141\n",
            "      - 200\n",
            "      - 69\n",
            "      - 158\n",
            "      - 51\n",
            "      - 186\n",
            "      - 126\n",
            "      - 191\n",
            "      - 12\n",
            "      - 124\n",
            "      - 188\n",
            "      - 139\n",
            "      - 156\n",
            "      - 26\n",
            "      - 174\n",
            "      - 29\n",
            "      - 116\n",
            "      - 200\n",
            "      - 84\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 115\n",
            "      - 200\n",
            "      - 81\n",
            "      episode_reward:\n",
            "      - 146.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 173.0\n",
            "      - 156.0\n",
            "      - 180.0\n",
            "      - 26.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 115.0\n",
            "      - 117.0\n",
            "      - 52.0\n",
            "      - 147.0\n",
            "      - 164.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 154.0\n",
            "      - 156.0\n",
            "      - 200.0\n",
            "      - 58.0\n",
            "      - 175.0\n",
            "      - 158.0\n",
            "      - 147.0\n",
            "      - 34.0\n",
            "      - 167.0\n",
            "      - 26.0\n",
            "      - 72.0\n",
            "      - 185.0\n",
            "      - 39.0\n",
            "      - 29.0\n",
            "      - 148.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 117.0\n",
            "      - 143.0\n",
            "      - 148.0\n",
            "      - 33.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 80.0\n",
            "      - 143.0\n",
            "      - 176.0\n",
            "      - 200.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 166.0\n",
            "      - 160.0\n",
            "      - 100.0\n",
            "      - 97.0\n",
            "      - 114.0\n",
            "      - 178.0\n",
            "      - 103.0\n",
            "      - 55.0\n",
            "      - 183.0\n",
            "      - 167.0\n",
            "      - 137.0\n",
            "      - 30.0\n",
            "      - 60.0\n",
            "      - 200.0\n",
            "      - 34.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 18.0\n",
            "      - 136.0\n",
            "      - 136.0\n",
            "      - 124.0\n",
            "      - 23.0\n",
            "      - 33.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 158.0\n",
            "      - 51.0\n",
            "      - 186.0\n",
            "      - 126.0\n",
            "      - 191.0\n",
            "      - 12.0\n",
            "      - 124.0\n",
            "      - 188.0\n",
            "      - 139.0\n",
            "      - 156.0\n",
            "      - 26.0\n",
            "      - 174.0\n",
            "      - 29.0\n",
            "      - 116.0\n",
            "      - 200.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13799446587504957\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10790449884113927\n",
            "      mean_inference_ms: 1.3060051288539294\n",
            "      mean_raw_obs_processing_ms: 0.24601407932979946\n",
            "  time_since_restore: 882.2823297977448\n",
            "  time_this_iter_s: 10.827054023742676\n",
            "  time_total_s: 882.2823297977448\n",
            "  timers:\n",
            "    learn_throughput: 29086.814\n",
            "    learn_time_ms: 6.876\n",
            "    load_throughput: 1051467.536\n",
            "    load_time_ms: 0.19\n",
            "    training_iteration_time_ms: 219.892\n",
            "    update_time_ms: 4.266\n",
            "  timestamp: 1656954755\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 421200\n",
            "  training_iteration: 82\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:40 (running for 00:15:10.04)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         882.282</td><td style=\"text-align: right;\">421200</td><td style=\"text-align: right;\">  131.55</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            131.55</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:45 (running for 00:15:15.15)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         882.282</td><td style=\"text-align: right;\">421200</td><td style=\"text-align: right;\">  131.55</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            131.55</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 430600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 430600\n",
            "    num_agent_steps_trained: 430600\n",
            "    num_env_steps_sampled: 430600\n",
            "    num_env_steps_trained: 430600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-12-45\n",
            "  done: false\n",
            "  episode_len_mean: 134.73\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 134.73\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 67\n",
            "  episodes_total: 4593\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 124.73465728759766\n",
            "          policy_loss: 473.4537353515625\n",
            "          var_gnorm: 27.80694580078125\n",
            "          vf_explained_var: 0.9503878951072693\n",
            "          vf_loss: 2605.217041015625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 430600\n",
            "    num_agent_steps_trained: 430600\n",
            "    num_env_steps_sampled: 430600\n",
            "    num_env_steps_trained: 430600\n",
            "  iterations_since_restore: 83\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 430600\n",
            "  num_agent_steps_trained: 430600\n",
            "  num_env_steps_sampled: 430600\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 430600\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.32000000000001\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13802635792731682\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1080365547144042\n",
            "    mean_inference_ms: 1.3057758782620112\n",
            "    mean_raw_obs_processing_ms: 0.24609195600577508\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 134.73\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 134.73\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 67\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 18\n",
            "      - 136\n",
            "      - 136\n",
            "      - 124\n",
            "      - 23\n",
            "      - 33\n",
            "      - 141\n",
            "      - 200\n",
            "      - 69\n",
            "      - 158\n",
            "      - 51\n",
            "      - 186\n",
            "      - 126\n",
            "      - 191\n",
            "      - 12\n",
            "      - 124\n",
            "      - 188\n",
            "      - 139\n",
            "      - 156\n",
            "      - 26\n",
            "      - 174\n",
            "      - 29\n",
            "      - 116\n",
            "      - 200\n",
            "      - 84\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 115\n",
            "      - 200\n",
            "      - 81\n",
            "      - 200\n",
            "      - 197\n",
            "      - 166\n",
            "      - 128\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 190\n",
            "      - 178\n",
            "      - 198\n",
            "      - 186\n",
            "      - 63\n",
            "      - 111\n",
            "      - 200\n",
            "      - 81\n",
            "      - 200\n",
            "      - 191\n",
            "      - 50\n",
            "      - 153\n",
            "      - 60\n",
            "      - 200\n",
            "      - 185\n",
            "      - 168\n",
            "      - 28\n",
            "      - 200\n",
            "      - 200\n",
            "      - 101\n",
            "      - 160\n",
            "      - 197\n",
            "      - 52\n",
            "      - 150\n",
            "      - 128\n",
            "      - 73\n",
            "      - 168\n",
            "      - 31\n",
            "      - 139\n",
            "      - 96\n",
            "      - 151\n",
            "      - 50\n",
            "      - 169\n",
            "      - 200\n",
            "      - 138\n",
            "      - 197\n",
            "      - 200\n",
            "      - 43\n",
            "      - 170\n",
            "      - 200\n",
            "      - 80\n",
            "      - 200\n",
            "      - 200\n",
            "      - 57\n",
            "      - 94\n",
            "      - 177\n",
            "      - 171\n",
            "      - 49\n",
            "      - 200\n",
            "      - 166\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 155\n",
            "      - 66\n",
            "      - 200\n",
            "      - 58\n",
            "      - 58\n",
            "      - 56\n",
            "      - 134\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 18.0\n",
            "      - 136.0\n",
            "      - 136.0\n",
            "      - 124.0\n",
            "      - 23.0\n",
            "      - 33.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 158.0\n",
            "      - 51.0\n",
            "      - 186.0\n",
            "      - 126.0\n",
            "      - 191.0\n",
            "      - 12.0\n",
            "      - 124.0\n",
            "      - 188.0\n",
            "      - 139.0\n",
            "      - 156.0\n",
            "      - 26.0\n",
            "      - 174.0\n",
            "      - 29.0\n",
            "      - 116.0\n",
            "      - 200.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 200.0\n",
            "      - 197.0\n",
            "      - 166.0\n",
            "      - 128.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 190.0\n",
            "      - 178.0\n",
            "      - 198.0\n",
            "      - 186.0\n",
            "      - 63.0\n",
            "      - 111.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 200.0\n",
            "      - 191.0\n",
            "      - 50.0\n",
            "      - 153.0\n",
            "      - 60.0\n",
            "      - 200.0\n",
            "      - 185.0\n",
            "      - 168.0\n",
            "      - 28.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 160.0\n",
            "      - 197.0\n",
            "      - 52.0\n",
            "      - 150.0\n",
            "      - 128.0\n",
            "      - 73.0\n",
            "      - 168.0\n",
            "      - 31.0\n",
            "      - 139.0\n",
            "      - 96.0\n",
            "      - 151.0\n",
            "      - 50.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 170.0\n",
            "      - 200.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 57.0\n",
            "      - 94.0\n",
            "      - 177.0\n",
            "      - 171.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 58.0\n",
            "      - 58.0\n",
            "      - 56.0\n",
            "      - 134.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13802635792731682\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1080365547144042\n",
            "      mean_inference_ms: 1.3057758782620112\n",
            "      mean_raw_obs_processing_ms: 0.24609195600577508\n",
            "  time_since_restore: 892.5343685150146\n",
            "  time_this_iter_s: 10.252038717269897\n",
            "  time_total_s: 892.5343685150146\n",
            "  timers:\n",
            "    learn_throughput: 32336.011\n",
            "    learn_time_ms: 6.185\n",
            "    load_throughput: 1064004.059\n",
            "    load_time_ms: 0.188\n",
            "    training_iteration_time_ms: 223.823\n",
            "    update_time_ms: 3.898\n",
            "  timestamp: 1656954765\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 430600\n",
            "  training_iteration: 83\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:50 (running for 00:15:20.33)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         892.534</td><td style=\"text-align: right;\">430600</td><td style=\"text-align: right;\">  134.73</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            134.73</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:12:55 (running for 00:15:25.42)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         892.534</td><td style=\"text-align: right;\">430600</td><td style=\"text-align: right;\">  134.73</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            134.73</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 431200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 431200\n",
            "    num_agent_steps_trained: 431200\n",
            "    num_env_steps_sampled: 431200\n",
            "    num_env_steps_trained: 431200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-12-56\n",
            "  done: false\n",
            "  episode_len_mean: 135.82\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 135.82\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 4597\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 116.3\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 116.3\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 92\n",
            "      - 124\n",
            "      - 19\n",
            "      - 200\n",
            "      - 90\n",
            "      - 138\n",
            "      - 200\n",
            "      - 162\n",
            "      - 200\n",
            "      - 167\n",
            "      - 200\n",
            "      - 20\n",
            "      - 185\n",
            "      - 60\n",
            "      - 32\n",
            "      - 47\n",
            "      - 82\n",
            "      - 41\n",
            "      - 86\n",
            "      - 181\n",
            "      episode_reward:\n",
            "      - 92.0\n",
            "      - 124.0\n",
            "      - 19.0\n",
            "      - 200.0\n",
            "      - 90.0\n",
            "      - 138.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 200.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 20.0\n",
            "      - 185.0\n",
            "      - 60.0\n",
            "      - 32.0\n",
            "      - 47.0\n",
            "      - 82.0\n",
            "      - 41.0\n",
            "      - 86.0\n",
            "      - 181.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10036861273667148\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07663846697110252\n",
            "      mean_inference_ms: 0.977246962788924\n",
            "      mean_raw_obs_processing_ms: 0.10852158455311896\n",
            "    timesteps_this_iter: 2326\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 123.603515625\n",
            "          policy_loss: 164.6840362548828\n",
            "          var_gnorm: 27.810976028442383\n",
            "          vf_explained_var: 0.3829650282859802\n",
            "          vf_loss: 11565.755859375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 431200\n",
            "    num_agent_steps_trained: 431200\n",
            "    num_env_steps_sampled: 431200\n",
            "    num_env_steps_trained: 431200\n",
            "  iterations_since_restore: 84\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 431200\n",
            "  num_agent_steps_trained: 431200\n",
            "  num_env_steps_sampled: 431200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 431200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.68666666666667\n",
            "    ram_util_percent: 21.699999999999996\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13804928870876995\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1080179143266965\n",
            "    mean_inference_ms: 1.3059036454198327\n",
            "    mean_raw_obs_processing_ms: 0.2460690017548412\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 135.82\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 135.82\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 124\n",
            "      - 23\n",
            "      - 33\n",
            "      - 141\n",
            "      - 200\n",
            "      - 69\n",
            "      - 158\n",
            "      - 51\n",
            "      - 186\n",
            "      - 126\n",
            "      - 191\n",
            "      - 12\n",
            "      - 124\n",
            "      - 188\n",
            "      - 139\n",
            "      - 156\n",
            "      - 26\n",
            "      - 174\n",
            "      - 29\n",
            "      - 116\n",
            "      - 200\n",
            "      - 84\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 110\n",
            "      - 115\n",
            "      - 200\n",
            "      - 81\n",
            "      - 200\n",
            "      - 197\n",
            "      - 166\n",
            "      - 128\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 190\n",
            "      - 178\n",
            "      - 198\n",
            "      - 186\n",
            "      - 63\n",
            "      - 111\n",
            "      - 200\n",
            "      - 81\n",
            "      - 200\n",
            "      - 191\n",
            "      - 50\n",
            "      - 153\n",
            "      - 60\n",
            "      - 200\n",
            "      - 185\n",
            "      - 168\n",
            "      - 28\n",
            "      - 200\n",
            "      - 200\n",
            "      - 101\n",
            "      - 160\n",
            "      - 197\n",
            "      - 52\n",
            "      - 150\n",
            "      - 128\n",
            "      - 73\n",
            "      - 168\n",
            "      - 31\n",
            "      - 139\n",
            "      - 96\n",
            "      - 151\n",
            "      - 50\n",
            "      - 169\n",
            "      - 200\n",
            "      - 138\n",
            "      - 197\n",
            "      - 200\n",
            "      - 43\n",
            "      - 170\n",
            "      - 200\n",
            "      - 80\n",
            "      - 200\n",
            "      - 200\n",
            "      - 57\n",
            "      - 94\n",
            "      - 177\n",
            "      - 171\n",
            "      - 49\n",
            "      - 200\n",
            "      - 166\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 155\n",
            "      - 66\n",
            "      - 200\n",
            "      - 58\n",
            "      - 58\n",
            "      - 56\n",
            "      - 134\n",
            "      - 200\n",
            "      - 139\n",
            "      - 186\n",
            "      - 74\n",
            "      episode_reward:\n",
            "      - 124.0\n",
            "      - 23.0\n",
            "      - 33.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 158.0\n",
            "      - 51.0\n",
            "      - 186.0\n",
            "      - 126.0\n",
            "      - 191.0\n",
            "      - 12.0\n",
            "      - 124.0\n",
            "      - 188.0\n",
            "      - 139.0\n",
            "      - 156.0\n",
            "      - 26.0\n",
            "      - 174.0\n",
            "      - 29.0\n",
            "      - 116.0\n",
            "      - 200.0\n",
            "      - 84.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 110.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 200.0\n",
            "      - 197.0\n",
            "      - 166.0\n",
            "      - 128.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 190.0\n",
            "      - 178.0\n",
            "      - 198.0\n",
            "      - 186.0\n",
            "      - 63.0\n",
            "      - 111.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 200.0\n",
            "      - 191.0\n",
            "      - 50.0\n",
            "      - 153.0\n",
            "      - 60.0\n",
            "      - 200.0\n",
            "      - 185.0\n",
            "      - 168.0\n",
            "      - 28.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 101.0\n",
            "      - 160.0\n",
            "      - 197.0\n",
            "      - 52.0\n",
            "      - 150.0\n",
            "      - 128.0\n",
            "      - 73.0\n",
            "      - 168.0\n",
            "      - 31.0\n",
            "      - 139.0\n",
            "      - 96.0\n",
            "      - 151.0\n",
            "      - 50.0\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 170.0\n",
            "      - 200.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 57.0\n",
            "      - 94.0\n",
            "      - 177.0\n",
            "      - 171.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 58.0\n",
            "      - 58.0\n",
            "      - 56.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 186.0\n",
            "      - 74.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13804928870876995\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1080179143266965\n",
            "      mean_inference_ms: 1.3059036454198327\n",
            "      mean_raw_obs_processing_ms: 0.2460690017548412\n",
            "  time_since_restore: 903.1889696121216\n",
            "  time_this_iter_s: 10.654601097106934\n",
            "  time_total_s: 903.1889696121216\n",
            "  timers:\n",
            "    learn_throughput: 32300.028\n",
            "    learn_time_ms: 6.192\n",
            "    load_throughput: 1064949.6\n",
            "    load_time_ms: 0.188\n",
            "    training_iteration_time_ms: 224.241\n",
            "    update_time_ms: 3.809\n",
            "  timestamp: 1656954776\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 431200\n",
            "  training_iteration: 84\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:01 (running for 00:15:31.02)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         903.189</td><td style=\"text-align: right;\">431200</td><td style=\"text-align: right;\">  135.82</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            135.82</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:06 (running for 00:15:36.11)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         903.189</td><td style=\"text-align: right;\">431200</td><td style=\"text-align: right;\">  135.82</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            135.82</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 440600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 440600\n",
            "    num_agent_steps_trained: 440600\n",
            "    num_env_steps_sampled: 440600\n",
            "    num_env_steps_trained: 440600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-13-06\n",
            "  done: false\n",
            "  episode_len_mean: 138.48\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 138.48\n",
            "  episode_reward_min: 24.0\n",
            "  episodes_this_iter: 68\n",
            "  episodes_total: 4665\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 122.8154525756836\n",
            "          policy_loss: 157.6516876220703\n",
            "          var_gnorm: 27.866092681884766\n",
            "          vf_explained_var: 0.08959013223648071\n",
            "          vf_loss: 11451.986328125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 440600\n",
            "    num_agent_steps_trained: 440600\n",
            "    num_env_steps_sampled: 440600\n",
            "    num_env_steps_trained: 440600\n",
            "  iterations_since_restore: 85\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 440600\n",
            "  num_agent_steps_trained: 440600\n",
            "  num_env_steps_sampled: 440600\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 440600\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.7\n",
            "    ram_util_percent: 21.706666666666663\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13812644609715768\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10806653589545263\n",
            "    mean_inference_ms: 1.30612580982216\n",
            "    mean_raw_obs_processing_ms: 0.24599470150689862\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 138.48\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 138.48\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 68\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 169\n",
            "      - 200\n",
            "      - 138\n",
            "      - 197\n",
            "      - 200\n",
            "      - 43\n",
            "      - 170\n",
            "      - 200\n",
            "      - 80\n",
            "      - 200\n",
            "      - 200\n",
            "      - 57\n",
            "      - 94\n",
            "      - 177\n",
            "      - 171\n",
            "      - 49\n",
            "      - 200\n",
            "      - 166\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 155\n",
            "      - 66\n",
            "      - 200\n",
            "      - 58\n",
            "      - 58\n",
            "      - 56\n",
            "      - 134\n",
            "      - 200\n",
            "      - 139\n",
            "      - 186\n",
            "      - 74\n",
            "      - 200\n",
            "      - 153\n",
            "      - 26\n",
            "      - 133\n",
            "      - 103\n",
            "      - 73\n",
            "      - 45\n",
            "      - 200\n",
            "      - 56\n",
            "      - 64\n",
            "      - 200\n",
            "      - 200\n",
            "      - 171\n",
            "      - 34\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 188\n",
            "      - 106\n",
            "      - 180\n",
            "      - 200\n",
            "      - 50\n",
            "      - 200\n",
            "      - 115\n",
            "      - 139\n",
            "      - 47\n",
            "      - 200\n",
            "      - 163\n",
            "      - 200\n",
            "      - 200\n",
            "      - 171\n",
            "      - 163\n",
            "      - 153\n",
            "      - 31\n",
            "      - 200\n",
            "      - 24\n",
            "      - 200\n",
            "      - 149\n",
            "      - 200\n",
            "      - 114\n",
            "      - 127\n",
            "      - 32\n",
            "      - 200\n",
            "      - 189\n",
            "      - 153\n",
            "      - 183\n",
            "      - 58\n",
            "      - 137\n",
            "      - 119\n",
            "      - 128\n",
            "      - 186\n",
            "      - 159\n",
            "      - 133\n",
            "      - 181\n",
            "      - 142\n",
            "      - 105\n",
            "      - 28\n",
            "      - 59\n",
            "      - 169\n",
            "      - 62\n",
            "      - 200\n",
            "      - 98\n",
            "      - 178\n",
            "      - 185\n",
            "      - 127\n",
            "      - 149\n",
            "      - 143\n",
            "      - 158\n",
            "      episode_reward:\n",
            "      - 169.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 170.0\n",
            "      - 200.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 57.0\n",
            "      - 94.0\n",
            "      - 177.0\n",
            "      - 171.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 58.0\n",
            "      - 58.0\n",
            "      - 56.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 186.0\n",
            "      - 74.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 26.0\n",
            "      - 133.0\n",
            "      - 103.0\n",
            "      - 73.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 64.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 34.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 106.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 139.0\n",
            "      - 47.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 163.0\n",
            "      - 153.0\n",
            "      - 31.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 127.0\n",
            "      - 32.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 153.0\n",
            "      - 183.0\n",
            "      - 58.0\n",
            "      - 137.0\n",
            "      - 119.0\n",
            "      - 128.0\n",
            "      - 186.0\n",
            "      - 159.0\n",
            "      - 133.0\n",
            "      - 181.0\n",
            "      - 142.0\n",
            "      - 105.0\n",
            "      - 28.0\n",
            "      - 59.0\n",
            "      - 169.0\n",
            "      - 62.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 178.0\n",
            "      - 185.0\n",
            "      - 127.0\n",
            "      - 149.0\n",
            "      - 143.0\n",
            "      - 158.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13812644609715768\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10806653589545263\n",
            "      mean_inference_ms: 1.30612580982216\n",
            "      mean_raw_obs_processing_ms: 0.24599470150689862\n",
            "  time_since_restore: 913.4101271629333\n",
            "  time_this_iter_s: 10.221157550811768\n",
            "  time_total_s: 913.4101271629333\n",
            "  timers:\n",
            "    learn_throughput: 30830.269\n",
            "    learn_time_ms: 6.487\n",
            "    load_throughput: 1031682.204\n",
            "    load_time_ms: 0.194\n",
            "    training_iteration_time_ms: 231.946\n",
            "    update_time_ms: 3.697\n",
            "  timestamp: 1656954786\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 440600\n",
            "  training_iteration: 85\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:11 (running for 00:15:41.27)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">          913.41</td><td style=\"text-align: right;\">440600</td><td style=\"text-align: right;\">  138.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            138.48</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:16 (running for 00:15:46.37)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">          913.41</td><td style=\"text-align: right;\">440600</td><td style=\"text-align: right;\">  138.48</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            138.48</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 441200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 441200\n",
            "    num_agent_steps_trained: 441200\n",
            "    num_env_steps_sampled: 441200\n",
            "    num_env_steps_trained: 441200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-13-19\n",
            "  done: false\n",
            "  episode_len_mean: 138.16\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 138.16\n",
            "  episode_reward_min: 24.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 4669\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 143.0\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 143.0\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 148\n",
            "      - 200\n",
            "      - 145\n",
            "      - 131\n",
            "      - 42\n",
            "      - 61\n",
            "      - 169\n",
            "      - 24\n",
            "      - 178\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 127\n",
            "      - 172\n",
            "      - 134\n",
            "      - 200\n",
            "      - 140\n",
            "      - 200\n",
            "      - 152\n",
            "      - 37\n",
            "      episode_reward:\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 145.0\n",
            "      - 131.0\n",
            "      - 42.0\n",
            "      - 61.0\n",
            "      - 169.0\n",
            "      - 24.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 127.0\n",
            "      - 172.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 37.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1005760175645436\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07684493396804923\n",
            "      mean_inference_ms: 0.980146162466373\n",
            "      mean_raw_obs_processing_ms: 0.10866412857202964\n",
            "    timesteps_this_iter: 2860\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 122.55778503417969\n",
            "          policy_loss: 436.1522216796875\n",
            "          var_gnorm: 27.86966896057129\n",
            "          vf_explained_var: 0.8665980100631714\n",
            "          vf_loss: 3180.666015625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 441200\n",
            "    num_agent_steps_trained: 441200\n",
            "    num_env_steps_sampled: 441200\n",
            "    num_env_steps_trained: 441200\n",
            "  iterations_since_restore: 86\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 441200\n",
            "  num_agent_steps_trained: 441200\n",
            "  num_env_steps_sampled: 441200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 441200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 74.48888888888888\n",
            "    ram_util_percent: 21.77777777777778\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1381472847328112\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10804701598539344\n",
            "    mean_inference_ms: 1.3062658225930057\n",
            "    mean_raw_obs_processing_ms: 0.24597332164660066\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 138.16\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 138.16\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 43\n",
            "      - 170\n",
            "      - 200\n",
            "      - 80\n",
            "      - 200\n",
            "      - 200\n",
            "      - 57\n",
            "      - 94\n",
            "      - 177\n",
            "      - 171\n",
            "      - 49\n",
            "      - 200\n",
            "      - 166\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 155\n",
            "      - 66\n",
            "      - 200\n",
            "      - 58\n",
            "      - 58\n",
            "      - 56\n",
            "      - 134\n",
            "      - 200\n",
            "      - 139\n",
            "      - 186\n",
            "      - 74\n",
            "      - 200\n",
            "      - 153\n",
            "      - 26\n",
            "      - 133\n",
            "      - 103\n",
            "      - 73\n",
            "      - 45\n",
            "      - 200\n",
            "      - 56\n",
            "      - 64\n",
            "      - 200\n",
            "      - 200\n",
            "      - 171\n",
            "      - 34\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 188\n",
            "      - 106\n",
            "      - 180\n",
            "      - 200\n",
            "      - 50\n",
            "      - 200\n",
            "      - 115\n",
            "      - 139\n",
            "      - 47\n",
            "      - 200\n",
            "      - 163\n",
            "      - 200\n",
            "      - 200\n",
            "      - 171\n",
            "      - 163\n",
            "      - 153\n",
            "      - 31\n",
            "      - 200\n",
            "      - 24\n",
            "      - 200\n",
            "      - 149\n",
            "      - 200\n",
            "      - 114\n",
            "      - 127\n",
            "      - 32\n",
            "      - 200\n",
            "      - 189\n",
            "      - 153\n",
            "      - 183\n",
            "      - 58\n",
            "      - 137\n",
            "      - 119\n",
            "      - 128\n",
            "      - 186\n",
            "      - 159\n",
            "      - 133\n",
            "      - 181\n",
            "      - 142\n",
            "      - 105\n",
            "      - 28\n",
            "      - 59\n",
            "      - 169\n",
            "      - 62\n",
            "      - 200\n",
            "      - 98\n",
            "      - 178\n",
            "      - 185\n",
            "      - 127\n",
            "      - 149\n",
            "      - 143\n",
            "      - 158\n",
            "      - 94\n",
            "      - 178\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 170.0\n",
            "      - 200.0\n",
            "      - 80.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 57.0\n",
            "      - 94.0\n",
            "      - 177.0\n",
            "      - 171.0\n",
            "      - 49.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 155.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 58.0\n",
            "      - 58.0\n",
            "      - 56.0\n",
            "      - 134.0\n",
            "      - 200.0\n",
            "      - 139.0\n",
            "      - 186.0\n",
            "      - 74.0\n",
            "      - 200.0\n",
            "      - 153.0\n",
            "      - 26.0\n",
            "      - 133.0\n",
            "      - 103.0\n",
            "      - 73.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 56.0\n",
            "      - 64.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 34.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 188.0\n",
            "      - 106.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 50.0\n",
            "      - 200.0\n",
            "      - 115.0\n",
            "      - 139.0\n",
            "      - 47.0\n",
            "      - 200.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 171.0\n",
            "      - 163.0\n",
            "      - 153.0\n",
            "      - 31.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 127.0\n",
            "      - 32.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 153.0\n",
            "      - 183.0\n",
            "      - 58.0\n",
            "      - 137.0\n",
            "      - 119.0\n",
            "      - 128.0\n",
            "      - 186.0\n",
            "      - 159.0\n",
            "      - 133.0\n",
            "      - 181.0\n",
            "      - 142.0\n",
            "      - 105.0\n",
            "      - 28.0\n",
            "      - 59.0\n",
            "      - 169.0\n",
            "      - 62.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 178.0\n",
            "      - 185.0\n",
            "      - 127.0\n",
            "      - 149.0\n",
            "      - 143.0\n",
            "      - 158.0\n",
            "      - 94.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1381472847328112\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10804701598539344\n",
            "      mean_inference_ms: 1.3062658225930057\n",
            "      mean_raw_obs_processing_ms: 0.24597332164660066\n",
            "  time_since_restore: 926.068834066391\n",
            "  time_this_iter_s: 12.658706903457642\n",
            "  time_total_s: 926.068834066391\n",
            "  timers:\n",
            "    learn_throughput: 24762.103\n",
            "    learn_time_ms: 8.077\n",
            "    load_throughput: 989689.476\n",
            "    load_time_ms: 0.202\n",
            "    training_iteration_time_ms: 243.578\n",
            "    update_time_ms: 4.794\n",
            "  timestamp: 1656954799\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 441200\n",
            "  training_iteration: 86\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:24 (running for 00:15:53.97)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         926.069</td><td style=\"text-align: right;\">441200</td><td style=\"text-align: right;\">  138.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            138.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:29 (running for 00:15:59.09)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         926.069</td><td style=\"text-align: right;\">441200</td><td style=\"text-align: right;\">  138.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            138.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 450600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 450600\n",
            "    num_agent_steps_trained: 450600\n",
            "    num_env_steps_sampled: 450600\n",
            "    num_env_steps_trained: 450600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-13-29\n",
            "  done: false\n",
            "  episode_len_mean: 143.78\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 143.78\n",
            "  episode_reward_min: 24.0\n",
            "  episodes_this_iter: 64\n",
            "  episodes_total: 4733\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 123.08521270751953\n",
            "          policy_loss: 567.513427734375\n",
            "          var_gnorm: 27.931222915649414\n",
            "          vf_explained_var: 0.06733351945877075\n",
            "          vf_loss: 4485.2763671875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 450600\n",
            "    num_agent_steps_trained: 450600\n",
            "    num_env_steps_sampled: 450600\n",
            "    num_env_steps_trained: 450600\n",
            "  iterations_since_restore: 87\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 450600\n",
            "  num_agent_steps_trained: 450600\n",
            "  num_env_steps_sampled: 450600\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 450600\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.26\n",
            "    ram_util_percent: 21.786666666666672\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1381913774979128\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10810576835005253\n",
            "    mean_inference_ms: 1.3063510537116318\n",
            "    mean_raw_obs_processing_ms: 0.24591205746939893\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 143.78\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 143.78\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 64\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 149\n",
            "      - 200\n",
            "      - 114\n",
            "      - 127\n",
            "      - 32\n",
            "      - 200\n",
            "      - 189\n",
            "      - 153\n",
            "      - 183\n",
            "      - 58\n",
            "      - 137\n",
            "      - 119\n",
            "      - 128\n",
            "      - 186\n",
            "      - 159\n",
            "      - 133\n",
            "      - 181\n",
            "      - 142\n",
            "      - 105\n",
            "      - 28\n",
            "      - 59\n",
            "      - 169\n",
            "      - 62\n",
            "      - 200\n",
            "      - 98\n",
            "      - 178\n",
            "      - 185\n",
            "      - 127\n",
            "      - 149\n",
            "      - 143\n",
            "      - 158\n",
            "      - 94\n",
            "      - 178\n",
            "      - 200\n",
            "      - 200\n",
            "      - 123\n",
            "      - 197\n",
            "      - 200\n",
            "      - 149\n",
            "      - 200\n",
            "      - 200\n",
            "      - 161\n",
            "      - 125\n",
            "      - 44\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 190\n",
            "      - 123\n",
            "      - 44\n",
            "      - 90\n",
            "      - 166\n",
            "      - 139\n",
            "      - 181\n",
            "      - 105\n",
            "      - 141\n",
            "      - 200\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 124\n",
            "      - 30\n",
            "      - 90\n",
            "      - 145\n",
            "      - 35\n",
            "      - 24\n",
            "      - 200\n",
            "      - 73\n",
            "      - 142\n",
            "      - 89\n",
            "      - 200\n",
            "      - 138\n",
            "      - 200\n",
            "      - 105\n",
            "      - 153\n",
            "      - 200\n",
            "      - 69\n",
            "      - 165\n",
            "      - 137\n",
            "      - 160\n",
            "      - 200\n",
            "      - 103\n",
            "      - 147\n",
            "      - 154\n",
            "      - 158\n",
            "      - 177\n",
            "      - 113\n",
            "      - 46\n",
            "      - 200\n",
            "      - 160\n",
            "      - 98\n",
            "      - 37\n",
            "      - 197\n",
            "      - 153\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 114.0\n",
            "      - 127.0\n",
            "      - 32.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 153.0\n",
            "      - 183.0\n",
            "      - 58.0\n",
            "      - 137.0\n",
            "      - 119.0\n",
            "      - 128.0\n",
            "      - 186.0\n",
            "      - 159.0\n",
            "      - 133.0\n",
            "      - 181.0\n",
            "      - 142.0\n",
            "      - 105.0\n",
            "      - 28.0\n",
            "      - 59.0\n",
            "      - 169.0\n",
            "      - 62.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 178.0\n",
            "      - 185.0\n",
            "      - 127.0\n",
            "      - 149.0\n",
            "      - 143.0\n",
            "      - 158.0\n",
            "      - 94.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 123.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 161.0\n",
            "      - 125.0\n",
            "      - 44.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 190.0\n",
            "      - 123.0\n",
            "      - 44.0\n",
            "      - 90.0\n",
            "      - 166.0\n",
            "      - 139.0\n",
            "      - 181.0\n",
            "      - 105.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 30.0\n",
            "      - 90.0\n",
            "      - 145.0\n",
            "      - 35.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 73.0\n",
            "      - 142.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 200.0\n",
            "      - 105.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 165.0\n",
            "      - 137.0\n",
            "      - 160.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 147.0\n",
            "      - 154.0\n",
            "      - 158.0\n",
            "      - 177.0\n",
            "      - 113.0\n",
            "      - 46.0\n",
            "      - 200.0\n",
            "      - 160.0\n",
            "      - 98.0\n",
            "      - 37.0\n",
            "      - 197.0\n",
            "      - 153.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1381913774979128\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10810576835005253\n",
            "      mean_inference_ms: 1.3063510537116318\n",
            "      mean_raw_obs_processing_ms: 0.24591205746939893\n",
            "  time_since_restore: 936.236085653305\n",
            "  time_this_iter_s: 10.167251586914062\n",
            "  time_total_s: 936.236085653305\n",
            "  timers:\n",
            "    learn_throughput: 27276.744\n",
            "    learn_time_ms: 7.332\n",
            "    load_throughput: 1069702.627\n",
            "    load_time_ms: 0.187\n",
            "    training_iteration_time_ms: 211.367\n",
            "    update_time_ms: 5.044\n",
            "  timestamp: 1656954809\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 450600\n",
            "  training_iteration: 87\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:34 (running for 00:16:04.17)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         936.236</td><td style=\"text-align: right;\">450600</td><td style=\"text-align: right;\">  143.78</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            143.78</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:39 (running for 00:16:09.26)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         936.236</td><td style=\"text-align: right;\">450600</td><td style=\"text-align: right;\">  143.78</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            143.78</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 451200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 451200\n",
            "    num_agent_steps_trained: 451200\n",
            "    num_env_steps_sampled: 451200\n",
            "    num_env_steps_trained: 451200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-13-41\n",
            "  done: false\n",
            "  episode_len_mean: 141.29\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 141.29\n",
            "  episode_reward_min: 24.0\n",
            "  episodes_this_iter: 7\n",
            "  episodes_total: 4740\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 145.9\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 145.9\n",
            "    episode_reward_min: 25.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 169\n",
            "      - 141\n",
            "      - 154\n",
            "      - 25\n",
            "      - 180\n",
            "      - 198\n",
            "      - 200\n",
            "      - 200\n",
            "      - 104\n",
            "      - 163\n",
            "      - 200\n",
            "      - 25\n",
            "      - 118\n",
            "      - 128\n",
            "      - 66\n",
            "      - 191\n",
            "      - 196\n",
            "      - 153\n",
            "      - 107\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 169.0\n",
            "      - 141.0\n",
            "      - 154.0\n",
            "      - 25.0\n",
            "      - 180.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 104.0\n",
            "      - 163.0\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "      - 118.0\n",
            "      - 128.0\n",
            "      - 66.0\n",
            "      - 191.0\n",
            "      - 196.0\n",
            "      - 153.0\n",
            "      - 107.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10056910057897765\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07683361913288594\n",
            "      mean_inference_ms: 0.9799787070137885\n",
            "      mean_raw_obs_processing_ms: 0.10860992996713544\n",
            "    timesteps_this_iter: 2918\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 121.59185028076172\n",
            "          policy_loss: 8.779309272766113\n",
            "          var_gnorm: 27.93427085876465\n",
            "          vf_explained_var: 0.041480183601379395\n",
            "          vf_loss: 17428.361328125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 451200\n",
            "    num_agent_steps_trained: 451200\n",
            "    num_env_steps_sampled: 451200\n",
            "    num_env_steps_trained: 451200\n",
            "  iterations_since_restore: 88\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 451200\n",
            "  num_agent_steps_trained: 451200\n",
            "  num_env_steps_sampled: 451200\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 451200\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 64.71875\n",
            "    ram_util_percent: 21.8\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13821789701007572\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10808954679821994\n",
            "    mean_inference_ms: 1.306493332087746\n",
            "    mean_raw_obs_processing_ms: 0.24588809576858967\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 141.29\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 141.29\n",
            "    episode_reward_min: 24.0\n",
            "    episodes_this_iter: 7\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 189\n",
            "      - 153\n",
            "      - 183\n",
            "      - 58\n",
            "      - 137\n",
            "      - 119\n",
            "      - 128\n",
            "      - 186\n",
            "      - 159\n",
            "      - 133\n",
            "      - 181\n",
            "      - 142\n",
            "      - 105\n",
            "      - 28\n",
            "      - 59\n",
            "      - 169\n",
            "      - 62\n",
            "      - 200\n",
            "      - 98\n",
            "      - 178\n",
            "      - 185\n",
            "      - 127\n",
            "      - 149\n",
            "      - 143\n",
            "      - 158\n",
            "      - 94\n",
            "      - 178\n",
            "      - 200\n",
            "      - 200\n",
            "      - 123\n",
            "      - 197\n",
            "      - 200\n",
            "      - 149\n",
            "      - 200\n",
            "      - 200\n",
            "      - 161\n",
            "      - 125\n",
            "      - 44\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 200\n",
            "      - 190\n",
            "      - 123\n",
            "      - 44\n",
            "      - 90\n",
            "      - 166\n",
            "      - 139\n",
            "      - 181\n",
            "      - 105\n",
            "      - 141\n",
            "      - 200\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 124\n",
            "      - 30\n",
            "      - 90\n",
            "      - 145\n",
            "      - 35\n",
            "      - 24\n",
            "      - 200\n",
            "      - 73\n",
            "      - 142\n",
            "      - 89\n",
            "      - 200\n",
            "      - 138\n",
            "      - 200\n",
            "      - 105\n",
            "      - 153\n",
            "      - 200\n",
            "      - 69\n",
            "      - 165\n",
            "      - 137\n",
            "      - 160\n",
            "      - 200\n",
            "      - 103\n",
            "      - 147\n",
            "      - 154\n",
            "      - 158\n",
            "      - 177\n",
            "      - 113\n",
            "      - 46\n",
            "      - 200\n",
            "      - 160\n",
            "      - 98\n",
            "      - 37\n",
            "      - 197\n",
            "      - 153\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 140\n",
            "      - 200\n",
            "      - 200\n",
            "      - 46\n",
            "      - 36\n",
            "      - 42\n",
            "      - 109\n",
            "      episode_reward:\n",
            "      - 189.0\n",
            "      - 153.0\n",
            "      - 183.0\n",
            "      - 58.0\n",
            "      - 137.0\n",
            "      - 119.0\n",
            "      - 128.0\n",
            "      - 186.0\n",
            "      - 159.0\n",
            "      - 133.0\n",
            "      - 181.0\n",
            "      - 142.0\n",
            "      - 105.0\n",
            "      - 28.0\n",
            "      - 59.0\n",
            "      - 169.0\n",
            "      - 62.0\n",
            "      - 200.0\n",
            "      - 98.0\n",
            "      - 178.0\n",
            "      - 185.0\n",
            "      - 127.0\n",
            "      - 149.0\n",
            "      - 143.0\n",
            "      - 158.0\n",
            "      - 94.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 123.0\n",
            "      - 197.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 161.0\n",
            "      - 125.0\n",
            "      - 44.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 190.0\n",
            "      - 123.0\n",
            "      - 44.0\n",
            "      - 90.0\n",
            "      - 166.0\n",
            "      - 139.0\n",
            "      - 181.0\n",
            "      - 105.0\n",
            "      - 141.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 124.0\n",
            "      - 30.0\n",
            "      - 90.0\n",
            "      - 145.0\n",
            "      - 35.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 73.0\n",
            "      - 142.0\n",
            "      - 89.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 200.0\n",
            "      - 105.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 69.0\n",
            "      - 165.0\n",
            "      - 137.0\n",
            "      - 160.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 147.0\n",
            "      - 154.0\n",
            "      - 158.0\n",
            "      - 177.0\n",
            "      - 113.0\n",
            "      - 46.0\n",
            "      - 200.0\n",
            "      - 160.0\n",
            "      - 98.0\n",
            "      - 37.0\n",
            "      - 197.0\n",
            "      - 153.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 36.0\n",
            "      - 42.0\n",
            "      - 109.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13821789701007572\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10808954679821994\n",
            "      mean_inference_ms: 1.306493332087746\n",
            "      mean_raw_obs_processing_ms: 0.24588809576858967\n",
            "  time_since_restore: 947.7461507320404\n",
            "  time_this_iter_s: 11.510065078735352\n",
            "  time_total_s: 947.7461507320404\n",
            "  timers:\n",
            "    learn_throughput: 25151.136\n",
            "    learn_time_ms: 7.952\n",
            "    load_throughput: 966540.846\n",
            "    load_time_ms: 0.207\n",
            "    training_iteration_time_ms: 214.989\n",
            "    update_time_ms: 5.153\n",
            "  timestamp: 1656954821\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 451200\n",
            "  training_iteration: 88\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:46 (running for 00:16:15.72)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         947.746</td><td style=\"text-align: right;\">451200</td><td style=\"text-align: right;\">  141.29</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            141.29</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:51 (running for 00:16:20.85)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         947.746</td><td style=\"text-align: right;\">451200</td><td style=\"text-align: right;\">  141.29</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  24</td><td style=\"text-align: right;\">            141.29</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 460800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 460800\n",
            "    num_agent_steps_trained: 460800\n",
            "    num_env_steps_sampled: 460800\n",
            "    num_env_steps_trained: 460800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-13-51\n",
            "  done: false\n",
            "  episode_len_mean: 128.03\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 128.03\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 76\n",
            "  episodes_total: 4816\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 123.30158233642578\n",
            "          policy_loss: 88.15249633789062\n",
            "          var_gnorm: 27.975990295410156\n",
            "          vf_explained_var: 0.2976135015487671\n",
            "          vf_loss: 17018.392578125\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 460800\n",
            "    num_agent_steps_trained: 460800\n",
            "    num_env_steps_sampled: 460800\n",
            "    num_env_steps_trained: 460800\n",
            "  iterations_since_restore: 89\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 460800\n",
            "  num_agent_steps_trained: 460800\n",
            "  num_env_steps_sampled: 460800\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 460800\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.30666666666667\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13826576455379144\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.108041210482112\n",
            "    mean_inference_ms: 1.3064354662275681\n",
            "    mean_raw_obs_processing_ms: 0.24566786224970905\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 128.03\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 128.03\n",
            "    episode_reward_min: 10.0\n",
            "    episodes_this_iter: 76\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 103\n",
            "      - 147\n",
            "      - 154\n",
            "      - 158\n",
            "      - 177\n",
            "      - 113\n",
            "      - 46\n",
            "      - 200\n",
            "      - 160\n",
            "      - 98\n",
            "      - 37\n",
            "      - 197\n",
            "      - 153\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 140\n",
            "      - 200\n",
            "      - 200\n",
            "      - 46\n",
            "      - 36\n",
            "      - 42\n",
            "      - 109\n",
            "      - 150\n",
            "      - 30\n",
            "      - 200\n",
            "      - 10\n",
            "      - 129\n",
            "      - 65\n",
            "      - 200\n",
            "      - 131\n",
            "      - 17\n",
            "      - 49\n",
            "      - 44\n",
            "      - 48\n",
            "      - 153\n",
            "      - 37\n",
            "      - 115\n",
            "      - 200\n",
            "      - 158\n",
            "      - 83\n",
            "      - 71\n",
            "      - 39\n",
            "      - 200\n",
            "      - 128\n",
            "      - 194\n",
            "      - 166\n",
            "      - 200\n",
            "      - 142\n",
            "      - 51\n",
            "      - 187\n",
            "      - 22\n",
            "      - 66\n",
            "      - 47\n",
            "      - 160\n",
            "      - 178\n",
            "      - 200\n",
            "      - 191\n",
            "      - 158\n",
            "      - 134\n",
            "      - 138\n",
            "      - 61\n",
            "      - 17\n",
            "      - 200\n",
            "      - 68\n",
            "      - 200\n",
            "      - 23\n",
            "      - 87\n",
            "      - 200\n",
            "      - 198\n",
            "      - 159\n",
            "      - 200\n",
            "      - 200\n",
            "      - 173\n",
            "      - 36\n",
            "      - 200\n",
            "      - 28\n",
            "      - 40\n",
            "      - 106\n",
            "      - 129\n",
            "      - 193\n",
            "      - 200\n",
            "      - 107\n",
            "      - 165\n",
            "      - 200\n",
            "      - 102\n",
            "      - 139\n",
            "      - 114\n",
            "      - 110\n",
            "      - 200\n",
            "      - 36\n",
            "      - 171\n",
            "      - 168\n",
            "      - 56\n",
            "      - 137\n",
            "      - 95\n",
            "      - 156\n",
            "      - 179\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 103.0\n",
            "      - 147.0\n",
            "      - 154.0\n",
            "      - 158.0\n",
            "      - 177.0\n",
            "      - 113.0\n",
            "      - 46.0\n",
            "      - 200.0\n",
            "      - 160.0\n",
            "      - 98.0\n",
            "      - 37.0\n",
            "      - 197.0\n",
            "      - 153.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 36.0\n",
            "      - 42.0\n",
            "      - 109.0\n",
            "      - 150.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 10.0\n",
            "      - 129.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 17.0\n",
            "      - 49.0\n",
            "      - 44.0\n",
            "      - 48.0\n",
            "      - 153.0\n",
            "      - 37.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 83.0\n",
            "      - 71.0\n",
            "      - 39.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 194.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 51.0\n",
            "      - 187.0\n",
            "      - 22.0\n",
            "      - 66.0\n",
            "      - 47.0\n",
            "      - 160.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 191.0\n",
            "      - 158.0\n",
            "      - 134.0\n",
            "      - 138.0\n",
            "      - 61.0\n",
            "      - 17.0\n",
            "      - 200.0\n",
            "      - 68.0\n",
            "      - 200.0\n",
            "      - 23.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 40.0\n",
            "      - 106.0\n",
            "      - 129.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 102.0\n",
            "      - 139.0\n",
            "      - 114.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 171.0\n",
            "      - 168.0\n",
            "      - 56.0\n",
            "      - 137.0\n",
            "      - 95.0\n",
            "      - 156.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13826576455379144\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.108041210482112\n",
            "      mean_inference_ms: 1.3064354662275681\n",
            "      mean_raw_obs_processing_ms: 0.24566786224970905\n",
            "  time_since_restore: 957.9120900630951\n",
            "  time_this_iter_s: 10.165939331054688\n",
            "  time_total_s: 957.9120900630951\n",
            "  timers:\n",
            "    learn_throughput: 28612.581\n",
            "    learn_time_ms: 6.99\n",
            "    load_throughput: 930516.694\n",
            "    load_time_ms: 0.215\n",
            "    training_iteration_time_ms: 213.919\n",
            "    update_time_ms: 4.446\n",
            "  timestamp: 1656954831\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 460800\n",
            "  training_iteration: 89\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:13:56 (running for 00:16:25.92)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         957.912</td><td style=\"text-align: right;\">460800</td><td style=\"text-align: right;\">  128.03</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            128.03</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:01 (running for 00:16:31.01)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         957.912</td><td style=\"text-align: right;\">460800</td><td style=\"text-align: right;\">  128.03</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            128.03</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 461400\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 461400\n",
            "    num_agent_steps_trained: 461400\n",
            "    num_env_steps_sampled: 461400\n",
            "    num_env_steps_trained: 461400\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-14-03\n",
            "  done: false\n",
            "  episode_len_mean: 129.57\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 129.57\n",
            "  episode_reward_min: 10.0\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 4819\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 160.45\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 160.45\n",
            "    episode_reward_min: 41.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 46\n",
            "      - 135\n",
            "      - 196\n",
            "      - 184\n",
            "      - 41\n",
            "      - 147\n",
            "      - 200\n",
            "      - 161\n",
            "      - 151\n",
            "      - 142\n",
            "      - 200\n",
            "      - 200\n",
            "      - 195\n",
            "      - 177\n",
            "      - 200\n",
            "      - 184\n",
            "      - 200\n",
            "      - 79\n",
            "      - 171\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 135.0\n",
            "      - 196.0\n",
            "      - 184.0\n",
            "      - 41.0\n",
            "      - 147.0\n",
            "      - 200.0\n",
            "      - 161.0\n",
            "      - 151.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 195.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 184.0\n",
            "      - 200.0\n",
            "      - 79.0\n",
            "      - 171.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10055648323280764\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07678941664735665\n",
            "      mean_inference_ms: 0.9794532320459316\n",
            "      mean_raw_obs_processing_ms: 0.10848462793869981\n",
            "    timesteps_this_iter: 3209\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 115.7870101928711\n",
            "          policy_loss: 111.80000305175781\n",
            "          var_gnorm: 27.978282928466797\n",
            "          vf_explained_var: 0.15814340114593506\n",
            "          vf_loss: 11537.3671875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 461400\n",
            "    num_agent_steps_trained: 461400\n",
            "    num_env_steps_sampled: 461400\n",
            "    num_env_steps_trained: 461400\n",
            "  iterations_since_restore: 90\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 461400\n",
            "  num_agent_steps_trained: 461400\n",
            "  num_env_steps_sampled: 461400\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 461400\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 64.74705882352941\n",
            "    ram_util_percent: 21.8\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13827267648137437\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.1080313130916414\n",
            "    mean_inference_ms: 1.3064692979674108\n",
            "    mean_raw_obs_processing_ms: 0.24565373505255983\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 129.57\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 129.57\n",
            "    episode_reward_min: 10.0\n",
            "    episodes_this_iter: 3\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 158\n",
            "      - 177\n",
            "      - 113\n",
            "      - 46\n",
            "      - 200\n",
            "      - 160\n",
            "      - 98\n",
            "      - 37\n",
            "      - 197\n",
            "      - 153\n",
            "      - 143\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 140\n",
            "      - 200\n",
            "      - 200\n",
            "      - 46\n",
            "      - 36\n",
            "      - 42\n",
            "      - 109\n",
            "      - 150\n",
            "      - 30\n",
            "      - 200\n",
            "      - 10\n",
            "      - 129\n",
            "      - 65\n",
            "      - 200\n",
            "      - 131\n",
            "      - 17\n",
            "      - 49\n",
            "      - 44\n",
            "      - 48\n",
            "      - 153\n",
            "      - 37\n",
            "      - 115\n",
            "      - 200\n",
            "      - 158\n",
            "      - 83\n",
            "      - 71\n",
            "      - 39\n",
            "      - 200\n",
            "      - 128\n",
            "      - 194\n",
            "      - 166\n",
            "      - 200\n",
            "      - 142\n",
            "      - 51\n",
            "      - 187\n",
            "      - 22\n",
            "      - 66\n",
            "      - 47\n",
            "      - 160\n",
            "      - 178\n",
            "      - 200\n",
            "      - 191\n",
            "      - 158\n",
            "      - 134\n",
            "      - 138\n",
            "      - 61\n",
            "      - 17\n",
            "      - 200\n",
            "      - 68\n",
            "      - 200\n",
            "      - 23\n",
            "      - 87\n",
            "      - 200\n",
            "      - 198\n",
            "      - 159\n",
            "      - 200\n",
            "      - 200\n",
            "      - 173\n",
            "      - 36\n",
            "      - 200\n",
            "      - 28\n",
            "      - 40\n",
            "      - 106\n",
            "      - 129\n",
            "      - 193\n",
            "      - 200\n",
            "      - 107\n",
            "      - 165\n",
            "      - 200\n",
            "      - 102\n",
            "      - 139\n",
            "      - 114\n",
            "      - 110\n",
            "      - 200\n",
            "      - 36\n",
            "      - 171\n",
            "      - 168\n",
            "      - 56\n",
            "      - 137\n",
            "      - 95\n",
            "      - 156\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      episode_reward:\n",
            "      - 158.0\n",
            "      - 177.0\n",
            "      - 113.0\n",
            "      - 46.0\n",
            "      - 200.0\n",
            "      - 160.0\n",
            "      - 98.0\n",
            "      - 37.0\n",
            "      - 197.0\n",
            "      - 153.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 140.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 46.0\n",
            "      - 36.0\n",
            "      - 42.0\n",
            "      - 109.0\n",
            "      - 150.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 10.0\n",
            "      - 129.0\n",
            "      - 65.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 17.0\n",
            "      - 49.0\n",
            "      - 44.0\n",
            "      - 48.0\n",
            "      - 153.0\n",
            "      - 37.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 83.0\n",
            "      - 71.0\n",
            "      - 39.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 194.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 51.0\n",
            "      - 187.0\n",
            "      - 22.0\n",
            "      - 66.0\n",
            "      - 47.0\n",
            "      - 160.0\n",
            "      - 178.0\n",
            "      - 200.0\n",
            "      - 191.0\n",
            "      - 158.0\n",
            "      - 134.0\n",
            "      - 138.0\n",
            "      - 61.0\n",
            "      - 17.0\n",
            "      - 200.0\n",
            "      - 68.0\n",
            "      - 200.0\n",
            "      - 23.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 40.0\n",
            "      - 106.0\n",
            "      - 129.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 102.0\n",
            "      - 139.0\n",
            "      - 114.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 171.0\n",
            "      - 168.0\n",
            "      - 56.0\n",
            "      - 137.0\n",
            "      - 95.0\n",
            "      - 156.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13827267648137437\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.1080313130916414\n",
            "      mean_inference_ms: 1.3064692979674108\n",
            "      mean_raw_obs_processing_ms: 0.24565373505255983\n",
            "  time_since_restore: 970.2541680335999\n",
            "  time_this_iter_s: 12.34207797050476\n",
            "  time_total_s: 970.2541680335999\n",
            "  timers:\n",
            "    learn_throughput: 25943.052\n",
            "    learn_time_ms: 7.709\n",
            "    load_throughput: 934663.844\n",
            "    load_time_ms: 0.214\n",
            "    training_iteration_time_ms: 214.323\n",
            "    update_time_ms: 4.337\n",
            "  timestamp: 1656954843\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 461400\n",
            "  training_iteration: 90\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:08 (running for 00:16:38.30)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         970.254</td><td style=\"text-align: right;\">461400</td><td style=\"text-align: right;\">  129.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            129.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:13 (running for 00:16:43.40)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         970.254</td><td style=\"text-align: right;\">461400</td><td style=\"text-align: right;\">  129.57</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  10</td><td style=\"text-align: right;\">            129.57</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 471000\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 471000\n",
            "    num_agent_steps_trained: 471000\n",
            "    num_env_steps_sampled: 471000\n",
            "    num_env_steps_trained: 471000\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-14-14\n",
            "  done: false\n",
            "  episode_len_mean: 147.78\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 147.78\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 63\n",
            "  episodes_total: 4882\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 119.98658752441406\n",
            "          policy_loss: 348.99658203125\n",
            "          var_gnorm: 28.024354934692383\n",
            "          vf_explained_var: 0.8992031216621399\n",
            "          vf_loss: 1855.906494140625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 471000\n",
            "    num_agent_steps_trained: 471000\n",
            "    num_env_steps_sampled: 471000\n",
            "    num_env_steps_trained: 471000\n",
            "  iterations_since_restore: 91\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 471000\n",
            "  num_agent_steps_trained: 471000\n",
            "  num_env_steps_sampled: 471000\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 471000\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.74666666666668\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13813652053695288\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10816675663385922\n",
            "    mean_inference_ms: 1.305335376522725\n",
            "    mean_raw_obs_processing_ms: 0.24566786441447597\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 147.78\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 147.78\n",
            "    episode_reward_min: 22.0\n",
            "    episodes_this_iter: 63\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 23\n",
            "      - 87\n",
            "      - 200\n",
            "      - 198\n",
            "      - 159\n",
            "      - 200\n",
            "      - 200\n",
            "      - 173\n",
            "      - 36\n",
            "      - 200\n",
            "      - 28\n",
            "      - 40\n",
            "      - 106\n",
            "      - 129\n",
            "      - 193\n",
            "      - 200\n",
            "      - 107\n",
            "      - 165\n",
            "      - 200\n",
            "      - 102\n",
            "      - 139\n",
            "      - 114\n",
            "      - 110\n",
            "      - 200\n",
            "      - 36\n",
            "      - 171\n",
            "      - 168\n",
            "      - 56\n",
            "      - 137\n",
            "      - 95\n",
            "      - 156\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 200\n",
            "      - 200\n",
            "      - 172\n",
            "      - 188\n",
            "      - 200\n",
            "      - 195\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 130\n",
            "      - 150\n",
            "      - 142\n",
            "      - 200\n",
            "      - 136\n",
            "      - 200\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 154\n",
            "      - 178\n",
            "      - 93\n",
            "      - 200\n",
            "      - 200\n",
            "      - 93\n",
            "      - 78\n",
            "      - 43\n",
            "      - 56\n",
            "      - 200\n",
            "      - 200\n",
            "      - 152\n",
            "      - 153\n",
            "      - 22\n",
            "      - 167\n",
            "      - 153\n",
            "      - 166\n",
            "      - 190\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 157\n",
            "      - 99\n",
            "      - 200\n",
            "      - 106\n",
            "      - 135\n",
            "      - 200\n",
            "      - 131\n",
            "      - 55\n",
            "      - 172\n",
            "      - 63\n",
            "      - 200\n",
            "      - 126\n",
            "      - 168\n",
            "      - 38\n",
            "      - 56\n",
            "      - 172\n",
            "      - 171\n",
            "      - 164\n",
            "      - 33\n",
            "      - 84\n",
            "      - 153\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 23.0\n",
            "      - 87.0\n",
            "      - 200.0\n",
            "      - 198.0\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 40.0\n",
            "      - 106.0\n",
            "      - 129.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 102.0\n",
            "      - 139.0\n",
            "      - 114.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 171.0\n",
            "      - 168.0\n",
            "      - 56.0\n",
            "      - 137.0\n",
            "      - 95.0\n",
            "      - 156.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 130.0\n",
            "      - 150.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 178.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 93.0\n",
            "      - 78.0\n",
            "      - 43.0\n",
            "      - 56.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 153.0\n",
            "      - 22.0\n",
            "      - 167.0\n",
            "      - 153.0\n",
            "      - 166.0\n",
            "      - 190.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 157.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 106.0\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 55.0\n",
            "      - 172.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 168.0\n",
            "      - 38.0\n",
            "      - 56.0\n",
            "      - 172.0\n",
            "      - 171.0\n",
            "      - 164.0\n",
            "      - 33.0\n",
            "      - 84.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13813652053695288\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10816675663385922\n",
            "      mean_inference_ms: 1.305335376522725\n",
            "      mean_raw_obs_processing_ms: 0.24566786441447597\n",
            "  time_since_restore: 980.4936339855194\n",
            "  time_this_iter_s: 10.239465951919556\n",
            "  time_total_s: 980.4936339855194\n",
            "  timers:\n",
            "    learn_throughput: 34193.3\n",
            "    learn_time_ms: 5.849\n",
            "    load_throughput: 1038451.102\n",
            "    load_time_ms: 0.193\n",
            "    training_iteration_time_ms: 210.333\n",
            "    update_time_ms: 3.684\n",
            "  timestamp: 1656954854\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 471000\n",
            "  training_iteration: 91\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:19 (running for 00:16:48.57)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         980.494</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\">  147.78</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            147.78</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:24 (running for 00:16:53.68)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         980.494</td><td style=\"text-align: right;\">471000</td><td style=\"text-align: right;\">  147.78</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            147.78</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 471600\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 471600\n",
            "    num_agent_steps_trained: 471600\n",
            "    num_env_steps_sampled: 471600\n",
            "    num_env_steps_trained: 471600\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-14-24\n",
            "  done: false\n",
            "  episode_len_mean: 148.18\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 148.18\n",
            "  episode_reward_min: 22.0\n",
            "  episodes_this_iter: 5\n",
            "  episodes_total: 4887\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 150.75\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 150.75\n",
            "    episode_reward_min: 38.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 132\n",
            "      - 178\n",
            "      - 38\n",
            "      - 185\n",
            "      - 163\n",
            "      - 102\n",
            "      - 141\n",
            "      - 145\n",
            "      - 200\n",
            "      - 103\n",
            "      - 200\n",
            "      - 200\n",
            "      - 157\n",
            "      - 128\n",
            "      - 164\n",
            "      - 200\n",
            "      - 138\n",
            "      - 137\n",
            "      - 104\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 132.0\n",
            "      - 178.0\n",
            "      - 38.0\n",
            "      - 185.0\n",
            "      - 163.0\n",
            "      - 102.0\n",
            "      - 141.0\n",
            "      - 145.0\n",
            "      - 200.0\n",
            "      - 103.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 157.0\n",
            "      - 128.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 138.0\n",
            "      - 137.0\n",
            "      - 104.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10052539511280083\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07673443137890805\n",
            "      mean_inference_ms: 0.9787397830070826\n",
            "      mean_raw_obs_processing_ms: 0.10836669478505408\n",
            "    timesteps_this_iter: 3015\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 119.15750885009766\n",
            "          policy_loss: 109.30885314941406\n",
            "          var_gnorm: 28.028533935546875\n",
            "          vf_explained_var: 0.10437339544296265\n",
            "          vf_loss: 17415.66796875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 471600\n",
            "    num_agent_steps_trained: 471600\n",
            "    num_env_steps_sampled: 471600\n",
            "    num_env_steps_trained: 471600\n",
            "  iterations_since_restore: 92\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 471600\n",
            "  num_agent_steps_trained: 471600\n",
            "  num_env_steps_sampled: 471600\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 471600\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.83999999999999\n",
            "    ram_util_percent: 21.806666666666672\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13816503857919232\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10813470416392551\n",
            "    mean_inference_ms: 1.3054816016056217\n",
            "    mean_raw_obs_processing_ms: 0.24563952191169813\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 148.18\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 148.18\n",
            "    episode_reward_min: 22.0\n",
            "    episodes_this_iter: 5\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 159\n",
            "      - 200\n",
            "      - 200\n",
            "      - 173\n",
            "      - 36\n",
            "      - 200\n",
            "      - 28\n",
            "      - 40\n",
            "      - 106\n",
            "      - 129\n",
            "      - 193\n",
            "      - 200\n",
            "      - 107\n",
            "      - 165\n",
            "      - 200\n",
            "      - 102\n",
            "      - 139\n",
            "      - 114\n",
            "      - 110\n",
            "      - 200\n",
            "      - 36\n",
            "      - 171\n",
            "      - 168\n",
            "      - 56\n",
            "      - 137\n",
            "      - 95\n",
            "      - 156\n",
            "      - 179\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 158\n",
            "      - 200\n",
            "      - 200\n",
            "      - 172\n",
            "      - 188\n",
            "      - 200\n",
            "      - 195\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 130\n",
            "      - 150\n",
            "      - 142\n",
            "      - 200\n",
            "      - 136\n",
            "      - 200\n",
            "      - 30\n",
            "      - 200\n",
            "      - 200\n",
            "      - 154\n",
            "      - 178\n",
            "      - 93\n",
            "      - 200\n",
            "      - 200\n",
            "      - 93\n",
            "      - 78\n",
            "      - 43\n",
            "      - 56\n",
            "      - 200\n",
            "      - 200\n",
            "      - 152\n",
            "      - 153\n",
            "      - 22\n",
            "      - 167\n",
            "      - 153\n",
            "      - 166\n",
            "      - 190\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 189\n",
            "      - 157\n",
            "      - 99\n",
            "      - 200\n",
            "      - 106\n",
            "      - 135\n",
            "      - 200\n",
            "      - 131\n",
            "      - 55\n",
            "      - 172\n",
            "      - 63\n",
            "      - 200\n",
            "      - 126\n",
            "      - 168\n",
            "      - 38\n",
            "      - 56\n",
            "      - 172\n",
            "      - 171\n",
            "      - 164\n",
            "      - 33\n",
            "      - 84\n",
            "      - 153\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 61\n",
            "      - 140\n",
            "      - 147\n",
            "      episode_reward:\n",
            "      - 159.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 173.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 28.0\n",
            "      - 40.0\n",
            "      - 106.0\n",
            "      - 129.0\n",
            "      - 193.0\n",
            "      - 200.0\n",
            "      - 107.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 102.0\n",
            "      - 139.0\n",
            "      - 114.0\n",
            "      - 110.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 171.0\n",
            "      - 168.0\n",
            "      - 56.0\n",
            "      - 137.0\n",
            "      - 95.0\n",
            "      - 156.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 130.0\n",
            "      - 150.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 30.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 154.0\n",
            "      - 178.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 93.0\n",
            "      - 78.0\n",
            "      - 43.0\n",
            "      - 56.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 153.0\n",
            "      - 22.0\n",
            "      - 167.0\n",
            "      - 153.0\n",
            "      - 166.0\n",
            "      - 190.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 157.0\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 106.0\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 55.0\n",
            "      - 172.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 168.0\n",
            "      - 38.0\n",
            "      - 56.0\n",
            "      - 172.0\n",
            "      - 171.0\n",
            "      - 164.0\n",
            "      - 33.0\n",
            "      - 84.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 140.0\n",
            "      - 147.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13816503857919232\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10813470416392551\n",
            "      mean_inference_ms: 1.3054816016056217\n",
            "      mean_raw_obs_processing_ms: 0.24563952191169813\n",
            "  time_since_restore: 991.372992515564\n",
            "  time_this_iter_s: 10.879358530044556\n",
            "  time_total_s: 991.372992515564\n",
            "  timers:\n",
            "    learn_throughput: 31451.579\n",
            "    learn_time_ms: 6.359\n",
            "    load_throughput: 984000.938\n",
            "    load_time_ms: 0.203\n",
            "    training_iteration_time_ms: 210.49\n",
            "    update_time_ms: 4.001\n",
            "  timestamp: 1656954864\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 471600\n",
            "  training_iteration: 92\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:30 (running for 00:16:59.49)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         991.373</td><td style=\"text-align: right;\">471600</td><td style=\"text-align: right;\">  148.18</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            148.18</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:35 (running for 00:17:04.60)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         991.373</td><td style=\"text-align: right;\">471600</td><td style=\"text-align: right;\">  148.18</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">            148.18</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 481200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 481200\n",
            "    num_agent_steps_trained: 481200\n",
            "    num_env_steps_sampled: 481200\n",
            "    num_env_steps_trained: 481200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-14-35\n",
            "  done: false\n",
            "  episode_len_mean: 132.95\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 132.95\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 73\n",
            "  episodes_total: 4960\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 120.92791748046875\n",
            "          policy_loss: -42.82810974121094\n",
            "          var_gnorm: 28.073652267456055\n",
            "          vf_explained_var: 0.13164383172988892\n",
            "          vf_loss: 18305.06640625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 481200\n",
            "    num_agent_steps_trained: 481200\n",
            "    num_env_steps_sampled: 481200\n",
            "    num_env_steps_trained: 481200\n",
            "  iterations_since_restore: 93\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 481200\n",
            "  num_agent_steps_trained: 481200\n",
            "  num_env_steps_sampled: 481200\n",
            "  num_env_steps_sampled_this_iter: 9600\n",
            "  num_env_steps_trained: 481200\n",
            "  num_env_steps_trained_this_iter: 9600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 93.38000000000002\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13819758256320966\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10806572064529565\n",
            "    mean_inference_ms: 1.3049518148335049\n",
            "    mean_raw_obs_processing_ms: 0.24544889191618785\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 132.95\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 132.95\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 73\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 99\n",
            "      - 200\n",
            "      - 106\n",
            "      - 135\n",
            "      - 200\n",
            "      - 131\n",
            "      - 55\n",
            "      - 172\n",
            "      - 63\n",
            "      - 200\n",
            "      - 126\n",
            "      - 168\n",
            "      - 38\n",
            "      - 56\n",
            "      - 172\n",
            "      - 171\n",
            "      - 164\n",
            "      - 33\n",
            "      - 84\n",
            "      - 153\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 61\n",
            "      - 140\n",
            "      - 147\n",
            "      - 153\n",
            "      - 70\n",
            "      - 139\n",
            "      - 200\n",
            "      - 52\n",
            "      - 183\n",
            "      - 181\n",
            "      - 200\n",
            "      - 200\n",
            "      - 136\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 164\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 150\n",
            "      - 60\n",
            "      - 128\n",
            "      - 200\n",
            "      - 79\n",
            "      - 129\n",
            "      - 79\n",
            "      - 172\n",
            "      - 52\n",
            "      - 182\n",
            "      - 12\n",
            "      - 200\n",
            "      - 43\n",
            "      - 159\n",
            "      - 139\n",
            "      - 85\n",
            "      - 62\n",
            "      - 120\n",
            "      - 128\n",
            "      - 169\n",
            "      - 36\n",
            "      - 106\n",
            "      - 177\n",
            "      - 200\n",
            "      - 73\n",
            "      - 151\n",
            "      - 191\n",
            "      - 149\n",
            "      - 171\n",
            "      - 109\n",
            "      - 87\n",
            "      - 194\n",
            "      - 106\n",
            "      - 200\n",
            "      - 24\n",
            "      - 200\n",
            "      - 82\n",
            "      - 40\n",
            "      - 188\n",
            "      - 200\n",
            "      - 196\n",
            "      - 97\n",
            "      - 86\n",
            "      - 112\n",
            "      - 53\n",
            "      - 73\n",
            "      - 130\n",
            "      - 19\n",
            "      - 110\n",
            "      - 49\n",
            "      - 172\n",
            "      - 135\n",
            "      - 27\n",
            "      - 75\n",
            "      - 200\n",
            "      - 177\n",
            "      episode_reward:\n",
            "      - 99.0\n",
            "      - 200.0\n",
            "      - 106.0\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 55.0\n",
            "      - 172.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 168.0\n",
            "      - 38.0\n",
            "      - 56.0\n",
            "      - 172.0\n",
            "      - 171.0\n",
            "      - 164.0\n",
            "      - 33.0\n",
            "      - 84.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 140.0\n",
            "      - 147.0\n",
            "      - 153.0\n",
            "      - 70.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 183.0\n",
            "      - 181.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 150.0\n",
            "      - 60.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 79.0\n",
            "      - 129.0\n",
            "      - 79.0\n",
            "      - 172.0\n",
            "      - 52.0\n",
            "      - 182.0\n",
            "      - 12.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 159.0\n",
            "      - 139.0\n",
            "      - 85.0\n",
            "      - 62.0\n",
            "      - 120.0\n",
            "      - 128.0\n",
            "      - 169.0\n",
            "      - 36.0\n",
            "      - 106.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 73.0\n",
            "      - 151.0\n",
            "      - 191.0\n",
            "      - 149.0\n",
            "      - 171.0\n",
            "      - 109.0\n",
            "      - 87.0\n",
            "      - 194.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 82.0\n",
            "      - 40.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 97.0\n",
            "      - 86.0\n",
            "      - 112.0\n",
            "      - 53.0\n",
            "      - 73.0\n",
            "      - 130.0\n",
            "      - 19.0\n",
            "      - 110.0\n",
            "      - 49.0\n",
            "      - 172.0\n",
            "      - 135.0\n",
            "      - 27.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13819758256320966\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10806572064529565\n",
            "      mean_inference_ms: 1.3049518148335049\n",
            "      mean_raw_obs_processing_ms: 0.24544889191618785\n",
            "  time_since_restore: 1001.4611575603485\n",
            "  time_this_iter_s: 10.088165044784546\n",
            "  time_total_s: 1001.4611575603485\n",
            "  timers:\n",
            "    learn_throughput: 32071.816\n",
            "    learn_time_ms: 6.236\n",
            "    load_throughput: 1064274.042\n",
            "    load_time_ms: 0.188\n",
            "    training_iteration_time_ms: 215.5\n",
            "    update_time_ms: 4.066\n",
            "  timestamp: 1656954875\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 481200\n",
            "  training_iteration: 93\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:40 (running for 00:17:09.65)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1001.46</td><td style=\"text-align: right;\">481200</td><td style=\"text-align: right;\">  132.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            132.95</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:45 (running for 00:17:14.76)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1001.46</td><td style=\"text-align: right;\">481200</td><td style=\"text-align: right;\">  132.95</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            132.95</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 481800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 481800\n",
            "    num_agent_steps_trained: 481800\n",
            "    num_env_steps_sampled: 481800\n",
            "    num_env_steps_trained: 481800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-14-46\n",
            "  done: false\n",
            "  episode_len_mean: 134.08\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 134.08\n",
            "  episode_reward_min: 12.0\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 4963\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 134.65\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 134.65\n",
            "    episode_reward_min: 20.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 156\n",
            "      - 51\n",
            "      - 124\n",
            "      - 91\n",
            "      - 136\n",
            "      - 20\n",
            "      - 115\n",
            "      - 51\n",
            "      - 155\n",
            "      - 170\n",
            "      - 200\n",
            "      - 43\n",
            "      - 200\n",
            "      - 179\n",
            "      - 200\n",
            "      - 195\n",
            "      - 200\n",
            "      - 152\n",
            "      - 131\n",
            "      - 124\n",
            "      episode_reward:\n",
            "      - 156.0\n",
            "      - 51.0\n",
            "      - 124.0\n",
            "      - 91.0\n",
            "      - 136.0\n",
            "      - 20.0\n",
            "      - 115.0\n",
            "      - 51.0\n",
            "      - 155.0\n",
            "      - 170.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 200.0\n",
            "      - 179.0\n",
            "      - 200.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 152.0\n",
            "      - 131.0\n",
            "      - 124.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10048356821147184\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07669378939141339\n",
            "      mean_inference_ms: 0.9784085707685943\n",
            "      mean_raw_obs_processing_ms: 0.10827284534574104\n",
            "    timesteps_this_iter: 2693\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 124.33061218261719\n",
            "          policy_loss: 88.4610595703125\n",
            "          var_gnorm: 28.076154708862305\n",
            "          vf_explained_var: 0.019639790058135986\n",
            "          vf_loss: 15104.966796875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 481800\n",
            "    num_agent_steps_trained: 481800\n",
            "    num_env_steps_sampled: 481800\n",
            "    num_env_steps_trained: 481800\n",
            "  iterations_since_restore: 94\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 481800\n",
            "  num_agent_steps_trained: 481800\n",
            "  num_env_steps_sampled: 481800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 481800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.4875\n",
            "    ram_util_percent: 21.8\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1382068872134644\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10805454477901211\n",
            "    mean_inference_ms: 1.3049834470509858\n",
            "    mean_raw_obs_processing_ms: 0.24543597102101347\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 134.08\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 134.08\n",
            "    episode_reward_min: 12.0\n",
            "    episodes_this_iter: 3\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 135\n",
            "      - 200\n",
            "      - 131\n",
            "      - 55\n",
            "      - 172\n",
            "      - 63\n",
            "      - 200\n",
            "      - 126\n",
            "      - 168\n",
            "      - 38\n",
            "      - 56\n",
            "      - 172\n",
            "      - 171\n",
            "      - 164\n",
            "      - 33\n",
            "      - 84\n",
            "      - 153\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 61\n",
            "      - 140\n",
            "      - 147\n",
            "      - 153\n",
            "      - 70\n",
            "      - 139\n",
            "      - 200\n",
            "      - 52\n",
            "      - 183\n",
            "      - 181\n",
            "      - 200\n",
            "      - 200\n",
            "      - 136\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 164\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 150\n",
            "      - 60\n",
            "      - 128\n",
            "      - 200\n",
            "      - 79\n",
            "      - 129\n",
            "      - 79\n",
            "      - 172\n",
            "      - 52\n",
            "      - 182\n",
            "      - 12\n",
            "      - 200\n",
            "      - 43\n",
            "      - 159\n",
            "      - 139\n",
            "      - 85\n",
            "      - 62\n",
            "      - 120\n",
            "      - 128\n",
            "      - 169\n",
            "      - 36\n",
            "      - 106\n",
            "      - 177\n",
            "      - 200\n",
            "      - 73\n",
            "      - 151\n",
            "      - 191\n",
            "      - 149\n",
            "      - 171\n",
            "      - 109\n",
            "      - 87\n",
            "      - 194\n",
            "      - 106\n",
            "      - 200\n",
            "      - 24\n",
            "      - 200\n",
            "      - 82\n",
            "      - 40\n",
            "      - 188\n",
            "      - 200\n",
            "      - 196\n",
            "      - 97\n",
            "      - 86\n",
            "      - 112\n",
            "      - 53\n",
            "      - 73\n",
            "      - 130\n",
            "      - 19\n",
            "      - 110\n",
            "      - 49\n",
            "      - 172\n",
            "      - 135\n",
            "      - 27\n",
            "      - 75\n",
            "      - 200\n",
            "      - 177\n",
            "      - 118\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 135.0\n",
            "      - 200.0\n",
            "      - 131.0\n",
            "      - 55.0\n",
            "      - 172.0\n",
            "      - 63.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 168.0\n",
            "      - 38.0\n",
            "      - 56.0\n",
            "      - 172.0\n",
            "      - 171.0\n",
            "      - 164.0\n",
            "      - 33.0\n",
            "      - 84.0\n",
            "      - 153.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 140.0\n",
            "      - 147.0\n",
            "      - 153.0\n",
            "      - 70.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 52.0\n",
            "      - 183.0\n",
            "      - 181.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 164.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 150.0\n",
            "      - 60.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 79.0\n",
            "      - 129.0\n",
            "      - 79.0\n",
            "      - 172.0\n",
            "      - 52.0\n",
            "      - 182.0\n",
            "      - 12.0\n",
            "      - 200.0\n",
            "      - 43.0\n",
            "      - 159.0\n",
            "      - 139.0\n",
            "      - 85.0\n",
            "      - 62.0\n",
            "      - 120.0\n",
            "      - 128.0\n",
            "      - 169.0\n",
            "      - 36.0\n",
            "      - 106.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 73.0\n",
            "      - 151.0\n",
            "      - 191.0\n",
            "      - 149.0\n",
            "      - 171.0\n",
            "      - 109.0\n",
            "      - 87.0\n",
            "      - 194.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 82.0\n",
            "      - 40.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 97.0\n",
            "      - 86.0\n",
            "      - 112.0\n",
            "      - 53.0\n",
            "      - 73.0\n",
            "      - 130.0\n",
            "      - 19.0\n",
            "      - 110.0\n",
            "      - 49.0\n",
            "      - 172.0\n",
            "      - 135.0\n",
            "      - 27.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1382068872134644\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10805454477901211\n",
            "      mean_inference_ms: 1.3049834470509858\n",
            "      mean_raw_obs_processing_ms: 0.24543597102101347\n",
            "  time_since_restore: 1012.477858543396\n",
            "  time_this_iter_s: 11.016700983047485\n",
            "  time_total_s: 1012.477858543396\n",
            "  timers:\n",
            "    learn_throughput: 31597.057\n",
            "    learn_time_ms: 6.33\n",
            "    load_throughput: 1049363.022\n",
            "    load_time_ms: 0.191\n",
            "    training_iteration_time_ms: 220.141\n",
            "    update_time_ms: 3.753\n",
            "  timestamp: 1656954886\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 481800\n",
            "  training_iteration: 94\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:51 (running for 00:17:20.70)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1012.48</td><td style=\"text-align: right;\">481800</td><td style=\"text-align: right;\">  134.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            134.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:14:56 (running for 00:17:25.79)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1012.48</td><td style=\"text-align: right;\">481800</td><td style=\"text-align: right;\">  134.08</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  12</td><td style=\"text-align: right;\">            134.08</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 491200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 491200\n",
            "    num_agent_steps_trained: 491200\n",
            "    num_env_steps_sampled: 491200\n",
            "    num_env_steps_trained: 491200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-14-56\n",
            "  done: false\n",
            "  episode_len_mean: 137.8\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 137.8\n",
            "  episode_reward_min: 19.0\n",
            "  episodes_this_iter: 65\n",
            "  episodes_total: 5028\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 123.77445983886719\n",
            "          policy_loss: 266.4149475097656\n",
            "          var_gnorm: 28.124683380126953\n",
            "          vf_explained_var: 0.7248280048370361\n",
            "          vf_loss: 5367.17041015625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 491200\n",
            "    num_agent_steps_trained: 491200\n",
            "    num_env_steps_sampled: 491200\n",
            "    num_env_steps_trained: 491200\n",
            "  iterations_since_restore: 95\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 491200\n",
            "  num_agent_steps_trained: 491200\n",
            "  num_env_steps_sampled: 491200\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 491200\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.87857142857145\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13815101738513563\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10810475655661435\n",
            "    mean_inference_ms: 1.3044873872847387\n",
            "    mean_raw_obs_processing_ms: 0.24539073823201307\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 137.8\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 137.8\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 65\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 73\n",
            "      - 151\n",
            "      - 191\n",
            "      - 149\n",
            "      - 171\n",
            "      - 109\n",
            "      - 87\n",
            "      - 194\n",
            "      - 106\n",
            "      - 200\n",
            "      - 24\n",
            "      - 200\n",
            "      - 82\n",
            "      - 40\n",
            "      - 188\n",
            "      - 200\n",
            "      - 196\n",
            "      - 97\n",
            "      - 86\n",
            "      - 112\n",
            "      - 53\n",
            "      - 73\n",
            "      - 130\n",
            "      - 19\n",
            "      - 110\n",
            "      - 49\n",
            "      - 172\n",
            "      - 135\n",
            "      - 27\n",
            "      - 75\n",
            "      - 200\n",
            "      - 177\n",
            "      - 118\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 22\n",
            "      - 142\n",
            "      - 134\n",
            "      - 70\n",
            "      - 144\n",
            "      - 121\n",
            "      - 68\n",
            "      - 156\n",
            "      - 149\n",
            "      - 42\n",
            "      - 154\n",
            "      - 200\n",
            "      - 147\n",
            "      - 99\n",
            "      - 142\n",
            "      - 147\n",
            "      - 25\n",
            "      - 173\n",
            "      - 200\n",
            "      - 143\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 142\n",
            "      - 75\n",
            "      - 200\n",
            "      - 170\n",
            "      - 32\n",
            "      - 196\n",
            "      - 200\n",
            "      - 200\n",
            "      - 125\n",
            "      - 200\n",
            "      - 143\n",
            "      - 200\n",
            "      - 150\n",
            "      - 153\n",
            "      - 127\n",
            "      - 130\n",
            "      - 185\n",
            "      - 200\n",
            "      - 182\n",
            "      - 24\n",
            "      - 151\n",
            "      - 190\n",
            "      - 33\n",
            "      - 140\n",
            "      - 100\n",
            "      - 198\n",
            "      - 200\n",
            "      - 125\n",
            "      - 24\n",
            "      - 200\n",
            "      - 62\n",
            "      - 140\n",
            "      - 104\n",
            "      - 200\n",
            "      - 135\n",
            "      - 162\n",
            "      - 195\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 73.0\n",
            "      - 151.0\n",
            "      - 191.0\n",
            "      - 149.0\n",
            "      - 171.0\n",
            "      - 109.0\n",
            "      - 87.0\n",
            "      - 194.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 82.0\n",
            "      - 40.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 97.0\n",
            "      - 86.0\n",
            "      - 112.0\n",
            "      - 53.0\n",
            "      - 73.0\n",
            "      - 130.0\n",
            "      - 19.0\n",
            "      - 110.0\n",
            "      - 49.0\n",
            "      - 172.0\n",
            "      - 135.0\n",
            "      - 27.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 22.0\n",
            "      - 142.0\n",
            "      - 134.0\n",
            "      - 70.0\n",
            "      - 144.0\n",
            "      - 121.0\n",
            "      - 68.0\n",
            "      - 156.0\n",
            "      - 149.0\n",
            "      - 42.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 99.0\n",
            "      - 142.0\n",
            "      - 147.0\n",
            "      - 25.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 170.0\n",
            "      - 32.0\n",
            "      - 196.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 150.0\n",
            "      - 153.0\n",
            "      - 127.0\n",
            "      - 130.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 24.0\n",
            "      - 151.0\n",
            "      - 190.0\n",
            "      - 33.0\n",
            "      - 140.0\n",
            "      - 100.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 62.0\n",
            "      - 140.0\n",
            "      - 104.0\n",
            "      - 200.0\n",
            "      - 135.0\n",
            "      - 162.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13815101738513563\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10810475655661435\n",
            "      mean_inference_ms: 1.3044873872847387\n",
            "      mean_raw_obs_processing_ms: 0.24539073823201307\n",
            "  time_since_restore: 1022.5338339805603\n",
            "  time_this_iter_s: 10.055975437164307\n",
            "  time_total_s: 1022.5338339805603\n",
            "  timers:\n",
            "    learn_throughput: 34039.564\n",
            "    learn_time_ms: 5.876\n",
            "    load_throughput: 999953.272\n",
            "    load_time_ms: 0.2\n",
            "    training_iteration_time_ms: 205.146\n",
            "    update_time_ms: 3.8\n",
            "  timestamp: 1656954896\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 491200\n",
            "  training_iteration: 95\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:01 (running for 00:17:30.83)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1022.53</td><td style=\"text-align: right;\">491200</td><td style=\"text-align: right;\">   137.8</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             137.8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:06 (running for 00:17:35.93)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1022.53</td><td style=\"text-align: right;\">491200</td><td style=\"text-align: right;\">   137.8</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">             137.8</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 491800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 491800\n",
            "    num_agent_steps_trained: 491800\n",
            "    num_env_steps_sampled: 491800\n",
            "    num_env_steps_trained: 491800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-15-09\n",
            "  done: false\n",
            "  episode_len_mean: 138.31\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 138.31\n",
            "  episode_reward_min: 19.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 5032\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 160.65\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 160.65\n",
            "    episode_reward_min: 36.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 148\n",
            "      - 200\n",
            "      - 128\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 174\n",
            "      - 144\n",
            "      - 152\n",
            "      - 159\n",
            "      - 161\n",
            "      - 36\n",
            "      - 132\n",
            "      - 200\n",
            "      - 165\n",
            "      - 200\n",
            "      - 200\n",
            "      - 126\n",
            "      - 164\n",
            "      - 124\n",
            "      episode_reward:\n",
            "      - 148.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 174.0\n",
            "      - 144.0\n",
            "      - 152.0\n",
            "      - 159.0\n",
            "      - 161.0\n",
            "      - 36.0\n",
            "      - 132.0\n",
            "      - 200.0\n",
            "      - 165.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 164.0\n",
            "      - 124.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10060678149748478\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.0768118036958119\n",
            "      mean_inference_ms: 0.9800561455258423\n",
            "      mean_raw_obs_processing_ms: 0.10832723585501039\n",
            "    timesteps_this_iter: 3213\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 127.97883605957031\n",
            "          policy_loss: 382.1749572753906\n",
            "          var_gnorm: 28.12702178955078\n",
            "          vf_explained_var: 0.8493669033050537\n",
            "          vf_loss: 3073.427734375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 491800\n",
            "    num_agent_steps_trained: 491800\n",
            "    num_env_steps_sampled: 491800\n",
            "    num_env_steps_trained: 491800\n",
            "  iterations_since_restore: 96\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 491800\n",
            "  num_agent_steps_trained: 491800\n",
            "  num_env_steps_sampled: 491800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 491800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 74.91578947368421\n",
            "    ram_util_percent: 21.810526315789474\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.138172012490869\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10808708906955307\n",
            "    mean_inference_ms: 1.3046217330126026\n",
            "    mean_raw_obs_processing_ms: 0.24537555236085706\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 138.31\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 138.31\n",
            "    episode_reward_min: 19.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 171\n",
            "      - 109\n",
            "      - 87\n",
            "      - 194\n",
            "      - 106\n",
            "      - 200\n",
            "      - 24\n",
            "      - 200\n",
            "      - 82\n",
            "      - 40\n",
            "      - 188\n",
            "      - 200\n",
            "      - 196\n",
            "      - 97\n",
            "      - 86\n",
            "      - 112\n",
            "      - 53\n",
            "      - 73\n",
            "      - 130\n",
            "      - 19\n",
            "      - 110\n",
            "      - 49\n",
            "      - 172\n",
            "      - 135\n",
            "      - 27\n",
            "      - 75\n",
            "      - 200\n",
            "      - 177\n",
            "      - 118\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 126\n",
            "      - 200\n",
            "      - 200\n",
            "      - 22\n",
            "      - 142\n",
            "      - 134\n",
            "      - 70\n",
            "      - 144\n",
            "      - 121\n",
            "      - 68\n",
            "      - 156\n",
            "      - 149\n",
            "      - 42\n",
            "      - 154\n",
            "      - 200\n",
            "      - 147\n",
            "      - 99\n",
            "      - 142\n",
            "      - 147\n",
            "      - 25\n",
            "      - 173\n",
            "      - 200\n",
            "      - 143\n",
            "      - 200\n",
            "      - 189\n",
            "      - 200\n",
            "      - 142\n",
            "      - 75\n",
            "      - 200\n",
            "      - 170\n",
            "      - 32\n",
            "      - 196\n",
            "      - 200\n",
            "      - 200\n",
            "      - 125\n",
            "      - 200\n",
            "      - 143\n",
            "      - 200\n",
            "      - 150\n",
            "      - 153\n",
            "      - 127\n",
            "      - 130\n",
            "      - 185\n",
            "      - 200\n",
            "      - 182\n",
            "      - 24\n",
            "      - 151\n",
            "      - 190\n",
            "      - 33\n",
            "      - 140\n",
            "      - 100\n",
            "      - 198\n",
            "      - 200\n",
            "      - 125\n",
            "      - 24\n",
            "      - 200\n",
            "      - 62\n",
            "      - 140\n",
            "      - 104\n",
            "      - 200\n",
            "      - 135\n",
            "      - 162\n",
            "      - 195\n",
            "      - 200\n",
            "      - 109\n",
            "      - 167\n",
            "      - 139\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 171.0\n",
            "      - 109.0\n",
            "      - 87.0\n",
            "      - 194.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 82.0\n",
            "      - 40.0\n",
            "      - 188.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 97.0\n",
            "      - 86.0\n",
            "      - 112.0\n",
            "      - 53.0\n",
            "      - 73.0\n",
            "      - 130.0\n",
            "      - 19.0\n",
            "      - 110.0\n",
            "      - 49.0\n",
            "      - 172.0\n",
            "      - 135.0\n",
            "      - 27.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 177.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 126.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 22.0\n",
            "      - 142.0\n",
            "      - 134.0\n",
            "      - 70.0\n",
            "      - 144.0\n",
            "      - 121.0\n",
            "      - 68.0\n",
            "      - 156.0\n",
            "      - 149.0\n",
            "      - 42.0\n",
            "      - 154.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 99.0\n",
            "      - 142.0\n",
            "      - 147.0\n",
            "      - 25.0\n",
            "      - 173.0\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 200.0\n",
            "      - 142.0\n",
            "      - 75.0\n",
            "      - 200.0\n",
            "      - 170.0\n",
            "      - 32.0\n",
            "      - 196.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 150.0\n",
            "      - 153.0\n",
            "      - 127.0\n",
            "      - 130.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 24.0\n",
            "      - 151.0\n",
            "      - 190.0\n",
            "      - 33.0\n",
            "      - 140.0\n",
            "      - 100.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 62.0\n",
            "      - 140.0\n",
            "      - 104.0\n",
            "      - 200.0\n",
            "      - 135.0\n",
            "      - 162.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 167.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.138172012490869\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10808708906955307\n",
            "      mean_inference_ms: 1.3046217330126026\n",
            "      mean_raw_obs_processing_ms: 0.24537555236085706\n",
            "  time_since_restore: 1035.7303977012634\n",
            "  time_this_iter_s: 13.196563720703125\n",
            "  time_total_s: 1035.7303977012634\n",
            "  timers:\n",
            "    learn_throughput: 28434.899\n",
            "    learn_time_ms: 7.034\n",
            "    load_throughput: 916587.413\n",
            "    load_time_ms: 0.218\n",
            "    training_iteration_time_ms: 232.304\n",
            "    update_time_ms: 4.777\n",
            "  timestamp: 1656954909\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 491800\n",
            "  training_iteration: 96\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:14 (running for 00:17:44.06)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1035.73</td><td style=\"text-align: right;\">491800</td><td style=\"text-align: right;\">  138.31</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            138.31</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:19 (running for 00:17:49.14)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1035.73</td><td style=\"text-align: right;\">491800</td><td style=\"text-align: right;\">  138.31</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  19</td><td style=\"text-align: right;\">            138.31</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 501200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 501200\n",
            "    num_agent_steps_trained: 501200\n",
            "    num_env_steps_sampled: 501200\n",
            "    num_env_steps_trained: 501200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-15-19\n",
            "  done: false\n",
            "  episode_len_mean: 142.67\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 142.67\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 67\n",
            "  episodes_total: 5099\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.0\n",
            "          model: {}\n",
            "          policy_entropy: 116.77161407470703\n",
            "          policy_loss: 414.05194091796875\n",
            "          var_gnorm: 28.176950454711914\n",
            "          vf_explained_var: 0.9332185387611389\n",
            "          vf_loss: 2298.01416015625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 501200\n",
            "    num_agent_steps_trained: 501200\n",
            "    num_env_steps_sampled: 501200\n",
            "    num_env_steps_trained: 501200\n",
            "  iterations_since_restore: 97\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 501200\n",
            "  num_agent_steps_trained: 501200\n",
            "  num_env_steps_sampled: 501200\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 501200\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.00714285714285\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1381616707242275\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10817292657952382\n",
            "    mean_inference_ms: 1.304595091057942\n",
            "    mean_raw_obs_processing_ms: 0.24532699814833592\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 142.67\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 142.67\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 67\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 143\n",
            "      - 200\n",
            "      - 150\n",
            "      - 153\n",
            "      - 127\n",
            "      - 130\n",
            "      - 185\n",
            "      - 200\n",
            "      - 182\n",
            "      - 24\n",
            "      - 151\n",
            "      - 190\n",
            "      - 33\n",
            "      - 140\n",
            "      - 100\n",
            "      - 198\n",
            "      - 200\n",
            "      - 125\n",
            "      - 24\n",
            "      - 200\n",
            "      - 62\n",
            "      - 140\n",
            "      - 104\n",
            "      - 200\n",
            "      - 135\n",
            "      - 162\n",
            "      - 195\n",
            "      - 200\n",
            "      - 109\n",
            "      - 167\n",
            "      - 139\n",
            "      - 200\n",
            "      - 172\n",
            "      - 161\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 40\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 34\n",
            "      - 97\n",
            "      - 160\n",
            "      - 193\n",
            "      - 152\n",
            "      - 163\n",
            "      - 181\n",
            "      - 64\n",
            "      - 93\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 36\n",
            "      - 106\n",
            "      - 200\n",
            "      - 40\n",
            "      - 39\n",
            "      - 200\n",
            "      - 172\n",
            "      - 168\n",
            "      - 166\n",
            "      - 109\n",
            "      - 200\n",
            "      - 61\n",
            "      - 32\n",
            "      - 172\n",
            "      - 153\n",
            "      - 25\n",
            "      - 165\n",
            "      - 141\n",
            "      - 94\n",
            "      - 22\n",
            "      - 200\n",
            "      - 88\n",
            "      - 32\n",
            "      - 147\n",
            "      - 108\n",
            "      - 200\n",
            "      - 193\n",
            "      - 185\n",
            "      - 136\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 15\n",
            "      - 118\n",
            "      - 200\n",
            "      - 71\n",
            "      - 200\n",
            "      - 61\n",
            "      - 200\n",
            "      - 81\n",
            "      - 186\n",
            "      - 200\n",
            "      - 66\n",
            "      - 200\n",
            "      - 200\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 143.0\n",
            "      - 200.0\n",
            "      - 150.0\n",
            "      - 153.0\n",
            "      - 127.0\n",
            "      - 130.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 24.0\n",
            "      - 151.0\n",
            "      - 190.0\n",
            "      - 33.0\n",
            "      - 140.0\n",
            "      - 100.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 62.0\n",
            "      - 140.0\n",
            "      - 104.0\n",
            "      - 200.0\n",
            "      - 135.0\n",
            "      - 162.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 167.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 40.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 34.0\n",
            "      - 97.0\n",
            "      - 160.0\n",
            "      - 193.0\n",
            "      - 152.0\n",
            "      - 163.0\n",
            "      - 181.0\n",
            "      - 64.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 40.0\n",
            "      - 39.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 168.0\n",
            "      - 166.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 32.0\n",
            "      - 172.0\n",
            "      - 153.0\n",
            "      - 25.0\n",
            "      - 165.0\n",
            "      - 141.0\n",
            "      - 94.0\n",
            "      - 22.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 32.0\n",
            "      - 147.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 185.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1381616707242275\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10817292657952382\n",
            "      mean_inference_ms: 1.304595091057942\n",
            "      mean_raw_obs_processing_ms: 0.24532699814833592\n",
            "  time_since_restore: 1045.7802805900574\n",
            "  time_this_iter_s: 10.049882888793945\n",
            "  time_total_s: 1045.7802805900574\n",
            "  timers:\n",
            "    learn_throughput: 31205.297\n",
            "    learn_time_ms: 6.409\n",
            "    load_throughput: 902680.297\n",
            "    load_time_ms: 0.222\n",
            "    training_iteration_time_ms: 216.404\n",
            "    update_time_ms: 4.147\n",
            "  timestamp: 1656954919\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 501200\n",
            "  training_iteration: 97\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:24 (running for 00:17:54.19)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1045.78</td><td style=\"text-align: right;\">501200</td><td style=\"text-align: right;\">  142.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            142.67</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:29 (running for 00:17:59.28)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1045.78</td><td style=\"text-align: right;\">501200</td><td style=\"text-align: right;\">  142.67</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            142.67</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 501800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 501800\n",
            "    num_agent_steps_trained: 501800\n",
            "    num_env_steps_sampled: 501800\n",
            "    num_env_steps_trained: 501800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-15-32\n",
            "  done: false\n",
            "  episode_len_mean: 142.16\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 142.16\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 3\n",
            "  episodes_total: 5102\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 174.35\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 174.35\n",
            "    episode_reward_min: 36.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 200\n",
            "      - 118\n",
            "      - 200\n",
            "      - 147\n",
            "      - 157\n",
            "      - 187\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 196\n",
            "      - 152\n",
            "      - 200\n",
            "      - 176\n",
            "      - 177\n",
            "      - 200\n",
            "      - 36\n",
            "      - 200\n",
            "      - 189\n",
            "      - 160\n",
            "      - 192\n",
            "      episode_reward:\n",
            "      - 200.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 147.0\n",
            "      - 157.0\n",
            "      - 187.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 196.0\n",
            "      - 152.0\n",
            "      - 200.0\n",
            "      - 176.0\n",
            "      - 177.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 189.0\n",
            "      - 160.0\n",
            "      - 192.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10056298853901875\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07676886186009528\n",
            "      mean_inference_ms: 0.9793800998604576\n",
            "      mean_raw_obs_processing_ms: 0.10820135395661983\n",
            "    timesteps_this_iter: 3487\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 124.58077239990234\n",
            "          policy_loss: 482.2642822265625\n",
            "          var_gnorm: 28.180078506469727\n",
            "          vf_explained_var: 0.48334211111068726\n",
            "          vf_loss: 2549.2763671875\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 501800\n",
            "    num_agent_steps_trained: 501800\n",
            "    num_env_steps_sampled: 501800\n",
            "    num_env_steps_trained: 501800\n",
            "  iterations_since_restore: 98\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 501800\n",
            "  num_agent_steps_trained: 501800\n",
            "  num_env_steps_sampled: 501800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 501800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 65.67368421052633\n",
            "    ram_util_percent: 21.8\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13818297299263077\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10815399578452414\n",
            "    mean_inference_ms: 1.3047318326383612\n",
            "    mean_raw_obs_processing_ms: 0.24531167858630065\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 142.16\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 142.16\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 3\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 150\n",
            "      - 153\n",
            "      - 127\n",
            "      - 130\n",
            "      - 185\n",
            "      - 200\n",
            "      - 182\n",
            "      - 24\n",
            "      - 151\n",
            "      - 190\n",
            "      - 33\n",
            "      - 140\n",
            "      - 100\n",
            "      - 198\n",
            "      - 200\n",
            "      - 125\n",
            "      - 24\n",
            "      - 200\n",
            "      - 62\n",
            "      - 140\n",
            "      - 104\n",
            "      - 200\n",
            "      - 135\n",
            "      - 162\n",
            "      - 195\n",
            "      - 200\n",
            "      - 109\n",
            "      - 167\n",
            "      - 139\n",
            "      - 200\n",
            "      - 172\n",
            "      - 161\n",
            "      - 200\n",
            "      - 186\n",
            "      - 200\n",
            "      - 40\n",
            "      - 115\n",
            "      - 200\n",
            "      - 200\n",
            "      - 34\n",
            "      - 97\n",
            "      - 160\n",
            "      - 193\n",
            "      - 152\n",
            "      - 163\n",
            "      - 181\n",
            "      - 64\n",
            "      - 93\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 36\n",
            "      - 106\n",
            "      - 200\n",
            "      - 40\n",
            "      - 39\n",
            "      - 200\n",
            "      - 172\n",
            "      - 168\n",
            "      - 166\n",
            "      - 109\n",
            "      - 200\n",
            "      - 61\n",
            "      - 32\n",
            "      - 172\n",
            "      - 153\n",
            "      - 25\n",
            "      - 165\n",
            "      - 141\n",
            "      - 94\n",
            "      - 22\n",
            "      - 200\n",
            "      - 88\n",
            "      - 32\n",
            "      - 147\n",
            "      - 108\n",
            "      - 200\n",
            "      - 193\n",
            "      - 185\n",
            "      - 136\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 15\n",
            "      - 118\n",
            "      - 200\n",
            "      - 71\n",
            "      - 200\n",
            "      - 61\n",
            "      - 200\n",
            "      - 81\n",
            "      - 186\n",
            "      - 200\n",
            "      - 66\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 149\n",
            "      - 143\n",
            "      episode_reward:\n",
            "      - 150.0\n",
            "      - 153.0\n",
            "      - 127.0\n",
            "      - 130.0\n",
            "      - 185.0\n",
            "      - 200.0\n",
            "      - 182.0\n",
            "      - 24.0\n",
            "      - 151.0\n",
            "      - 190.0\n",
            "      - 33.0\n",
            "      - 140.0\n",
            "      - 100.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 125.0\n",
            "      - 24.0\n",
            "      - 200.0\n",
            "      - 62.0\n",
            "      - 140.0\n",
            "      - 104.0\n",
            "      - 200.0\n",
            "      - 135.0\n",
            "      - 162.0\n",
            "      - 195.0\n",
            "      - 200.0\n",
            "      - 109.0\n",
            "      - 167.0\n",
            "      - 139.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 161.0\n",
            "      - 200.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 40.0\n",
            "      - 115.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 34.0\n",
            "      - 97.0\n",
            "      - 160.0\n",
            "      - 193.0\n",
            "      - 152.0\n",
            "      - 163.0\n",
            "      - 181.0\n",
            "      - 64.0\n",
            "      - 93.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 106.0\n",
            "      - 200.0\n",
            "      - 40.0\n",
            "      - 39.0\n",
            "      - 200.0\n",
            "      - 172.0\n",
            "      - 168.0\n",
            "      - 166.0\n",
            "      - 109.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 32.0\n",
            "      - 172.0\n",
            "      - 153.0\n",
            "      - 25.0\n",
            "      - 165.0\n",
            "      - 141.0\n",
            "      - 94.0\n",
            "      - 22.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 32.0\n",
            "      - 147.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 185.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 143.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13818297299263077\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10815399578452414\n",
            "      mean_inference_ms: 1.3047318326383612\n",
            "      mean_raw_obs_processing_ms: 0.24531167858630065\n",
            "  time_since_restore: 1058.7057168483734\n",
            "  time_this_iter_s: 12.92543625831604\n",
            "  time_total_s: 1058.7057168483734\n",
            "  timers:\n",
            "    learn_throughput: 32587.243\n",
            "    learn_time_ms: 6.137\n",
            "    load_throughput: 881990.117\n",
            "    load_time_ms: 0.227\n",
            "    training_iteration_time_ms: 219.824\n",
            "    update_time_ms: 4.062\n",
            "  timestamp: 1656954932\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 501800\n",
            "  training_iteration: 98\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:37 (running for 00:18:07.16)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1058.71</td><td style=\"text-align: right;\">501800</td><td style=\"text-align: right;\">  142.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            142.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:42 (running for 00:18:12.26)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1058.71</td><td style=\"text-align: right;\">501800</td><td style=\"text-align: right;\">  142.16</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            142.16</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 511200\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 511200\n",
            "    num_agent_steps_trained: 511200\n",
            "    num_env_steps_sampled: 511200\n",
            "    num_env_steps_trained: 511200\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-15-42\n",
            "  done: false\n",
            "  episode_len_mean: 142.83\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 142.83\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 66\n",
            "  episodes_total: 5168\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 39.999996185302734\n",
            "          model: {}\n",
            "          policy_entropy: 119.31948852539062\n",
            "          policy_loss: 558.6322631835938\n",
            "          var_gnorm: 28.223634719848633\n",
            "          vf_explained_var: 0.03114795684814453\n",
            "          vf_loss: 4299.82275390625\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 511200\n",
            "    num_agent_steps_trained: 511200\n",
            "    num_env_steps_sampled: 511200\n",
            "    num_env_steps_trained: 511200\n",
            "  iterations_since_restore: 99\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 511200\n",
            "  num_agent_steps_trained: 511200\n",
            "  num_env_steps_sampled: 511200\n",
            "  num_env_steps_sampled_this_iter: 9400\n",
            "  num_env_steps_trained: 511200\n",
            "  num_env_steps_trained_this_iter: 9400\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 94.40000000000003\n",
            "    ram_util_percent: 21.800000000000004\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.13822570211852497\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10818805638378755\n",
            "    mean_inference_ms: 1.3050472740144485\n",
            "    mean_raw_obs_processing_ms: 0.24528972029640528\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 142.83\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 142.83\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 66\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 153\n",
            "      - 25\n",
            "      - 165\n",
            "      - 141\n",
            "      - 94\n",
            "      - 22\n",
            "      - 200\n",
            "      - 88\n",
            "      - 32\n",
            "      - 147\n",
            "      - 108\n",
            "      - 200\n",
            "      - 193\n",
            "      - 185\n",
            "      - 136\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 15\n",
            "      - 118\n",
            "      - 200\n",
            "      - 71\n",
            "      - 200\n",
            "      - 61\n",
            "      - 200\n",
            "      - 81\n",
            "      - 186\n",
            "      - 200\n",
            "      - 66\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 149\n",
            "      - 143\n",
            "      - 198\n",
            "      - 200\n",
            "      - 200\n",
            "      - 128\n",
            "      - 200\n",
            "      - 27\n",
            "      - 200\n",
            "      - 77\n",
            "      - 200\n",
            "      - 200\n",
            "      - 129\n",
            "      - 47\n",
            "      - 200\n",
            "      - 159\n",
            "      - 67\n",
            "      - 174\n",
            "      - 200\n",
            "      - 176\n",
            "      - 200\n",
            "      - 96\n",
            "      - 42\n",
            "      - 45\n",
            "      - 105\n",
            "      - 23\n",
            "      - 167\n",
            "      - 200\n",
            "      - 158\n",
            "      - 111\n",
            "      - 62\n",
            "      - 42\n",
            "      - 182\n",
            "      - 69\n",
            "      - 180\n",
            "      - 200\n",
            "      - 45\n",
            "      - 200\n",
            "      - 156\n",
            "      - 187\n",
            "      - 186\n",
            "      - 188\n",
            "      - 136\n",
            "      - 200\n",
            "      - 20\n",
            "      - 182\n",
            "      - 142\n",
            "      - 200\n",
            "      - 42\n",
            "      - 200\n",
            "      - 200\n",
            "      - 148\n",
            "      - 161\n",
            "      - 172\n",
            "      - 200\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 164\n",
            "      - 34\n",
            "      - 124\n",
            "      - 79\n",
            "      - 107\n",
            "      - 176\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 25\n",
            "      episode_reward:\n",
            "      - 153.0\n",
            "      - 25.0\n",
            "      - 165.0\n",
            "      - 141.0\n",
            "      - 94.0\n",
            "      - 22.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 32.0\n",
            "      - 147.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 185.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 143.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 200.0\n",
            "      - 77.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 129.0\n",
            "      - 47.0\n",
            "      - 200.0\n",
            "      - 159.0\n",
            "      - 67.0\n",
            "      - 174.0\n",
            "      - 200.0\n",
            "      - 176.0\n",
            "      - 200.0\n",
            "      - 96.0\n",
            "      - 42.0\n",
            "      - 45.0\n",
            "      - 105.0\n",
            "      - 23.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 111.0\n",
            "      - 62.0\n",
            "      - 42.0\n",
            "      - 182.0\n",
            "      - 69.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 156.0\n",
            "      - 187.0\n",
            "      - 186.0\n",
            "      - 188.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 20.0\n",
            "      - 182.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 42.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 148.0\n",
            "      - 161.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 164.0\n",
            "      - 34.0\n",
            "      - 124.0\n",
            "      - 79.0\n",
            "      - 107.0\n",
            "      - 176.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.13822570211852497\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10818805638378755\n",
            "      mean_inference_ms: 1.3050472740144485\n",
            "      mean_raw_obs_processing_ms: 0.24528972029640528\n",
            "  time_since_restore: 1068.9344594478607\n",
            "  time_this_iter_s: 10.228742599487305\n",
            "  time_total_s: 1068.9344594478607\n",
            "  timers:\n",
            "    learn_throughput: 28460.368\n",
            "    learn_time_ms: 7.027\n",
            "    load_throughput: 1017664.443\n",
            "    load_time_ms: 0.197\n",
            "    training_iteration_time_ms: 218.653\n",
            "    update_time_ms: 4.145\n",
            "  timestamp: 1656954942\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 511200\n",
            "  training_iteration: 99\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:47 (running for 00:18:17.42)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1068.93</td><td style=\"text-align: right;\">511200</td><td style=\"text-align: right;\">  142.83</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            142.83</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:53 (running for 00:18:22.50)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 3.0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status  </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>RUNNING </td><td>172.28.0.2:483</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1068.93</td><td style=\"text-align: right;\">511200</td><td style=\"text-align: right;\">  142.83</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            142.83</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for A2C_CartPole-v0_64300_00000:\n",
            "  agent_timesteps_total: 511800\n",
            "  counters:\n",
            "    num_agent_steps_sampled: 511800\n",
            "    num_agent_steps_trained: 511800\n",
            "    num_env_steps_sampled: 511800\n",
            "    num_env_steps_trained: 511800\n",
            "  custom_metrics: {}\n",
            "  date: 2022-07-04_17-15-54\n",
            "  done: true\n",
            "  episode_len_mean: 145.56\n",
            "  episode_media: {}\n",
            "  episode_reward_max: 200.0\n",
            "  episode_reward_mean: 145.56\n",
            "  episode_reward_min: 15.0\n",
            "  episodes_this_iter: 4\n",
            "  episodes_total: 5172\n",
            "  evaluation:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 128.15\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 128.15\n",
            "    episode_reward_min: 20.0\n",
            "    episodes_this_iter: 20\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 76\n",
            "      - 183\n",
            "      - 200\n",
            "      - 180\n",
            "      - 200\n",
            "      - 132\n",
            "      - 83\n",
            "      - 50\n",
            "      - 200\n",
            "      - 36\n",
            "      - 200\n",
            "      - 20\n",
            "      - 39\n",
            "      - 102\n",
            "      - 139\n",
            "      - 184\n",
            "      - 42\n",
            "      - 120\n",
            "      - 188\n",
            "      - 189\n",
            "      episode_reward:\n",
            "      - 76.0\n",
            "      - 183.0\n",
            "      - 200.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 132.0\n",
            "      - 83.0\n",
            "      - 50.0\n",
            "      - 200.0\n",
            "      - 36.0\n",
            "      - 200.0\n",
            "      - 20.0\n",
            "      - 39.0\n",
            "      - 102.0\n",
            "      - 139.0\n",
            "      - 184.0\n",
            "      - 42.0\n",
            "      - 120.0\n",
            "      - 188.0\n",
            "      - 189.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.10056473545370608\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.07677228256005479\n",
            "      mean_inference_ms: 0.9791567651245078\n",
            "      mean_raw_obs_processing_ms: 0.10813706840640305\n",
            "    timesteps_this_iter: 2563\n",
            "  experiment_id: b951ea1c0e79425694e01342b4e5a055\n",
            "  hostname: 8670c96f88b8\n",
            "  info:\n",
            "    learner:\n",
            "      default_policy:\n",
            "        custom_metrics: {}\n",
            "        learner_stats:\n",
            "          cur_lr: 9.999999747378752e-05\n",
            "          entropy_coeff: 0.009999999776482582\n",
            "          grad_gnorm: 40.000003814697266\n",
            "          model: {}\n",
            "          policy_entropy: 122.8663330078125\n",
            "          policy_loss: -57.85553741455078\n",
            "          var_gnorm: 28.225378036499023\n",
            "          vf_explained_var: 0.6322542428970337\n",
            "          vf_loss: 13115.833984375\n",
            "        num_agent_steps_trained: 200.0\n",
            "    num_agent_steps_sampled: 511800\n",
            "    num_agent_steps_trained: 511800\n",
            "    num_env_steps_sampled: 511800\n",
            "    num_env_steps_trained: 511800\n",
            "  iterations_since_restore: 100\n",
            "  node_ip: 172.28.0.2\n",
            "  num_agent_steps_sampled: 511800\n",
            "  num_agent_steps_trained: 511800\n",
            "  num_env_steps_sampled: 511800\n",
            "  num_env_steps_sampled_this_iter: 600\n",
            "  num_env_steps_trained: 511800\n",
            "  num_env_steps_trained_this_iter: 600\n",
            "  num_healthy_workers: 2\n",
            "  off_policy_estimator: {}\n",
            "  perf:\n",
            "    cpu_util_percent: 64.725\n",
            "    ram_util_percent: 21.8\n",
            "  pid: 483\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_action_processing_ms: 0.1382482195976449\n",
            "    mean_env_render_ms: 0.0\n",
            "    mean_env_wait_ms: 0.10816987905538632\n",
            "    mean_inference_ms: 1.3051820934881115\n",
            "    mean_raw_obs_processing_ms: 0.24527808886404134\n",
            "  sampler_results:\n",
            "    custom_metrics: {}\n",
            "    episode_len_mean: 145.56\n",
            "    episode_media: {}\n",
            "    episode_reward_max: 200.0\n",
            "    episode_reward_mean: 145.56\n",
            "    episode_reward_min: 15.0\n",
            "    episodes_this_iter: 4\n",
            "    hist_stats:\n",
            "      episode_lengths:\n",
            "      - 94\n",
            "      - 22\n",
            "      - 200\n",
            "      - 88\n",
            "      - 32\n",
            "      - 147\n",
            "      - 108\n",
            "      - 200\n",
            "      - 193\n",
            "      - 185\n",
            "      - 136\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 15\n",
            "      - 118\n",
            "      - 200\n",
            "      - 71\n",
            "      - 200\n",
            "      - 61\n",
            "      - 200\n",
            "      - 81\n",
            "      - 186\n",
            "      - 200\n",
            "      - 66\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 149\n",
            "      - 143\n",
            "      - 198\n",
            "      - 200\n",
            "      - 200\n",
            "      - 128\n",
            "      - 200\n",
            "      - 27\n",
            "      - 200\n",
            "      - 77\n",
            "      - 200\n",
            "      - 200\n",
            "      - 129\n",
            "      - 47\n",
            "      - 200\n",
            "      - 159\n",
            "      - 67\n",
            "      - 174\n",
            "      - 200\n",
            "      - 176\n",
            "      - 200\n",
            "      - 96\n",
            "      - 42\n",
            "      - 45\n",
            "      - 105\n",
            "      - 23\n",
            "      - 167\n",
            "      - 200\n",
            "      - 158\n",
            "      - 111\n",
            "      - 62\n",
            "      - 42\n",
            "      - 182\n",
            "      - 69\n",
            "      - 180\n",
            "      - 200\n",
            "      - 45\n",
            "      - 200\n",
            "      - 156\n",
            "      - 187\n",
            "      - 186\n",
            "      - 188\n",
            "      - 136\n",
            "      - 200\n",
            "      - 20\n",
            "      - 182\n",
            "      - 142\n",
            "      - 200\n",
            "      - 42\n",
            "      - 200\n",
            "      - 200\n",
            "      - 148\n",
            "      - 161\n",
            "      - 172\n",
            "      - 200\n",
            "      - 200\n",
            "      - 166\n",
            "      - 200\n",
            "      - 164\n",
            "      - 34\n",
            "      - 124\n",
            "      - 79\n",
            "      - 107\n",
            "      - 176\n",
            "      - 200\n",
            "      - 200\n",
            "      - 200\n",
            "      - 25\n",
            "      - 200\n",
            "      - 200\n",
            "      - 162\n",
            "      - 195\n",
            "      episode_reward:\n",
            "      - 94.0\n",
            "      - 22.0\n",
            "      - 200.0\n",
            "      - 88.0\n",
            "      - 32.0\n",
            "      - 147.0\n",
            "      - 108.0\n",
            "      - 200.0\n",
            "      - 193.0\n",
            "      - 185.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 15.0\n",
            "      - 118.0\n",
            "      - 200.0\n",
            "      - 71.0\n",
            "      - 200.0\n",
            "      - 61.0\n",
            "      - 200.0\n",
            "      - 81.0\n",
            "      - 186.0\n",
            "      - 200.0\n",
            "      - 66.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 149.0\n",
            "      - 143.0\n",
            "      - 198.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 128.0\n",
            "      - 200.0\n",
            "      - 27.0\n",
            "      - 200.0\n",
            "      - 77.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 129.0\n",
            "      - 47.0\n",
            "      - 200.0\n",
            "      - 159.0\n",
            "      - 67.0\n",
            "      - 174.0\n",
            "      - 200.0\n",
            "      - 176.0\n",
            "      - 200.0\n",
            "      - 96.0\n",
            "      - 42.0\n",
            "      - 45.0\n",
            "      - 105.0\n",
            "      - 23.0\n",
            "      - 167.0\n",
            "      - 200.0\n",
            "      - 158.0\n",
            "      - 111.0\n",
            "      - 62.0\n",
            "      - 42.0\n",
            "      - 182.0\n",
            "      - 69.0\n",
            "      - 180.0\n",
            "      - 200.0\n",
            "      - 45.0\n",
            "      - 200.0\n",
            "      - 156.0\n",
            "      - 187.0\n",
            "      - 186.0\n",
            "      - 188.0\n",
            "      - 136.0\n",
            "      - 200.0\n",
            "      - 20.0\n",
            "      - 182.0\n",
            "      - 142.0\n",
            "      - 200.0\n",
            "      - 42.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 148.0\n",
            "      - 161.0\n",
            "      - 172.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 166.0\n",
            "      - 200.0\n",
            "      - 164.0\n",
            "      - 34.0\n",
            "      - 124.0\n",
            "      - 79.0\n",
            "      - 107.0\n",
            "      - 176.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 25.0\n",
            "      - 200.0\n",
            "      - 200.0\n",
            "      - 162.0\n",
            "      - 195.0\n",
            "    off_policy_estimator: {}\n",
            "    policy_reward_max: {}\n",
            "    policy_reward_mean: {}\n",
            "    policy_reward_min: {}\n",
            "    sampler_perf:\n",
            "      mean_action_processing_ms: 0.1382482195976449\n",
            "      mean_env_render_ms: 0.0\n",
            "      mean_env_wait_ms: 0.10816987905538632\n",
            "      mean_inference_ms: 1.3051820934881115\n",
            "      mean_raw_obs_processing_ms: 0.24527808886404134\n",
            "  time_since_restore: 1080.193810224533\n",
            "  time_this_iter_s: 11.259350776672363\n",
            "  time_total_s: 1080.193810224533\n",
            "  timers:\n",
            "    learn_throughput: 27244.586\n",
            "    learn_time_ms: 7.341\n",
            "    load_throughput: 981353.299\n",
            "    load_time_ms: 0.204\n",
            "    training_iteration_time_ms: 216.912\n",
            "    update_time_ms: 3.732\n",
            "  timestamp: 1656954954\n",
            "  timesteps_since_restore: 0\n",
            "  timesteps_total: 511800\n",
            "  training_iteration: 100\n",
            "  trial_id: '64300_00000'\n",
            "  warmup_time: 12.468660354614258\n",
            "  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "== Status ==<br>Current time: 2022-07-04 17:15:54 (running for 00:18:23.79)<br>Memory usage on this node: 2.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/80 CPUs, 0/2 GPUs, 0.0/7.36 GiB heap, 0.0/3.68 GiB objects<br>Result logdir: /root/ray_results/A2C<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
              "<thead>\n",
              "<tr><th>Trial name                 </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>A2C_CartPole-v0_64300_00000</td><td>TERMINATED</td><td>172.28.0.2:483</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1080.19</td><td style=\"text-align: right;\">511800</td><td style=\"text-align: right;\">  145.56</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">                  15</td><td style=\"text-align: right;\">            145.56</td></tr>\n",
              "</tbody>\n",
              "</table><br><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-04 17:15:54,908\tINFO tune.py:748 -- Total run time: 1104.58 seconds (1103.75 seconds for the tuning loop).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "y0W-RikHO2Bu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(dfs.values())[0])\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "9uL31wFgO3PR",
        "outputId": "def8326e-bdf6-4805-e22d-56da1d112008"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    episode_reward_max  episode_reward_min  episode_reward_mean  \\\n",
              "0                 87.0                 9.0            26.210526   \n",
              "1                 69.0                 9.0            29.080000   \n",
              "2                200.0                11.0            47.314433   \n",
              "3                200.0                14.0            53.390000   \n",
              "4                200.0                14.0            65.965035   \n",
              "..                 ...                 ...                  ...   \n",
              "95               200.0                19.0           138.310000   \n",
              "96               200.0                15.0           142.670000   \n",
              "97               200.0                15.0           142.160000   \n",
              "98               200.0                15.0           142.830000   \n",
              "99               200.0                15.0           145.560000   \n",
              "\n",
              "    episode_len_mean  episodes_this_iter  num_healthy_workers  \\\n",
              "0          26.210526                 304                    2   \n",
              "1          29.080000                  60                    2   \n",
              "2          47.314433                 194                    2   \n",
              "3          53.390000                  22                    2   \n",
              "4          65.965035                 143                    2   \n",
              "..               ...                 ...                  ...   \n",
              "95        138.310000                   4                    2   \n",
              "96        142.670000                  67                    2   \n",
              "97        142.160000                   3                    2   \n",
              "98        142.830000                  66                    2   \n",
              "99        145.560000                   4                    2   \n",
              "\n",
              "    num_agent_steps_sampled  num_agent_steps_trained  num_env_steps_sampled  \\\n",
              "0                      8000                     8000                   8000   \n",
              "1                      9800                     9800                   9800   \n",
              "2                     19000                    19000                  19000   \n",
              "3                     20400                    20400                  20400   \n",
              "4                     29800                    29800                  29800   \n",
              "..                      ...                      ...                    ...   \n",
              "95                   491800                   491800                 491800   \n",
              "96                   501200                   501200                 501200   \n",
              "97                   501800                   501800                 501800   \n",
              "98                   511200                   511200                 511200   \n",
              "99                   511800                   511800                 511800   \n",
              "\n",
              "    num_env_steps_trained  ...  \\\n",
              "0                    8000  ...   \n",
              "1                    9800  ...   \n",
              "2                   19000  ...   \n",
              "3                   20400  ...   \n",
              "4                   29800  ...   \n",
              "..                    ...  ...   \n",
              "95                 491800  ...   \n",
              "96                 501200  ...   \n",
              "97                 501800  ...   \n",
              "98                 511200  ...   \n",
              "99                 511800  ...   \n",
              "\n",
              "    sampler_results/sampler_perf/mean_env_render_ms  \\\n",
              "0                                               0.0   \n",
              "1                                               0.0   \n",
              "2                                               0.0   \n",
              "3                                               0.0   \n",
              "4                                               0.0   \n",
              "..                                              ...   \n",
              "95                                              0.0   \n",
              "96                                              0.0   \n",
              "97                                              0.0   \n",
              "98                                              0.0   \n",
              "99                                              0.0   \n",
              "\n",
              "    info/learner/default_policy/num_agent_steps_trained  \\\n",
              "0                                               200.0     \n",
              "1                                               200.0     \n",
              "2                                               200.0     \n",
              "3                                               200.0     \n",
              "4                                               200.0     \n",
              "..                                                ...     \n",
              "95                                              200.0     \n",
              "96                                              200.0     \n",
              "97                                              200.0     \n",
              "98                                              200.0     \n",
              "99                                              200.0     \n",
              "\n",
              "    info/learner/default_policy/learner_stats/cur_lr  \\\n",
              "0                                             0.0001   \n",
              "1                                             0.0001   \n",
              "2                                             0.0001   \n",
              "3                                             0.0001   \n",
              "4                                             0.0001   \n",
              "..                                               ...   \n",
              "95                                            0.0001   \n",
              "96                                            0.0001   \n",
              "97                                            0.0001   \n",
              "98                                            0.0001   \n",
              "99                                            0.0001   \n",
              "\n",
              "    info/learner/default_policy/learner_stats/entropy_coeff  \\\n",
              "0                                                0.01         \n",
              "1                                                0.01         \n",
              "2                                                0.01         \n",
              "3                                                0.01         \n",
              "4                                                0.01         \n",
              "..                                                ...         \n",
              "95                                               0.01         \n",
              "96                                               0.01         \n",
              "97                                               0.01         \n",
              "98                                               0.01         \n",
              "99                                               0.01         \n",
              "\n",
              "    info/learner/default_policy/learner_stats/policy_loss  \\\n",
              "0                                          990.509500       \n",
              "1                                         1118.689500       \n",
              "2                                         1036.048000       \n",
              "3                                         1114.106900       \n",
              "4                                         1019.970950       \n",
              "..                                                ...       \n",
              "95                                         382.174960       \n",
              "96                                         414.051940       \n",
              "97                                         482.264280       \n",
              "98                                         558.632260       \n",
              "99                                         -57.855537       \n",
              "\n",
              "    info/learner/default_policy/learner_stats/policy_entropy  \\\n",
              "0                                          135.521200          \n",
              "1                                          134.570130          \n",
              "2                                          120.052370          \n",
              "3                                          126.743870          \n",
              "4                                          121.981800          \n",
              "..                                                ...          \n",
              "95                                         127.978836          \n",
              "96                                         116.771614          \n",
              "97                                         124.580770          \n",
              "98                                         119.319490          \n",
              "99                                         122.866330          \n",
              "\n",
              "    info/learner/default_policy/learner_stats/var_gnorm  \\\n",
              "0                                           22.649471     \n",
              "1                                           22.654884     \n",
              "2                                           22.712559     \n",
              "3                                           22.714193     \n",
              "4                                           22.744802     \n",
              "..                                                ...     \n",
              "95                                          28.127022     \n",
              "96                                          28.176950     \n",
              "97                                          28.180079     \n",
              "98                                          28.223635     \n",
              "99                                          28.225378     \n",
              "\n",
              "   info/learner/default_policy/learner_stats/vf_loss  \\\n",
              "0                                          7529.3290   \n",
              "1                                          9555.7580   \n",
              "2                                          8769.5670   \n",
              "3                                         11360.4820   \n",
              "4                                          9900.1230   \n",
              "..                                               ...   \n",
              "95                                         3073.4277   \n",
              "96                                         2298.0142   \n",
              "97                                         2549.2764   \n",
              "98                                         4299.8228   \n",
              "99                                        13115.8340   \n",
              "\n",
              "   info/learner/default_policy/learner_stats/grad_gnorm  \\\n",
              "0                                           40.000000     \n",
              "1                                           40.000000     \n",
              "2                                           39.999996     \n",
              "3                                           39.999996     \n",
              "4                                           40.000000     \n",
              "..                                                ...     \n",
              "95                                          40.000000     \n",
              "96                                          40.000000     \n",
              "97                                          39.999996     \n",
              "98                                          39.999996     \n",
              "99                                          40.000004     \n",
              "\n",
              "   info/learner/default_policy/learner_stats/vf_explained_var  \n",
              "0                                           -0.001363          \n",
              "1                                           -0.009660          \n",
              "2                                            0.004441          \n",
              "3                                            0.002859          \n",
              "4                                            0.009924          \n",
              "..                                                ...          \n",
              "95                                           0.849367          \n",
              "96                                           0.933219          \n",
              "97                                           0.483342          \n",
              "98                                           0.031148          \n",
              "99                                           0.632254          \n",
              "\n",
              "[100 rows x 74 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bce11a95-9494-4650-9f3f-845104315ab5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode_reward_max</th>\n",
              "      <th>episode_reward_min</th>\n",
              "      <th>episode_reward_mean</th>\n",
              "      <th>episode_len_mean</th>\n",
              "      <th>episodes_this_iter</th>\n",
              "      <th>num_healthy_workers</th>\n",
              "      <th>num_agent_steps_sampled</th>\n",
              "      <th>num_agent_steps_trained</th>\n",
              "      <th>num_env_steps_sampled</th>\n",
              "      <th>num_env_steps_trained</th>\n",
              "      <th>...</th>\n",
              "      <th>sampler_results/sampler_perf/mean_env_render_ms</th>\n",
              "      <th>info/learner/default_policy/num_agent_steps_trained</th>\n",
              "      <th>info/learner/default_policy/learner_stats/cur_lr</th>\n",
              "      <th>info/learner/default_policy/learner_stats/entropy_coeff</th>\n",
              "      <th>info/learner/default_policy/learner_stats/policy_loss</th>\n",
              "      <th>info/learner/default_policy/learner_stats/policy_entropy</th>\n",
              "      <th>info/learner/default_policy/learner_stats/var_gnorm</th>\n",
              "      <th>info/learner/default_policy/learner_stats/vf_loss</th>\n",
              "      <th>info/learner/default_policy/learner_stats/grad_gnorm</th>\n",
              "      <th>info/learner/default_policy/learner_stats/vf_explained_var</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>87.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>26.210526</td>\n",
              "      <td>26.210526</td>\n",
              "      <td>304</td>\n",
              "      <td>2</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000</td>\n",
              "      <td>8000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>990.509500</td>\n",
              "      <td>135.521200</td>\n",
              "      <td>22.649471</td>\n",
              "      <td>7529.3290</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>-0.001363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>69.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>29.080000</td>\n",
              "      <td>29.080000</td>\n",
              "      <td>60</td>\n",
              "      <td>2</td>\n",
              "      <td>9800</td>\n",
              "      <td>9800</td>\n",
              "      <td>9800</td>\n",
              "      <td>9800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1118.689500</td>\n",
              "      <td>134.570130</td>\n",
              "      <td>22.654884</td>\n",
              "      <td>9555.7580</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>-0.009660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>200.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>47.314433</td>\n",
              "      <td>47.314433</td>\n",
              "      <td>194</td>\n",
              "      <td>2</td>\n",
              "      <td>19000</td>\n",
              "      <td>19000</td>\n",
              "      <td>19000</td>\n",
              "      <td>19000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1036.048000</td>\n",
              "      <td>120.052370</td>\n",
              "      <td>22.712559</td>\n",
              "      <td>8769.5670</td>\n",
              "      <td>39.999996</td>\n",
              "      <td>0.004441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>200.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>53.390000</td>\n",
              "      <td>53.390000</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>20400</td>\n",
              "      <td>20400</td>\n",
              "      <td>20400</td>\n",
              "      <td>20400</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1114.106900</td>\n",
              "      <td>126.743870</td>\n",
              "      <td>22.714193</td>\n",
              "      <td>11360.4820</td>\n",
              "      <td>39.999996</td>\n",
              "      <td>0.002859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>200.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>65.965035</td>\n",
              "      <td>65.965035</td>\n",
              "      <td>143</td>\n",
              "      <td>2</td>\n",
              "      <td>29800</td>\n",
              "      <td>29800</td>\n",
              "      <td>29800</td>\n",
              "      <td>29800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1019.970950</td>\n",
              "      <td>121.981800</td>\n",
              "      <td>22.744802</td>\n",
              "      <td>9900.1230</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.009924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>200.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>138.310000</td>\n",
              "      <td>138.310000</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>491800</td>\n",
              "      <td>491800</td>\n",
              "      <td>491800</td>\n",
              "      <td>491800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>382.174960</td>\n",
              "      <td>127.978836</td>\n",
              "      <td>28.127022</td>\n",
              "      <td>3073.4277</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.849367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>200.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>142.670000</td>\n",
              "      <td>142.670000</td>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>501200</td>\n",
              "      <td>501200</td>\n",
              "      <td>501200</td>\n",
              "      <td>501200</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>414.051940</td>\n",
              "      <td>116.771614</td>\n",
              "      <td>28.176950</td>\n",
              "      <td>2298.0142</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.933219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>200.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>142.160000</td>\n",
              "      <td>142.160000</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>501800</td>\n",
              "      <td>501800</td>\n",
              "      <td>501800</td>\n",
              "      <td>501800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>482.264280</td>\n",
              "      <td>124.580770</td>\n",
              "      <td>28.180079</td>\n",
              "      <td>2549.2764</td>\n",
              "      <td>39.999996</td>\n",
              "      <td>0.483342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>200.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>142.830000</td>\n",
              "      <td>142.830000</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>511200</td>\n",
              "      <td>511200</td>\n",
              "      <td>511200</td>\n",
              "      <td>511200</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>558.632260</td>\n",
              "      <td>119.319490</td>\n",
              "      <td>28.223635</td>\n",
              "      <td>4299.8228</td>\n",
              "      <td>39.999996</td>\n",
              "      <td>0.031148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>200.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>145.560000</td>\n",
              "      <td>145.560000</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>511800</td>\n",
              "      <td>511800</td>\n",
              "      <td>511800</td>\n",
              "      <td>511800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>200.0</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-57.855537</td>\n",
              "      <td>122.866330</td>\n",
              "      <td>28.225378</td>\n",
              "      <td>13115.8340</td>\n",
              "      <td>40.000004</td>\n",
              "      <td>0.632254</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 74 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bce11a95-9494-4650-9f3f-845104315ab5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bce11a95-9494-4650-9f3f-845104315ab5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bce11a95-9494-4650-9f3f-845104315ab5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('a2c.csv') "
      ],
      "metadata": {
        "id": "WPjZMckcPA50"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ray.rllib.agents.a3c as a3c"
      ],
      "metadata": {
        "id": "WcYjsEoy4lYF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = analysis.get_best_checkpoint(\n",
        "    metric=\"episode_reward_mean\", \n",
        "    mode=\"max\", \n",
        "    trial=analysis.trials[0]\n",
        ")"
      ],
      "metadata": {
        "id": "Cs8TH5DrQkHI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = a3c.A2CTrainer(config={\"env\":\"CartPole-v0\",\"evaluation_interval\":2,\"evaluation_num_episodes\": 20})\n",
        "agent.restore(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDJRCOak4u8z",
        "outputId": "b7b25e59-a37e-404b-de29-0e0ebd198a60"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-07-04 17:27:13,839\tWARNING logger.py:337 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
            "2022-07-04 17:27:13,848\tINFO trainer.py:2333 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
            "2022-07-04 17:27:13,855\tWARNING deprecation.py:47 -- DeprecationWarning: `evaluation_num_episodes` has been deprecated. Use ``evaluation_duration` and `evaluation_duration_unit=episodes`` instead. This will raise an error in the future!\n",
            "2022-07-04 17:27:13,858\tINFO trainer.py:906 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
            "2022-07-04 17:27:24,527\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
            "2022-07-04 17:27:25,369\tINFO trainable.py:163 -- Trainable.setup took 11.523 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
            "2022-07-04 17:27:25,371\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
            "2022-07-04 17:27:25,430\tINFO trainable.py:589 -- Restored on 172.28.0.2 from checkpoint: /root/ray_results/A2C/A2C_CartPole-v0_64300_00000_0_2022-07-04_16-57-30/checkpoint_000100/checkpoint-100\n",
            "2022-07-04 17:27:25,432\tINFO trainable.py:597 -- Current state after restoring: {'_iteration': 100, '_timesteps_total': None, '_time_total': 1080.193810224533, '_episodes_total': 5172}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ],
      "metadata": {
        "id": "1yh2PoDQFJZ_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym"
      ],
      "metadata": {
        "id": "FCeEKsp28XZv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rendering Dependencies\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# Gym Dependencies\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym[box2d] > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "U8F1uySVNwyx",
        "outputId": "a055aef7-f165-4287-ff72-c2c3cac4e3dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-63.1.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed setuptools-63.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Google Colab needs to render the environment to a virtual display\n",
        "# we will record this as a video and play it after the training has finished\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[-1]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "metadata": {
        "id": "WFZsZXtDN8HO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "game = \"CartPole-v0\"\n",
        "\n",
        "nr_of_runs = 10\n",
        "current_run = 1\n",
        "\n",
        "if current_run == nr_of_runs:\n",
        "  env = wrap_env(gym.make(game))\n",
        "else:\n",
        "  env = gym.make(game)\n",
        "observation = env.reset()\n",
        "timestep = 0\n",
        "\n",
        "while current_run < nr_of_runs+1:\n",
        "    # render the current frame to the video recorder of Google Colab\n",
        "    if current_run == nr_of_runs:\n",
        "      env.render()\n",
        "    \n",
        "    # your agent goes here \n",
        "    # action_space.sample() results in a random action being picked\n",
        "    action = agent.compute_action(observation)\n",
        "    \n",
        "    # apply the action to the real environment and forward the game\n",
        "    observation, reward, done, info = env.step(action) \n",
        "    timestep += 1\n",
        "    \n",
        "    if done:\n",
        "      # for Monte Carlo Method you will need to update your value matrix here\n",
        "      # Temporal Difference Learning updates the value matrix in every step\n",
        "      \n",
        "      # this test ends after 'nr_of_runs' (default = 10)\n",
        "      # change the variable at the top if you want to train longer (recommended) \n",
        "      print(\"run: \"+ str(current_run) + \" took \" + str(timestep) + \" timesteps\")\n",
        "      current_run += 1\n",
        "      timestep = 0\n",
        "      if current_run == nr_of_runs:\n",
        "        # record the last run\n",
        "        env.close()\n",
        "        env = wrap_env(gym.make(game))\n",
        "      else:\n",
        "        env.close()\n",
        "        env = gym.make(game)\n",
        "      \n",
        "      observation = env.reset()\n",
        "show_video()  #only shows the last run\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "oA0Ffb4CPEME",
        "outputId": "837d7cdb-e2d5-42ab-fc3d-567aed422388"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run: 1 took 200 timesteps\n",
            "run: 2 took 200 timesteps\n",
            "run: 3 took 145 timesteps\n",
            "run: 4 took 130 timesteps\n",
            "run: 5 took 140 timesteps\n",
            "run: 6 took 200 timesteps\n",
            "run: 7 took 37 timesteps\n",
            "run: 8 took 54 timesteps\n",
            "run: 9 took 173 timesteps\n",
            "run: 10 took 195 timesteps\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<video alt=\"test\" autoplay \n",
              "                controls style=\"height: 400px;\">\n",
              "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "             </video>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This Section is only for resources**"
      ],
      "metadata": {
        "id": "7jYQszBvXXwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game = 'CartPole-v1'\n",
        "# other interesting and simple environments: Pong-v0, MsPacman-v0, CarRacing-v0\n",
        "\n",
        "nr_of_runs = 10\n",
        "current_run = 1\n",
        "\n",
        "if current_run == nr_of_runs:\n",
        "  env = wrap_env(gym.make(game))\n",
        "else:\n",
        "  env = gym.make(game)\n",
        "observation = env.reset()\n",
        "timestep = 0\n",
        "\n",
        "while current_run < nr_of_runs+1:\n",
        "    # render the current frame to the video recorder of Google Colab\n",
        "    if current_run == nr_of_runs:\n",
        "      env.render()\n",
        "    \n",
        "    # your agent goes here \n",
        "    # action_space.sample() results in a random action being picked\n",
        "    action = env.action_space.sample()\n",
        "    \n",
        "    # apply the action to the real environment and forward the game\n",
        "    observation, reward, done, info = env.step(action) \n",
        "    timestep += 1\n",
        "    \n",
        "    if done:\n",
        "      # for Monte Carlo Method you will need to update your value matrix here\n",
        "      # Temporal Difference Learning updates the value matrix in every step\n",
        "      \n",
        "      # this test ends after 'nr_of_runs' (default = 10)\n",
        "      # change the variable at the top if you want to train longer (recommended) \n",
        "      print(\"run: \"+ str(current_run) + \" took \" + str(timestep) + \" timesteps\")\n",
        "      current_run += 1\n",
        "      timestep = 0\n",
        "      if current_run == nr_of_runs:\n",
        "        # record the last run\n",
        "        env.close()\n",
        "        env = wrap_env(gym.make(game))\n",
        "      else:\n",
        "        env.close()\n",
        "        env = gym.make(game)\n",
        "      \n",
        "      observation = env.reset()\n",
        "      show_video()  #only shows the last run\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "L8TgVzWCOWVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}