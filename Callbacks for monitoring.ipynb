{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Callbacks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XLUPAkCR1mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4679ab8f-2e72-4afa-9ecc-d28e9d75acdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-1.5.0-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.21.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.3.5)\n",
            "Collecting gym==0.21\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (1.11.0+cu113)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (5.4.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (4.1.2.30)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.4\n",
            "  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from stable-baselines3[extra]) (2.8.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3[extra]) (4.11.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.4->stable-baselines3[extra]) (5.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (4.64.0)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3[extra]) (3.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.37.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (57.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.46.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.7)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.1.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->stable-baselines3[extra]) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616826 sha256=c1e2903d20a46cf17b1286c1f83639fdf838431ec40d9cd3735ae39d8fb6b6dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n",
            "  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=9ecce4680a7ab00301869a8161924cd011233e1eccfacb82e0b9718d674e0896\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: gym, AutoROM.accept-rom-license, autorom, stable-baselines3, ale-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0 stable-baselines3-1.5.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Callbacks\n",
        "A callback is a set of functions that will be called at given stages of the training procedure. We can use callbacks to access internal state of the RL model during training. It allows one to do monitoring, auto saving, model manipulation, progress bars, …\n",
        "To build a custom callback, you need to create a class that derives from BaseCallback. This will give you access to events (_on_training_start, _on_step) and useful variables (like self.model for the RL model).\n"
      ],
      "metadata": {
        "id": "u0OGwseJEuLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "\n",
        "class CustomCallback(BaseCallback):\n",
        "    \"\"\"\n",
        "    A custom callback that derives from ``BaseCallback``.\n",
        "\n",
        "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
        "    \"\"\"\n",
        "    def __init__(self, verbose=0):\n",
        "        super(CustomCallback, self).__init__(verbose)\n",
        "        # Those variables will be accessible in the callback\n",
        "        # (they are defined in the base class)\n",
        "        # The RL model\n",
        "        # self.model = None  # type: BaseAlgorithm\n",
        "        # An alias for self.model.get_env(), the environment used for training\n",
        "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
        "        # Number of time the callback was called\n",
        "        # self.n_calls = 0  # type: int\n",
        "        # self.num_timesteps = 0  # type: int\n",
        "        # local and global variables\n",
        "        # self.locals = None  # type: Dict[str, Any]\n",
        "        # self.globals = None  # type: Dict[str, Any]\n",
        "        # The logger object, used to report things in the terminal\n",
        "        # self.logger = None  # stable_baselines3.common.logger\n",
        "        # # Sometimes, for event callback, it is useful\n",
        "        # # to have access to the parent object\n",
        "        # self.parent = None  # type: Optional[BaseCallback]\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        \"\"\"\n",
        "        This method is called before the first rollout starts.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_rollout_start(self) -> None:\n",
        "        \"\"\"\n",
        "        A rollout is the collection of environment interaction\n",
        "        using the current policy.\n",
        "        This event is triggered before collecting new samples.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        \"\"\"\n",
        "        This method will be called by the model after each call to `env.step()`.\n",
        "\n",
        "        For child callback (of an `EventCallback`), this will be called\n",
        "        when the event is triggered.\n",
        "\n",
        "        :return: (bool) If the callback returns False, training is aborted early.\n",
        "        \"\"\"\n",
        "        return True\n",
        "\n",
        "    def _on_rollout_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before updating the policy.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def _on_training_end(self) -> None:\n",
        "        \"\"\"\n",
        "        This event is triggered before exiting the `learn()` method.\n",
        "        \"\"\"\n",
        "        pass"
      ],
      "metadata": {
        "id": "CtDQDtLkEzjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "# Save a checkpoint every 1000 steps\n",
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/',\n",
        "                                         name_prefix='rl_model')\n",
        "\n",
        "model = SAC('MlpPolicy', 'Pendulum-v1')\n",
        "model.learn(2000, callback=checkpoint_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EMq2p1zFnS7",
        "outputId": "7955cb83-f31c-459e-eaac-1486a973e68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7fdd319e3350>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "# Separate evaluation env\n",
        "eval_env = gym.make('Pendulum-v1')\n",
        "# Use deterministic actions for evaluation\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
        "                             log_path='./logs/', eval_freq=500,\n",
        "                             deterministic=True, render=False)\n",
        "\n",
        "model = SAC('MlpPolicy', 'Pendulum-v1')\n",
        "model.learn(5000, callback=eval_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q060I5YjF1Lg",
        "outputId": "c96955ed-dc75-4e6f-944b-95a4d1fda1bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=500, episode_reward=-1747.92 +/- 144.14\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000, episode_reward=-1673.81 +/- 139.82\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1500, episode_reward=-1377.39 +/- 138.81\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=-1138.73 +/- 84.72\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2500, episode_reward=-452.07 +/- 125.37\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3000, episode_reward=-154.50 +/- 148.97\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3500, episode_reward=-196.58 +/- 93.37\n",
            "Episode length: 200.00 +/- 0.00\n",
            "Eval num_timesteps=4000, episode_reward=-466.67 +/- 471.22\n",
            "Episode length: 200.00 +/- 0.00\n",
            "Eval num_timesteps=4500, episode_reward=-129.92 +/- 86.56\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=5000, episode_reward=-97.99 +/- 91.51\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7fdc6a192450>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./logs/')\n",
        "# Separate evaluation env\n",
        "eval_env = gym.make('Pendulum-v1')\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/best_model',\n",
        "                             log_path='./logs/results', eval_freq=500)\n",
        "# Create the callback list\n",
        "callback = CallbackList([checkpoint_callback, eval_callback])\n",
        "\n",
        "model = SAC('MlpPolicy', 'Pendulum-v1')\n",
        "# Equivalent to:\n",
        "# model.learn(5000, callback=[checkpoint_callback, eval_callback])\n",
        "model.learn(5000, callback=callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BRlH1NTIQ4Z",
        "outputId": "ce05c6f7-7d0d-4f90-c5b0-b5037bd8e83b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=500, episode_reward=-1550.58 +/- 204.18\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1000, episode_reward=-1745.85 +/- 130.22\n",
            "Episode length: 200.00 +/- 0.00\n",
            "Eval num_timesteps=1500, episode_reward=-1320.89 +/- 53.21\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2000, episode_reward=-1140.84 +/- 73.10\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2500, episode_reward=-914.97 +/- 85.99\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3000, episode_reward=-878.96 +/- 15.72\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3500, episode_reward=-413.86 +/- 86.31\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4000, episode_reward=-202.49 +/- 128.98\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4500, episode_reward=-156.61 +/- 59.79\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "Eval num_timesteps=5000, episode_reward=-124.75 +/- 78.82\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7fdc67cde250>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
        "\n",
        "# Separate evaluation env\n",
        "eval_env = gym.make('CartPole-v1')\n",
        "# Stop training when the model reaches the reward threshold\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-200, verbose=1)\n",
        "eval_callback = EvalCallback(eval_env, callback_on_new_best=callback_on_best, verbose=1)\n",
        "\n",
        "model = SAC('MlpPolicy', 'Pendulum-v1', verbose=1)\n",
        "# Almost infinite number of timesteps, but the training will stop\n",
        "# early as soon as the reward threshold is reached\n",
        "model.learn(int(1e10), callback=eval_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsAyPc24IxS0",
        "outputId": "2b792e30-597e-4c14-cab7-a9ecb73ffd76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'Pendulum-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 200       |\n",
            "|    ep_rew_mean     | -1.53e+03 |\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 46        |\n",
            "|    time_elapsed    | 17        |\n",
            "|    total_timesteps | 800       |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 25.2      |\n",
            "|    critic_loss     | 0.221     |\n",
            "|    ent_coef        | 0.812     |\n",
            "|    ent_coef_loss   | -0.345    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 699       |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 200       |\n",
            "|    ep_rew_mean     | -1.57e+03 |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 42        |\n",
            "|    time_elapsed    | 37        |\n",
            "|    total_timesteps | 1600      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 52.6      |\n",
            "|    critic_loss     | 0.247     |\n",
            "|    ent_coef        | 0.641     |\n",
            "|    ent_coef_loss   | -0.654    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 1499      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 200       |\n",
            "|    ep_rew_mean     | -1.44e+03 |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 41        |\n",
            "|    time_elapsed    | 57        |\n",
            "|    total_timesteps | 2400      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 72.6      |\n",
            "|    critic_loss     | 0.488     |\n",
            "|    ent_coef        | 0.521     |\n",
            "|    ent_coef_loss   | -0.691    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 2299      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 200       |\n",
            "|    ep_rew_mean     | -1.27e+03 |\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 40        |\n",
            "|    time_elapsed    | 78        |\n",
            "|    total_timesteps | 3200      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 81.3      |\n",
            "|    critic_loss     | 0.643     |\n",
            "|    ent_coef        | 0.445     |\n",
            "|    ent_coef_loss   | -0.433    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 3099      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 200       |\n",
            "|    ep_rew_mean     | -1.05e+03 |\n",
            "| time/              |           |\n",
            "|    episodes        | 20        |\n",
            "|    fps             | 40        |\n",
            "|    time_elapsed    | 98        |\n",
            "|    total_timesteps | 4000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 88.1      |\n",
            "|    critic_loss     | 0.703     |\n",
            "|    ent_coef        | 0.392     |\n",
            "|    ent_coef_loss   | -0.518    |\n",
            "|    learning_rate   | 0.0003    |\n",
            "|    n_updates       | 3899      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -902     |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 40       |\n",
            "|    time_elapsed    | 119      |\n",
            "|    total_timesteps | 4800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 83       |\n",
            "|    critic_loss     | 0.853    |\n",
            "|    ent_coef        | 0.34     |\n",
            "|    ent_coef_loss   | -0.403   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4699     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -799     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 40       |\n",
            "|    time_elapsed    | 139      |\n",
            "|    total_timesteps | 5600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 86.5     |\n",
            "|    critic_loss     | 1.12     |\n",
            "|    ent_coef        | 0.289    |\n",
            "|    ent_coef_loss   | -0.306   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 5499     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -714     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 39       |\n",
            "|    time_elapsed    | 160      |\n",
            "|    total_timesteps | 6400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 77.3     |\n",
            "|    critic_loss     | 1.35     |\n",
            "|    ent_coef        | 0.246    |\n",
            "|    ent_coef_loss   | -0.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 6299     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -672     |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 39       |\n",
            "|    time_elapsed    | 181      |\n",
            "|    total_timesteps | 7200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 87.4     |\n",
            "|    critic_loss     | 4.77     |\n",
            "|    ent_coef        | 0.211    |\n",
            "|    ent_coef_loss   | -0.163   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 7099     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -620     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 39       |\n",
            "|    time_elapsed    | 202      |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 62.2     |\n",
            "|    critic_loss     | 4.01     |\n",
            "|    ent_coef        | 0.195    |\n",
            "|    ent_coef_loss   | -0.2     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 7899     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -574     |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 39       |\n",
            "|    time_elapsed    | 223      |\n",
            "|    total_timesteps | 8800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 70       |\n",
            "|    critic_loss     | 8.73     |\n",
            "|    ent_coef        | 0.187    |\n",
            "|    ent_coef_loss   | 0.0853   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 8699     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -541     |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 39       |\n",
            "|    time_elapsed    | 245      |\n",
            "|    total_timesteps | 9600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 67.2     |\n",
            "|    critic_loss     | 4.07     |\n",
            "|    ent_coef        | 0.177    |\n",
            "|    ent_coef_loss   | -0.368   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9499     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-121.73 +/- 77.76\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -122     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 56.1     |\n",
            "|    critic_loss     | 1.95     |\n",
            "|    ent_coef        | 0.169    |\n",
            "|    ent_coef_loss   | -0.214   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 9899     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Stopping training because the mean reward -121.73  is above the threshold -200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7fdc67c82990>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import StopTrainingOnMaxEpisodes\n",
        "\n",
        "# Stops training when the model reaches the maximum number of episodes\n",
        "callback_max_episodes = StopTrainingOnMaxEpisodes(max_episodes=5, verbose=1)\n",
        "\n",
        "model = SAC('MlpPolicy', 'Pendulum-v1', verbose=1)\n",
        "# Almost infinite number of timesteps, but the training will stop\n",
        "# early as soon as the max number of episodes is reached\n",
        "model.learn(int(1e10), callback=callback_max_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPat7mhYJW-5",
        "outputId": "1f30711e-a40c-48d1-965f-d65f4c8ca0e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'Pendulum-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -1.6e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 45       |\n",
            "|    time_elapsed    | 17       |\n",
            "|    total_timesteps | 800      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 24.6     |\n",
            "|    critic_loss     | 0.234    |\n",
            "|    ent_coef        | 0.812    |\n",
            "|    ent_coef_loss   | -0.341   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 699      |\n",
            "---------------------------------\n",
            "Stopping training with a total of 1000 steps because the SAC model reached max_episodes=5, by playing for 5 episodes \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7fdc67c759d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
        "\n",
        "# Separate evaluation env\n",
        "eval_env = gym.make(\"Pendulum-v1\")\n",
        "# Stop training if there is no improvement after more than 3 evaluations\n",
        "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
        "eval_callback = EvalCallback(eval_env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
        "\n",
        "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", learning_rate=1e-3, verbose=1)\n",
        "# Almost infinite number of timesteps, but the training will stop early\n",
        "# as soon as the the number of consecutive evaluations without model\n",
        "# improvement is greater than 3\n",
        "model.learn(int(1e10), callback=eval_callback)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHI6exisKrmy",
        "outputId": "cbcdb4ff-ce66-4915-f1a3-69a6897b9002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'Pendulum-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -1.3e+03 |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 47       |\n",
            "|    time_elapsed    | 16       |\n",
            "|    total_timesteps | 800      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 21.7     |\n",
            "|    critic_loss     | 0.0584   |\n",
            "|    ent_coef        | 0.507    |\n",
            "|    ent_coef_loss   | -0.993   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 699      |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/evaluation.py:69: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval num_timesteps=1000, episode_reward=-1771.76 +/- 54.40\n",
            "Episode length: 200.00 +/- 0.00\n",
            "----------------------------------\n",
            "| eval/              |           |\n",
            "|    mean_ep_length  | 200       |\n",
            "|    mean_reward     | -1.77e+03 |\n",
            "| time/              |           |\n",
            "|    total_timesteps | 1000      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 27.4      |\n",
            "|    critic_loss     | 0.042     |\n",
            "|    ent_coef        | 0.422     |\n",
            "|    ent_coef_loss   | -1.21     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 899       |\n",
            "----------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 200       |\n",
            "|    ep_rew_mean     | -1.42e+03 |\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 41        |\n",
            "|    time_elapsed    | 38        |\n",
            "|    total_timesteps | 1600      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 46.4      |\n",
            "|    critic_loss     | 0.061     |\n",
            "|    ent_coef        | 0.259     |\n",
            "|    ent_coef_loss   | -0.974    |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 1499      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=2000, episode_reward=-830.41 +/- 452.49\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -830     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 57.8     |\n",
            "|    critic_loss     | 0.064    |\n",
            "|    ent_coef        | 0.213    |\n",
            "|    ent_coef_loss   | -0.278   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 1899     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "----------------------------------\n",
            "| rollout/           |           |\n",
            "|    ep_len_mean     | 200       |\n",
            "|    ep_rew_mean     | -1.26e+03 |\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 40        |\n",
            "|    time_elapsed    | 59        |\n",
            "|    total_timesteps | 2400      |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 60.6      |\n",
            "|    critic_loss     | 0.0824    |\n",
            "|    ent_coef        | 0.2       |\n",
            "|    ent_coef_loss   | -0.34     |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 2299      |\n",
            "----------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=-177.45 +/- 124.79\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -177     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 66       |\n",
            "|    critic_loss     | 0.0906   |\n",
            "|    ent_coef        | 0.201    |\n",
            "|    ent_coef_loss   | -0.111   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 2899     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -996     |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 39       |\n",
            "|    time_elapsed    | 81       |\n",
            "|    total_timesteps | 3200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 66.5     |\n",
            "|    critic_loss     | 0.147    |\n",
            "|    ent_coef        | 0.2      |\n",
            "|    ent_coef_loss   | -0.157   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3099     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=-557.39 +/- 533.94\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -557     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 62.6     |\n",
            "|    critic_loss     | 0.379    |\n",
            "|    ent_coef        | 0.173    |\n",
            "|    ent_coef_loss   | -0.0617  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3899     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -841     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 103      |\n",
            "|    total_timesteps | 4000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -725     |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 123      |\n",
            "|    total_timesteps | 4800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 63.3     |\n",
            "|    critic_loss     | 0.434    |\n",
            "|    ent_coef        | 0.136    |\n",
            "|    ent_coef_loss   | 0.0251   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 4699     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=-573.77 +/- 510.25\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -574     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 57.8     |\n",
            "|    critic_loss     | 0.54     |\n",
            "|    ent_coef        | 0.128    |\n",
            "|    ent_coef_loss   | -0.224   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 4899     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -683     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 144      |\n",
            "|    total_timesteps | 5600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 63.9     |\n",
            "|    critic_loss     | 1.12     |\n",
            "|    ent_coef        | 0.114    |\n",
            "|    ent_coef_loss   | 0.259    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5499     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=6000, episode_reward=-120.29 +/- 73.69\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -120     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 6000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 54.4     |\n",
            "|    critic_loss     | 0.965    |\n",
            "|    ent_coef        | 0.107    |\n",
            "|    ent_coef_loss   | -0.713   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 5899     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -620     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 166      |\n",
            "|    total_timesteps | 6400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 60.9     |\n",
            "|    critic_loss     | 1.96     |\n",
            "|    ent_coef        | 0.101    |\n",
            "|    ent_coef_loss   | 0.0904   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 6299     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=7000, episode_reward=-191.15 +/- 58.19\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -191     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 7000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 57.5     |\n",
            "|    critic_loss     | 1.97     |\n",
            "|    ent_coef        | 0.0881   |\n",
            "|    ent_coef_loss   | 0.0101   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 6899     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -565     |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 188      |\n",
            "|    total_timesteps | 7200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 58       |\n",
            "|    critic_loss     | 1.37     |\n",
            "|    ent_coef        | 0.0851   |\n",
            "|    ent_coef_loss   | -0.3     |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7099     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=8000, episode_reward=-165.33 +/- 52.29\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -165     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 8000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 49.1     |\n",
            "|    critic_loss     | 1.19     |\n",
            "|    ent_coef        | 0.0778   |\n",
            "|    ent_coef_loss   | -0.341   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7899     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -517     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 38       |\n",
            "|    time_elapsed    | 210      |\n",
            "|    total_timesteps | 8000     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -484     |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 37       |\n",
            "|    time_elapsed    | 232      |\n",
            "|    total_timesteps | 8800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 54.9     |\n",
            "|    critic_loss     | 1.55     |\n",
            "|    ent_coef        | 0.0733   |\n",
            "|    ent_coef_loss   | 0.0115   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8699     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=9000, episode_reward=-174.27 +/- 127.93\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -174     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 9000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 48.9     |\n",
            "|    critic_loss     | 1.29     |\n",
            "|    ent_coef        | 0.0713   |\n",
            "|    ent_coef_loss   | -0.157   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 8899     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 200      |\n",
            "|    ep_rew_mean     | -454     |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 37       |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 9600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 42.9     |\n",
            "|    critic_loss     | 1.19     |\n",
            "|    ent_coef        | 0.0625   |\n",
            "|    ent_coef_loss   | -0.515   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9499     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=10000, episode_reward=-139.69 +/- 109.03\n",
            "Episode length: 200.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 200      |\n",
            "|    mean_reward     | -140     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 10000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 37.4     |\n",
            "|    critic_loss     | 1.32     |\n",
            "|    ent_coef        | 0.0571   |\n",
            "|    ent_coef_loss   | -0.045   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 9899     |\n",
            "---------------------------------\n",
            "Stopping training because there was no new best model in the last 4 evaluations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<stable_baselines3.sac.sac.SAC at 0x7fdc67c259d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0QkMObl1LGaU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}